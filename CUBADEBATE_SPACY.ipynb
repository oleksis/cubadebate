{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17056,
     "status": "ok",
     "timestamp": 1587740583735,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "2uwNZ8LNlcXZ",
    "outputId": "d17cd409-2981-4c77-ea3a-0df6e11a47c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordCloud installed.\n",
      "Spacy es_core_news_model installed.\n",
      "Restart the runtime!\n"
     ]
    }
   ],
   "source": [
    "!pip install dask\n",
    "!pip install wordcloud\n",
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print('Dask installed.')\n",
    "print('WordCloud installed.')\n",
    "print('Spacy es_core_news_model installed.\\nRestart the runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Qi1yZ1EbYpQ"
   },
   "source": [
    "Go to Restart runtime..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 6.06s\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from collections import Counter, OrderedDict\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List, Optional, Dict, Tuple, Iterable, Any\n",
    "\n",
    "import requests\n",
    "import dask\n",
    "from dask import bag as db\n",
    "from dask.delayed import delayed, Delayed\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Annotations\n",
    "CommentList = List[Optional[Dict[str, Any]]]\n",
    "Document = str\n",
    "DocumentList = List[Document]\n",
    "DocumentNormalizedList = List[DocumentList]\n",
    "TermFrequency = Dict[str, float]\n",
    "IDF = Dict[str, float]\n",
    "TFIDF = Dict[str, float]\n",
    "TFIDF_List = List[TFIDF]\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionErrorRequests = requests.exceptions.ConnectionError\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_comments_json(page: int = 1) -> CommentList:\n",
    "    \"\"\"Get JSON of comments from endpoint API Wordpress v2\n",
    "\n",
    "    RETURNS (CommentList): Delayed Response List of comments in JSON format\n",
    "    to compute for Dask.Bag\n",
    "    \"\"\"\n",
    "    params = {\"page\": str(page)}\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            resp = session.get(COMMENTS_ENDPOINT,\n",
    "                               params=params)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json()\n",
    "                return results\n",
    "    except ConnectionErrorRequests:\n",
    "        pass  # Try again! Occurred Connection Error\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_comments(pages: int) -> List[str]:\n",
    "    \"\"\"Get List of comments per page\"\"\"\n",
    "    comments_delayed = (get_comments_json(page)\n",
    "                        for page in range(1, pages + 1))\n",
    "\n",
    "    comments_bag = db.from_delayed(comments_delayed).map(lambda d: d['content']['rendered'])\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    comments: List[str] = comments_bag.compute(scheduler='threads')\n",
    "    return comments\n",
    "\n",
    "\n",
    "_start = time.time()\n",
    "comments_list = get_comments(pages=1)\n",
    "_end = time.time()\n",
    "print(f'{len(comments_list)} Comments downloaded in {(_end - _start):.2f}s')\n",
    "# comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980 Comments downloaded in 609.51s\n"
     ]
    }
   ],
   "source": [
    "NUM_PAGES = 100\n",
    "\n",
    "_start = time.time()\n",
    "corpus = get_comments(pages=NUM_PAGES)\n",
    "_end = time.time()\n",
    "print(f'{len(corpus)} Comments downloaded in {(_end - _start):.2f}s')\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20379,
     "status": "ok",
     "timestamp": 1587741838415,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "b4r5kXdGlK8r",
    "outputId": "c9245c95-647a-4e15-efb6-3b06ae130184",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980 Documents normalized in 282.10s\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "\n",
    "def remplace_accents(text: str) -> str:\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r'á', 'a', text, flags=re.I)\n",
    "    text = re.sub(r'é', 'e', text, flags=re.I)\n",
    "    text = re.sub(r'í', 'i', text, flags=re.I)\n",
    "    text = re.sub(r'ó', 'o', text, flags=re.I)\n",
    "    text = re.sub(r'ú', 'u', text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(markup: str) -> str:\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', markup)\n",
    "    url_pattern = r'(http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "@delayed\n",
    "def clean(doc: str) -> List[str]:\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "\n",
    "    def is_token_allowed(token: Token) -> bool:\n",
    "        \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "        if (not token or not token.string.strip() or\n",
    "                token.is_stop or token.is_punct or len(token) < 3):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def preprocess_token(token: Token) -> str:\n",
    "        \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "        result = remplace_accents(token.lemma_.strip().lower())\n",
    "        return result\n",
    "\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = [preprocess_token(token) for token in nlp(text)\n",
    "              if is_token_allowed(token)]\n",
    "    normalized = \" \".join(word for word in tokens)\n",
    "    return normalized.split()\n",
    "\n",
    "\n",
    "def get_documents_delayed(documents: DocumentList) -> List[Delayed]:\n",
    "    \"\"\"Return List[Delayed] clean document\"\"\"\n",
    "    results = []  # type: List[dask.delayed.Delayed]\n",
    "\n",
    "    for document in documents:\n",
    "        results.append(clean(document))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_documents_normalized(documents: DocumentList) -> DocumentNormalizedList:\n",
    "    \"\"\"Return DocumentNormalizedList clean document\"\"\"\n",
    "    return list(*dask.compute(get_documents_delayed(documents)))\n",
    "\n",
    "\n",
    "# Process corpus using Dask.delayed\n",
    "_start = time.time()\n",
    "documents_normalized = get_documents_normalized(corpus)\n",
    "_end = time.time()\n",
    "print(f'{len(documents_normalized)} Documents normalized in {(_end - _start):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5335,
     "status": "ok",
     "timestamp": 1587741858758,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1Dce5BcYlK88",
    "outputId": "c47b6fdf-d088-4f58-a67f-1698c65f258a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF comments list\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "def comment_tf(document: DocumentList) -> TermFrequency:\n",
    "    \"\"\"Term Frequency for a document\"\"\"\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "\n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def comments_tf(documents: DocumentNormalizedList) -> List[TermFrequency]:\n",
    "    \"\"\"Term Frequency for many document\"\"\"\n",
    "    return [comment_tf(comment)\n",
    "            for comment in documents]\n",
    "\n",
    "\n",
    "def count_dict(documents: DocumentNormalizedList) -> Dict[str, int]:\n",
    "    \"\"\"Counter the word in all documents at least ones\"\"\"\n",
    "    counts = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            _value = counts.get(word, 0)\n",
    "            counts[word] = _value + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def idf_dict(documents: DocumentNormalizedList) -> IDF:\n",
    "    \"\"\"Inverse Document Frequency in all Documents\"\"\"\n",
    "    idf_d = dict()\n",
    "\n",
    "    counts = count_dict(documents)\n",
    "\n",
    "    for word in counts:\n",
    "        idf_d[word] = math.log(len(documents) / counts[word])\n",
    "\n",
    "    return idf_d\n",
    "\n",
    "\n",
    "def comments_tfidf(documents: DocumentNormalizedList) -> TFIDF_List:\n",
    "    tfidf_comments = []\n",
    "\n",
    "    idf_comments = idf_dict(documents)\n",
    "\n",
    "    def compute_tfidf_comment(document: DocumentList) -> TFIDF:\n",
    "        tfidf_comment = dict()\n",
    "\n",
    "        tf_comment = comment_tf(document)\n",
    "\n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "\n",
    "        return tfidf_comment\n",
    "\n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "\n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "tfidf_list = comments_tfidf(documents_normalized)\n",
    "# tfidf_list\n",
    "print('TF-IDF comments list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "unordered_tfidf: Dict[str, float] = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for w, value in tfidf.items():\n",
    "        if w in unordered_tfidf:\n",
    "            unordered_tfidf[w] += value\n",
    "        else:\n",
    "            unordered_tfidf[w] = value\n",
    "\n",
    "\n",
    "def sort_tfidf(tfidf_unordered) -> Iterable[Tuple[Any, Any]]:\n",
    "    return sorted(tfidf_unordered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# unordered_tfidf\n",
    "ordered_comments_tfidf = OrderedDict(sort_tfidf(unordered_tfidf))\n",
    "# Save to JSON file\n",
    "with open('comments_tfidf.json', 'w') as file_json:\n",
    "    json.dump(ordered_comments_tfidf, file_json)\n",
    "print('TF-IDF ordered saved to comments_tfidf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO: str = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK: str = 'https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg'\n",
    "\n",
    "# download mask images\n",
    "# !wget http://media.cubadebate.cu/wp-content/gallery/la-habana-nocturna/app_la-habana_05.jpg -O la_hababa.jpg\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, 'wb') as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print('Image downloaded.')\n",
    "    else:\n",
    "        print('Image No downloaded!')\n",
    "else:\n",
    "    image_downloaded: str = IMG_CAPITOLIO.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "    print(f\"Image: {image_downloaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = 'wordcloud_cubadebate.png'\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=1000,\n",
    "    #     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color='white',\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color='steelblue',\n",
    "    stopwords=STOP_WORDS).generate_from_frequencies(ordered_comments_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print('WordCloud Cubadebate image saved.')\n",
    "fig = plt.figure(figsize = (22, 15),)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1587742239903,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Gs20IRy8cvpQ",
    "outputId": "cb62b934-baf8-4f47-b9b0-b920740b9d4a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported image to html.\n"
     ]
    }
   ],
   "source": [
    "with open(IMG_WORDCLOUD, 'rb') as file_img:\n",
    "    data = base64.b64encode(file_img.read()).decode('utf-8')\n",
    "\n",
    "img_str = '''\n",
    "<img width=\"100%\" height=\"100%\" \n",
    "src=\"data:image/png;base64,{}\" />\n",
    "'''.format(data)\n",
    "\n",
    "with open('wordcloud_cubadebate.html', 'w') as _html:\n",
    "    _html.write(img_str)\n",
    "\n",
    "print('Exported image to html.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
