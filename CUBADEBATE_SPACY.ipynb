{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17056,
     "status": "ok",
     "timestamp": 1587740583735,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "2uwNZ8LNlcXZ",
    "outputId": "d17cd409-2981-4c77-ea3a-0df6e11a47c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordCloud installed.\n",
      "Spacy es_core_news_model installed.\n",
      "Restart the runtime!\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print('WordCloud installed.')\n",
    "print('Spacy es_core_news_model installed.\\nRestart the runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Qi1yZ1EbYpQ"
   },
   "source": [
    "Go to Restart runtime..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 7.45s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import dask\n",
    "import dask.delayed as delayed\n",
    "import dask.bag as db\n",
    "\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionError = requests.exceptions.ConnectionError\n",
    "\n",
    "@delayed\n",
    "def get_comments_json(page=1):\n",
    "    params = {\"page\": str(page)}\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            response = session.get(COMMENTS_ENDPOINT,\n",
    "                                   params=params)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json()\n",
    "                return results\n",
    "    except ConnectionError as error:\n",
    "        pass\n",
    "        #print(f\"Try again! Ocurred Connection Error: {error}\")\n",
    "    return results\n",
    "        \n",
    "start = time.time()\n",
    "comments_list = list(*dask.compute(get_comments_json()))\n",
    "end = time.time()\n",
    "print(f'{len(comments_list)} Comments downloaded in {(end - start):.2f}s')\n",
    "#comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990 Comments downloaded in 396.30s\n"
     ]
    }
   ],
   "source": [
    "def get_comments(pages):\n",
    "    comments_delayed = (get_comments_json(page)\n",
    "                        for page in range(1, pages+1))\n",
    "                        \n",
    "    comments = db.from_delayed(comments_delayed).map(lambda d: d['content']['rendered'])\n",
    "\n",
    "    return comments\n",
    "\n",
    "NUM_PAGES=100\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#In Windows the Dask.Bag is multiprocessing by default chage to threads\n",
    "documents = get_comments(NUM_PAGES).compute(scheduler='threads') #type: List[str]\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'{len(documents)} Comments downloaded in {(end - start):.2f}s')\n",
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20379,
     "status": "ok",
     "timestamp": 1587741838415,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "b4r5kXdGlK8r",
    "outputId": "c9245c95-647a-4e15-efb6-3b06ae130184",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990 Documents normalized in 279.34s\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def remplace_accents(text):\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r'á', 'a', text, flags=re.I)\n",
    "    text = re.sub(r'é', 'e', text, flags=re.I)\n",
    "    text = re.sub(r'í', 'i', text, flags=re.I)\n",
    "    text = re.sub(r'ó', 'o', text, flags=re.I)\n",
    "    text = re.sub(r'ú', 'u', text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "def get_text(markup):\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', markup)\n",
    "    url_pattern = r'(http|ftp)s?://(?:[a-zA-Z]|                   [0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(url_pattern,'', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "@delayed\n",
    "def clean(doc):\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "\n",
    "    def is_token_allowed(token):\n",
    "      \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "      if (not token or not token.string.strip() or\n",
    "          token.is_stop or token.is_punct or len(token) < 3):\n",
    "        return False\n",
    "      return True\n",
    "\n",
    "    def preprocess_token(token):\n",
    "      \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "      result = remplace_accents(token.lemma_.strip().lower())\n",
    "      return result\n",
    "\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = [preprocess_token(word) for word in nlp(text)\n",
    "              if is_token_allowed(word)]\n",
    "    normalized = \" \".join(word for word in tokens)\n",
    "    return normalized.split()\n",
    "\n",
    "def get_documents_normalized(documents):\n",
    "    \"\"\"Return List[List[str]] clean document\"\"\"\n",
    "    results = [] #type: List[dask.Delayed]\n",
    "    \n",
    "    for document in documents:\n",
    "        results.append(clean(document))\n",
    "    \n",
    "    return results\n",
    "\n",
    "#Process documents using Dask.delayed\n",
    "documents_normalized = [] #type: List[List[str]]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "documents_normalized = list(*dask.compute(get_documents_normalized(documents)))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'{len(documents_normalized)} Documents normalized in {(end - start):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5335,
     "status": "ok",
     "timestamp": 1587741858758,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1Dce5BcYlK88",
    "outputId": "c47b6fdf-d088-4f58-a67f-1698c65f258a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF comments list\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def comment_tf_dict(document):\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "    \n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "    \n",
    "    return tf_dict\n",
    "\n",
    "def comments_tf_dict(documents):\n",
    "    return [comment_tf_dict(comment)\n",
    "            for comment in documents]\n",
    "\n",
    "def count_dict(documents):\n",
    "    counts = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            value = counts.get(word, 0)\n",
    "            counts[word] = value + 1\n",
    "    return counts\n",
    "\n",
    "def idf_dict(documents):\n",
    "    idf_dict = dict()\n",
    "    \n",
    "    counts = count_dict(documents)\n",
    "    \n",
    "    for word in counts:\n",
    "        idf_dict[word] = math.log(len(documents) / counts[word])\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "def comments_tfidf_dict(documents):\n",
    "    tfidf_comments = []\n",
    "    \n",
    "    idf_comments = idf_dict(documents)\n",
    "    \n",
    "    def compute_tfidf_comment(document):\n",
    "        tfidf_comment = dict()\n",
    "        \n",
    "        tf_comment = comment_tf_dict(document)\n",
    "        \n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "        \n",
    "        return tfidf_comment\n",
    "    \n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "    \n",
    "    \n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "tfidf_list = comments_tfidf_dict(documents_normalized)\n",
    "#tfidf_list\n",
    "print('TF-IDF comments list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "ordered_tfidf = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for word, value in tfidf.items():\n",
    "        if word in ordered_tfidf:\n",
    "            ordered_tfidf[word] += value\n",
    "        else:\n",
    "            ordered_tfidf[word] = value\n",
    "\n",
    "#ordered_tfidf\n",
    "ordered_tfidf = OrderedDict(sorted(ordered_tfidf.items(), key=lambda x: x[1], reverse=True))\n",
    "# Save to JSON file\n",
    "with open('comments_tfidf.json', 'w') as file_json:\n",
    "    json.dump(ordered_tfidf, file_json)\n",
    "print('TF-IDF ordered saved to comments_tfidf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK = 'https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg'\n",
    "\n",
    "# download mask images\n",
    "#!wget http://media.cubadebate.cu/wp-content/gallery/la-habana-nocturna/app_la-habana_05.jpg -O la_hababa.jpg\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, 'wb') as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print('Image downloaded.')\n",
    "    else:\n",
    "        print('Image No downloaded!')\n",
    "else:\n",
    "    image_downloaded = IMG_CAPITOLIO.split('\\\\')[-1].split('/')[-1]\n",
    "    print(f\"Image: {image_downloaded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = 'wordcloud_cubadebate.png'\n",
    "# Spacy Stop_Words\n",
    "STOP_WORDS = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=1000,\n",
    "#     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color = 'white',\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color='steelblue',\n",
    "    stopwords = STOP_WORDS).generate_from_frequencies(ordered_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print('WordCloud Cubadebate image saved.')\n",
    "fig = plt.figure(\n",
    "    figsize = (22, 15),\n",
    "    )\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1587742239903,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Gs20IRy8cvpQ",
    "outputId": "cb62b934-baf8-4f47-b9b0-b920740b9d4a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported image to html.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "data = ''\n",
    "with open(IMG_WORDCLOUD, 'rb') as file_img:\n",
    "    data = base64.b64encode(file_img.read()).decode('utf-8')\n",
    "\n",
    "img_str = '''\n",
    "<img width=\"100%\" height=\"100%\" \n",
    "src=\"data:image/png;base64,{}\" />\n",
    "'''.format(data)\n",
    "\n",
    "with open('wordcloud_cubadebate.html', 'w') as _html:\n",
    "    _html.write(img_str)\n",
    "\n",
    "print('Exported image to html.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
