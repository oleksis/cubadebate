{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import Counter, OrderedDict\n",
    "from datetime import datetime\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from string import Template\n",
    "import time\n",
    "from typing import cast, List, Optional, Dict, Tuple, Iterable, Any\n",
    "\n",
    "import requests\n",
    "import dask\n",
    "from dask import bag as db\n",
    "from dask import dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Annotations\n",
    "ElementList = List[Optional[Dict[str, Any]]]\n",
    "Document = str\n",
    "DocumentList = List[Document]\n",
    "DocumentNormalizedList = List[DocumentList]\n",
    "TermFrequency = Dict[str, float]\n",
    "IDF = Dict[str, float]\n",
    "TFIDF = Dict[str, float]\n",
    "TFIDF_List = List[TFIDF]\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "SEARCH_ENDPOINT = CUBADEBATE_API + \"search/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionErrorRequests = requests.exceptions.ConnectionError\n",
    "\n",
    "FECHA_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "\n",
    "def create_datetime(string: str) -> Optional[datetime]:\n",
    "    \"\"\"Create datetime from string '2014-07-03T23:27:51'\"\"\"\n",
    "    date_time = None\n",
    "\n",
    "    try:\n",
    "        if string is not None:\n",
    "            date_time = datetime.strptime(string, FECHA_FORMAT)\n",
    "    except ValueError:\n",
    "        print(\"Format no valid!\")\n",
    "\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def save_elements_json(name: str, line: str = None, mode=\"w\"):\n",
    "    \"\"\"Save elements to file, by lines JSON elments.\"\"\"\n",
    "    with open(name, mode) as _file:\n",
    "        if line:\n",
    "            _file.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def read_elements_json(name: str) -> Optional[list]:\n",
    "    \"\"\"Read by lines JSON data in file name.\n",
    "    Return list of elements or None\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(name, \"r\") as _file:\n",
    "            for line in _file.readlines():\n",
    "                if line:\n",
    "                    data += json.loads(line)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"Error reading lines of _file: {name}\")\n",
    "    except FileNotFoundError as f_error:\n",
    "        print(f_error)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_comments_file(name: str) -> DataFrame:\n",
    "    \"\"\"Get List of comments from file name\"\"\"\n",
    "    comments_json = read_elements_json(name)\n",
    "    comments_bag = db.from_sequence(comments_json).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[str] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_elements_json(url: str, **kwargs) -> ElementList:\n",
    "    \"\"\"Get JSON of list elements from endpoint API Wordpress v2\n",
    "\n",
    "    RETURNS (ElementList): Delayed Response List of elements in JSON format\n",
    "    to compute for Dask.Bag\n",
    "    \"\"\"\n",
    "    params = {\"page\": \"1\"}\n",
    "    params.update(kwargs)\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            resp = session.get(url, params=params)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json()\n",
    "                if params.get(\"file\"):\n",
    "                    save_elements_json(\n",
    "                        params.get(\"file\", \"elements.dat\"), resp.text, \"a\"\n",
    "                    )\n",
    "                return results\n",
    "    except ConnectionErrorRequests:\n",
    "        pass  # Try again! Occurred Connection Error\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_comments(pages: int) -> DataFrame:\n",
    "    \"\"\"Get List of comments per page\"\"\"\n",
    "    comments_delayed = (\n",
    "        get_elements_json(url=COMMENTS_ENDPOINT, page=str(page), file=\"comments.dat\")\n",
    "        for page in range(1, pages + 1)\n",
    "    )\n",
    "\n",
    "    comments_bag = db.from_delayed(comments_delayed).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[dict] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "def get_searches(words: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get List of searches one page\"\"\"\n",
    "    search_delayed = (\n",
    "        get_elements_json(url=SEARCH_ENDPOINT, search=word) for word in words\n",
    "    )\n",
    "\n",
    "    searches_bag = db.from_delayed(search_delayed)\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        searches = searches_bag.compute()\n",
    "    return searches\n",
    "\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "bow_lemma_token: Dict[str, Token] = dict()\n",
    "\n",
    "\n",
    "def remplace_accents(text: str) -> str:\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r\"á\", \"a\", text, flags=re.I)\n",
    "    text = re.sub(r\"é\", \"e\", text, flags=re.I)\n",
    "    text = re.sub(r\"í\", \"i\", text, flags=re.I)\n",
    "    text = re.sub(r\"ó\", \"o\", text, flags=re.I)\n",
    "    text = re.sub(r\"ú\", \"u\", text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(markup: str) -> str:\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", markup)\n",
    "    url_pattern = r\"(http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_token(token: Token) -> str:\n",
    "    \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "    result = remplace_accents(token.lemma_.strip().lower())\n",
    "    return result\n",
    "\n",
    "\n",
    "def is_token_allowed(token: Token) -> bool:\n",
    "    \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "    # Avoid token: inmiscuyéndose lemma_ \"inmiscuir el\"\n",
    "    if (\n",
    "        not token\n",
    "        or token.is_space\n",
    "        or token.is_stop\n",
    "        or token.is_punct\n",
    "        or len(token) < 3\n",
    "        or \" \" in token.lemma_.strip()\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean(doc: str) -> List[str]:\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = []\n",
    "\n",
    "    for token in nlp(text):\n",
    "        if is_token_allowed(token):\n",
    "            word_lemma = preprocess_token(token)\n",
    "            bow_lemma_token[word_lemma] = token\n",
    "            tokens.append(word_lemma)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_ngram(document: DocumentList, ngram=1) -> DocumentList:\n",
    "    \"\"\"Create N-Gram from document\"\"\"\n",
    "    return [\n",
    "        \" \".join(document[i : i + ngram]) for i in range(len(document) - (ngram - 1))\n",
    "    ]\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "def comment_tf(document: DocumentList) -> TermFrequency:\n",
    "    \"\"\"Term Frequency for a document\"\"\"\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "\n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def comments_tf(documents: DocumentNormalizedList) -> List[TermFrequency]:\n",
    "    \"\"\"Term Frequency for many document\"\"\"\n",
    "    return [comment_tf(comment) for comment in documents]\n",
    "\n",
    "\n",
    "def count_dict(documents: DocumentNormalizedList) -> Dict[str, int]:\n",
    "    \"\"\"Counter the word in all documents at least ones\"\"\"\n",
    "    counts: Dict[str, int] = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            _value = counts.get(word, 0)\n",
    "            counts[word] = _value + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def idf_dict(documents: DocumentNormalizedList) -> IDF:\n",
    "    \"\"\"Inverse Document Frequency in all Documents\"\"\"\n",
    "    idf_d = dict()\n",
    "\n",
    "    counts = count_dict(documents)\n",
    "\n",
    "    for word in counts:\n",
    "        idf_d[word] = math.log(len(documents) / counts[word])\n",
    "\n",
    "    return idf_d\n",
    "\n",
    "\n",
    "def comments_tfidf(documents: DocumentNormalizedList) -> TFIDF_List:\n",
    "    \"\"\"TF-IDF of all Documents Normalized\"\"\"\n",
    "    tfidf_comments = []\n",
    "\n",
    "    idf_comments = idf_dict(documents)\n",
    "\n",
    "    def compute_tfidf_comment(document: DocumentList) -> TFIDF:\n",
    "        \"\"\"Compute TF-IDF for a Document\"\"\"\n",
    "        tfidf_comment = dict()\n",
    "\n",
    "        tf_comment = comment_tf(document)\n",
    "\n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "\n",
    "        return tfidf_comment\n",
    "\n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "\n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "def sort_tfidf(tfidf_unordered) -> Iterable[Tuple[Any, Any]]:\n",
    "    return sorted(tfidf_unordered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def export_image2html(image_name: str) -> None:\n",
    "    \"\"\"Export a image to html file\"\"\"\n",
    "    with open(image_name, \"rb\") as file_img:\n",
    "        data = base64.b64encode(file_img.read()).decode(\"utf-8\")\n",
    "\n",
    "    img_str = \"\"\"\n",
    "    <img width=\"100%\" height=\"100%\"\n",
    "    src=\"data:image/png;base64,{}\" />\n",
    "    \"\"\".format(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    with open(\"wordcloud_cubadebate.html\", \"w\") as _html:\n",
    "        _html.write(img_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "## Download Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 13.64s\n"
     ]
    }
   ],
   "source": [
    "_start = time.time()\n",
    "df_comments =  get_comments(pages=1)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# df_comments.info()\n",
    "# df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Comments downloaded in 401.77s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9174963</td>\n",
       "      <td>1518497</td>\n",
       "      <td>ArielGM</td>\n",
       "      <td>2021-03-23 21:18:56</td>\n",
       "      <td>&lt;p&gt;Ser o no ser...Si lo vacunaron dentro del f...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9174955</td>\n",
       "      <td>1518497</td>\n",
       "      <td>Maria isabel</td>\n",
       "      <td>2021-03-23 21:17:56</td>\n",
       "      <td>&lt;p&gt;Como saber si soy alérgica al timerosal y c...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9174909</td>\n",
       "      <td>1518497</td>\n",
       "      <td>Guajiro del monte</td>\n",
       "      <td>2021-03-23 21:06:44</td>\n",
       "      <td>&lt;p&gt;Cuando llevaran el tema de la epidemiología...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9174907</td>\n",
       "      <td>1518497</td>\n",
       "      <td>Pedro Pablo</td>\n",
       "      <td>2021-03-23 21:06:09</td>\n",
       "      <td>&lt;p&gt;La doctora  explicó que los 44000 que parti...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9174889</td>\n",
       "      <td>1518497</td>\n",
       "      <td>Guajiro del monte</td>\n",
       "      <td>2021-03-23 21:01:43</td>\n",
       "      <td>&lt;p&gt;Gran logro de la ciencia cubana, no es un m...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9169753</td>\n",
       "      <td>1517403</td>\n",
       "      <td>JG</td>\n",
       "      <td>2021-03-22 19:15:22</td>\n",
       "      <td>&lt;p&gt;Gracias Buena Fe&lt;/p&gt;\\n</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/21/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9169751</td>\n",
       "      <td>1517773</td>\n",
       "      <td>David</td>\n",
       "      <td>2021-03-22 19:15:01</td>\n",
       "      <td>&lt;p&gt;Cuando terminen los ensayos clínicos. Es qu...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/22/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>9169747</td>\n",
       "      <td>1517753</td>\n",
       "      <td>El médico</td>\n",
       "      <td>2021-03-22 19:14:04</td>\n",
       "      <td>&lt;p&gt;Es muy injusto.Se favorece a Matanzas.Estoy...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/22/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9169745</td>\n",
       "      <td>1517403</td>\n",
       "      <td>Josep</td>\n",
       "      <td>2021-03-22 19:13:58</td>\n",
       "      <td>&lt;p&gt;Asimismo Lazarito VIVA CUBA LIBRE&lt;/p&gt;\\n</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/21/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9169743</td>\n",
       "      <td>1517343</td>\n",
       "      <td>elsie rabago</td>\n",
       "      <td>2021-03-22 19:13:43</td>\n",
       "      <td>&lt;p&gt;Pero en tu envío cdo lo encargan del exteri...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post        author_name                date  \\\n",
       "0    9174963  1518497            ArielGM 2021-03-23 21:18:56   \n",
       "1    9174955  1518497       Maria isabel 2021-03-23 21:17:56   \n",
       "2    9174909  1518497  Guajiro del monte 2021-03-23 21:06:44   \n",
       "3    9174907  1518497        Pedro Pablo 2021-03-23 21:06:09   \n",
       "4    9174889  1518497  Guajiro del monte 2021-03-23 21:01:43   \n",
       "..       ...      ...                ...                 ...   \n",
       "995  9169753  1517403                 JG 2021-03-22 19:15:22   \n",
       "996  9169751  1517773              David 2021-03-22 19:15:01   \n",
       "997  9169747  1517753          El médico 2021-03-22 19:14:04   \n",
       "998  9169745  1517403              Josep 2021-03-22 19:13:58   \n",
       "999  9169743  1517343       elsie rabago 2021-03-22 19:13:43   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Ser o no ser...Si lo vacunaron dentro del f...   \n",
       "1    <p>Como saber si soy alérgica al timerosal y c...   \n",
       "2    <p>Cuando llevaran el tema de la epidemiología...   \n",
       "3    <p>La doctora  explicó que los 44000 que parti...   \n",
       "4    <p>Gran logro de la ciencia cubana, no es un m...   \n",
       "..                                                 ...   \n",
       "995                          <p>Gracias Buena Fe</p>\\n   \n",
       "996  <p>Cuando terminen los ensayos clínicos. Es qu...   \n",
       "997  <p>Es muy injusto.Se favorece a Matanzas.Estoy...   \n",
       "998         <p>Asimismo Lazarito VIVA CUBA LIBRE</p>\\n   \n",
       "999  <p>Pero en tu envío cdo lo encargan del exteri...   \n",
       "\n",
       "                                                  link  \n",
       "0    http://www.cubadebate.cu/noticias/2021/03/23/c...  \n",
       "1    http://www.cubadebate.cu/noticias/2021/03/23/c...  \n",
       "2    http://www.cubadebate.cu/noticias/2021/03/23/c...  \n",
       "3    http://www.cubadebate.cu/noticias/2021/03/23/c...  \n",
       "4    http://www.cubadebate.cu/noticias/2021/03/23/c...  \n",
       "..                                                 ...  \n",
       "995  http://www.cubadebate.cu/noticias/2021/03/21/b...  \n",
       "996  http://www.cubadebate.cu/noticias/2021/03/22/c...  \n",
       "997  http://www.cubadebate.cu/noticias/2021/03/22/p...  \n",
       "998  http://www.cubadebate.cu/noticias/2021/03/21/b...  \n",
       "999  http://www.cubadebate.cu/especiales/2021/03/22...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate file comments.dat\n",
    "save_elements_json(\"comments.dat\")\n",
    "NUM_PAGES = 100\n",
    "\n",
    "_start = time.time()\n",
    "df_comments = get_comments(pages=NUM_PAGES)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not df_comments.empty, \"No comments to process\"\n",
    "# Uncomment the next for get the comments from file\n",
    "# df_comments = get_comments_file(\"comments.dat\")\n",
    "# print(f\"{len(df_comments)} Comments from file comments.dat\")\n",
    "# df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process corpus. Normalize. Using threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Documents normalized in 232.92s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9174963</td>\n",
       "      <td>1518497</td>\n",
       "      <td>ArielGM</td>\n",
       "      <td>2021-03-23 21:18:56</td>\n",
       "      <td>&lt;p&gt;Ser o no ser...Si lo vacunaron dentro del f...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "      <td>[vacunar, fase, poner, placebo, antigeno, medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9174955</td>\n",
       "      <td>1518497</td>\n",
       "      <td>Maria isabel</td>\n",
       "      <td>2021-03-23 21:17:56</td>\n",
       "      <td>&lt;p&gt;Como saber si soy alérgica al timerosal y c...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "      <td>[alergica, timerosal, tocar, ensayo, provincia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9174909</td>\n",
       "      <td>1518497</td>\n",
       "      <td>Guajiro del monte</td>\n",
       "      <td>2021-03-23 21:06:44</td>\n",
       "      <td>&lt;p&gt;Cuando llevaran el tema de la epidemiología...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "      <td>[llevar, tema, epidemiologia, mesa, redondo, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9174907</td>\n",
       "      <td>1518497</td>\n",
       "      <td>Pedro Pablo</td>\n",
       "      <td>2021-03-23 21:06:09</td>\n",
       "      <td>&lt;p&gt;La doctora  explicó que los 44000 que parti...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "      <td>[doctora, 44000, participar, fase, iii, sobera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9174889</td>\n",
       "      <td>1518497</td>\n",
       "      <td>Guajiro del monte</td>\n",
       "      <td>2021-03-23 21:01:43</td>\n",
       "      <td>&lt;p&gt;Gran logro de la ciencia cubana, no es un m...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/23/c...</td>\n",
       "      <td>[logro, ciencia, cubano, milagro, humano, eter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9169753</td>\n",
       "      <td>1517403</td>\n",
       "      <td>JG</td>\n",
       "      <td>2021-03-22 19:15:22</td>\n",
       "      <td>&lt;p&gt;Gracias Buena Fe&lt;/p&gt;\\n</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/21/b...</td>\n",
       "      <td>[gracia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9169751</td>\n",
       "      <td>1517773</td>\n",
       "      <td>David</td>\n",
       "      <td>2021-03-22 19:15:01</td>\n",
       "      <td>&lt;p&gt;Cuando terminen los ensayos clínicos. Es qu...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/22/c...</td>\n",
       "      <td>[terminir, ensayo, clinico, acaso, ensayar, pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>9169747</td>\n",
       "      <td>1517753</td>\n",
       "      <td>El médico</td>\n",
       "      <td>2021-03-22 19:14:04</td>\n",
       "      <td>&lt;p&gt;Es muy injusto.Se favorece a Matanzas.Estoy...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/22/p...</td>\n",
       "      <td>[injusto.se, favorecer, matanzas.estoy, apoyar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9169745</td>\n",
       "      <td>1517403</td>\n",
       "      <td>Josep</td>\n",
       "      <td>2021-03-22 19:13:58</td>\n",
       "      <td>&lt;p&gt;Asimismo Lazarito VIVA CUBA LIBRE&lt;/p&gt;\\n</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/21/b...</td>\n",
       "      <td>[asimismo, lazarito, vivo, cuba, libre]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9169743</td>\n",
       "      <td>1517343</td>\n",
       "      <td>elsie rabago</td>\n",
       "      <td>2021-03-22 19:13:43</td>\n",
       "      <td>&lt;p&gt;Pero en tu envío cdo lo encargan del exteri...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/22...</td>\n",
       "      <td>[envio, cdoir, encarguir, exterior, traer, mal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post        author_name                date  \\\n",
       "0    9174963  1518497            ArielGM 2021-03-23 21:18:56   \n",
       "1    9174955  1518497       Maria isabel 2021-03-23 21:17:56   \n",
       "2    9174909  1518497  Guajiro del monte 2021-03-23 21:06:44   \n",
       "3    9174907  1518497        Pedro Pablo 2021-03-23 21:06:09   \n",
       "4    9174889  1518497  Guajiro del monte 2021-03-23 21:01:43   \n",
       "..       ...      ...                ...                 ...   \n",
       "995  9169753  1517403                 JG 2021-03-22 19:15:22   \n",
       "996  9169751  1517773              David 2021-03-22 19:15:01   \n",
       "997  9169747  1517753          El médico 2021-03-22 19:14:04   \n",
       "998  9169745  1517403              Josep 2021-03-22 19:13:58   \n",
       "999  9169743  1517343       elsie rabago 2021-03-22 19:13:43   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Ser o no ser...Si lo vacunaron dentro del f...   \n",
       "1    <p>Como saber si soy alérgica al timerosal y c...   \n",
       "2    <p>Cuando llevaran el tema de la epidemiología...   \n",
       "3    <p>La doctora  explicó que los 44000 que parti...   \n",
       "4    <p>Gran logro de la ciencia cubana, no es un m...   \n",
       "..                                                 ...   \n",
       "995                          <p>Gracias Buena Fe</p>\\n   \n",
       "996  <p>Cuando terminen los ensayos clínicos. Es qu...   \n",
       "997  <p>Es muy injusto.Se favorece a Matanzas.Estoy...   \n",
       "998         <p>Asimismo Lazarito VIVA CUBA LIBRE</p>\\n   \n",
       "999  <p>Pero en tu envío cdo lo encargan del exteri...   \n",
       "\n",
       "                                                  link  \\\n",
       "0    http://www.cubadebate.cu/noticias/2021/03/23/c...   \n",
       "1    http://www.cubadebate.cu/noticias/2021/03/23/c...   \n",
       "2    http://www.cubadebate.cu/noticias/2021/03/23/c...   \n",
       "3    http://www.cubadebate.cu/noticias/2021/03/23/c...   \n",
       "4    http://www.cubadebate.cu/noticias/2021/03/23/c...   \n",
       "..                                                 ...   \n",
       "995  http://www.cubadebate.cu/noticias/2021/03/21/b...   \n",
       "996  http://www.cubadebate.cu/noticias/2021/03/22/c...   \n",
       "997  http://www.cubadebate.cu/noticias/2021/03/22/p...   \n",
       "998  http://www.cubadebate.cu/noticias/2021/03/21/b...   \n",
       "999  http://www.cubadebate.cu/especiales/2021/03/22...   \n",
       "\n",
       "                                                  text  \n",
       "0    [vacunar, fase, poner, placebo, antigeno, medi...  \n",
       "1    [alergica, timerosal, tocar, ensayo, provincia...  \n",
       "2    [llevar, tema, epidemiologia, mesa, redondo, t...  \n",
       "3    [doctora, 44000, participar, fase, iii, sobera...  \n",
       "4    [logro, ciencia, cubano, milagro, humano, eter...  \n",
       "..                                                 ...  \n",
       "995                                           [gracia]  \n",
       "996  [terminir, ensayo, clinico, acaso, ensayar, pe...  \n",
       "997  [injusto.se, favorecer, matanzas.estoy, apoyar...  \n",
       "998            [asimismo, lazarito, vivo, cuba, libre]  \n",
       "999  [envio, cdoir, encarguir, exterior, traer, mal...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process corpus. Normalize. Using threads\n",
    "_start = time.time()\n",
    "ddf_comments = dd.from_pandas(df_comments, chunksize=10)\n",
    "df_comments[\"text\"] = (\n",
    "    cast(dd.core.Series, ddf_comments[\"content\"])\n",
    "    .apply(clean, meta=(\"content\", \"object\"))\n",
    "    .compute()\n",
    ")\n",
    "documents_normalized = list(df_comments[\"text\"].values)\n",
    "_end = time.time()\n",
    "print(f\"{len(documents_normalized)} Documents normalized in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Comments list\n",
    "tfidf_list = comments_tfidf(documents_normalized)\n",
    "\n",
    "unordered_tfidf: Dict[str, float] = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf[w] = unordered_tfidf.get(w, 0) + value\n",
    "\n",
    "\n",
    "# unordered_tfidf\n",
    "ordered_comments_tfidf = OrderedDict(sort_tfidf(unordered_tfidf))\n",
    "\n",
    "# WordCloud with word_token bigrams (ngram=2)\n",
    "token_comments_tfidf = dict()\n",
    "unordered_tfidf_ngram: Dict[str, float] = dict()\n",
    "documents_normalized_ngram = list()\n",
    "\n",
    "for doc_norm in documents_normalized:\n",
    "    documents_normalized_ngram.append(create_ngram(doc_norm, ngram=2))\n",
    "\n",
    "tfidf_list_ngram = comments_tfidf(documents_normalized_ngram)\n",
    "\n",
    "for tfidf in tfidf_list_ngram:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf_ngram[w] = unordered_tfidf_ngram.get(w, 0) + value\n",
    "\n",
    "ordered_comments_tfidf_ngram = OrderedDict(sort_tfidf(unordered_tfidf_ngram))\n",
    "\n",
    "for lemma_, value in ordered_comments_tfidf_ngram.items():\n",
    "    _token = \" \".join([str(bow_lemma_token[lm]) for lm in lemma_.split(\" \")])\n",
    "    token_comments_tfidf[_token] = value\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"comments_tfidf.json\", \"w\") as file_json:\n",
    "    json.dump(ordered_comments_tfidf_ngram, file_json)\n",
    "print(\"TF-IDF ordered saved to comments_tfidf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Habana Capitolio Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO: str = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example\n",
    "# in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK: str = (\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg\"\n",
    ")\n",
    "\n",
    "# download mask images\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, \"wb\") as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print(\"Image downloaded.\")\n",
    "    else:\n",
    "        print(\"Image No downloaded!\")\n",
    "else:\n",
    "    image_downloaded: str = IMG_CAPITOLIO.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "    print(f\"Image: {image_downloaded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordsCloud Cubadebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = \"wordcloud_cubadebate.png\"\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=500,\n",
    "    #     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color=\"white\",\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color=\"steelblue\",\n",
    "    stopwords=STOP_WORDS,\n",
    ").generate_from_frequencies(token_comments_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print(\"WordCloud Cubadebate image saved.\\n\")\n",
    "fig = plt.figure(figsize = (22, 15),)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Words (Lemma) with most TF-IDF (unigram: ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda DataFrame shape:(6428, 2)\n",
      "GroupBy DataFrame shape:(18, 2)\n",
      "\n",
      "    TF-IDF          Word\n",
      "0       50     felicidad\n",
      "1       41    presidente\n",
      "2       16        precio\n",
      "3       14       ciencia\n",
      "4       13          cuba\n",
      "5       12  felicitacion\n",
      "6       11       comprar\n",
      "7       10           ver\n",
      "8        9       orgullo\n",
      "9        8    produccion\n",
      "10       7         pesos\n",
      "11       6        idiota\n",
      "12       5         carne\n",
      "13       4      realidad\n",
      "14       3       palabra\n",
      "15       2         calle\n",
      "16       1         queja\n",
      "17       0      caracter\n",
      "\n",
      "Top Word Token and Post\n",
      "\n",
      "                   id                                              title  \\\n",
      "precio        1518247                        El alto precio de construir   \n",
      "cuba          1518755  Las razones del voluntario con quien inició la...   \n",
      "ver           1518363  ¿Anexión de Cuba? Un neoconservador estadounid...   \n",
      "presidente    1518269  Dr. Eulogio Pimentel, vicepresidente de BioCub...   \n",
      "ciencia       1517477  Gobernar con la Ciencia y con Todos: ¡1 000 cu...   \n",
      "felicidad     1516825  Día Internacional de la Felicidad: ¿Qué nos ha...   \n",
      "palabra       1516747  La danza de las palabras en la luz: un pacto e...   \n",
      "orgullo       1514959                           Un orgullo de ser cubano   \n",
      "calle         1514143  Manifestantes otra vez a las calles en Londres...   \n",
      "felicitacion  1513501  Esteban Lazo envía carta de felicitación a per...   \n",
      "caracter      1512627  Tarea Ordenamiento: Nuevas medidas de carácter...   \n",
      "realidad      1510351  Serie Madame C. J. Walker, entre ficción y rea...   \n",
      "produccion    1506519  SoundCloud pagará a los artistas en función de...   \n",
      "comprar       1495609  COVID-19 en el mundo: Venezuela creará fondo c...   \n",
      "pesos         1479939  Gobierno cubano otorgará nuevo financiamiento ...   \n",
      "idiota        1439781  Trump critica al doctor Fauci, experto en COVI...   \n",
      "queja         1419381  Consejo de Defensa de La Habana analiza quejas...   \n",
      "carne         1409317  Recibió Elián su carnet como militante del Par...   \n",
      "\n",
      "                                                            url        date  \n",
      "precio        http://www.cubadebate.cu/especiales/2021/03/23...  2021/03/23  \n",
      "cuba          http://www.cubadebate.cu/especiales/2021/03/23...  2021/03/23  \n",
      "ver           http://www.cubadebate.cu/especiales/2021/03/23...  2021/03/23  \n",
      "presidente    http://www.cubadebate.cu/especiales/2021/03/23...  2021/03/23  \n",
      "ciencia       http://www.cubadebate.cu/opinion/2021/03/22/go...  2021/03/22  \n",
      "felicidad     http://www.cubadebate.cu/fotorreportajes/2021/...  2021/03/20  \n",
      "palabra       http://www.cubadebate.cu/especiales/2021/03/20...  2021/03/20  \n",
      "orgullo       http://www.cubadebate.cu/opinion/2021/03/16/un...  2021/03/16  \n",
      "calle         http://www.cubadebate.cu/noticias/2021/03/15/m...  2021/03/15  \n",
      "felicitacion  http://www.cubadebate.cu/noticias/2021/03/14/e...  2021/03/14  \n",
      "caracter      http://www.cubadebate.cu/noticias/2021/03/12/t...  2021/03/12  \n",
      "realidad      http://www.cubadebate.cu/especiales/2021/03/09...  2021/03/09  \n",
      "produccion    http://www.cubadebate.cu/noticias/2021/03/02/s...  2021/03/02  \n",
      "comprar       http://www.cubadebate.cu/noticias/2021/02/12/c...  2021/02/12  \n",
      "pesos         http://www.cubadebate.cu/noticias/2021/01/11/g...  2021/01/11  \n",
      "idiota        http://www.cubadebate.cu/noticias/2020/10/19/t...  2020/10/19  \n",
      "queja         http://www.cubadebate.cu/noticias/2020/09/05/c...  2020/09/05  \n",
      "carne         http://www.cubadebate.cu/noticias/2020/08/14/r...  2020/08/14  \n",
      "\n",
      "Saved top_word_post.json\n",
      "\n",
      "Rewrite index.html.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Words (Lemma) with most TF-IDF (unigram: ngram=1)\n",
    "df = (\n",
    "    DataFrame.from_dict(\n",
    "        data=ordered_comments_tfidf, dtype=int, orient=\"index\", columns=[\"TF-IDF\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(index=str, columns={\"index\": \"Word\"})\n",
    ")\n",
    "rows, columns = df.shape\n",
    "print(f\"Panda DataFrame shape:({rows}, {columns})\")\n",
    "\n",
    "df_gb = df.groupby([\"TF-IDF\"], sort=False).first().reset_index()\n",
    "rows, columns = df_gb.shape\n",
    "print(f\"GroupBy DataFrame shape:({rows}, {columns})\\n\")\n",
    "print(df_gb)\n",
    "\n",
    "words_token = list(df_gb[\"Word\"].values)\n",
    "searches_dict = dict()\n",
    "searches_ids = set()\n",
    "\n",
    "for w_token in words_token:\n",
    "    search_list = get_searches([w_token])\n",
    "    # Get first searched result\n",
    "    if len(search_list) > 0:\n",
    "        search_id = search_list[0][\"id\"]\n",
    "        if search_id not in searches_ids:\n",
    "            searches_ids.add(search_id)\n",
    "            searches_dict[w_token] = search_list[0]\n",
    "\n",
    "# DataFrame First Search Posts\n",
    "if len(searches_dict) > 0:\n",
    "    searches_df = DataFrame.from_dict(\n",
    "        data=searches_dict,\n",
    "        orient=\"index\",\n",
    "    ).drop(columns=[\"type\", \"subtype\", \"_links\"])\n",
    "\n",
    "    df_dates = searches_df[\"url\"].str.extract(r\"(?P<date>\\d{4}/\\d{2}/\\d{2})\")\n",
    "\n",
    "    searches_by_dates = searches_df.join(df_dates).sort_values(\n",
    "        by=\"date\", ascending=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop Word Token and Post\\n\")\n",
    "    print(searches_by_dates)\n",
    "    searches_by_dates.to_json(os.path.join(_dir, \"top_word_post.json\"))\n",
    "    print(\"\\nSaved top_word_post.json\\n\")\n",
    "\n",
    "    if os.path.isfile(\"index.tpl\"):\n",
    "        with open(\"index.html\", \"w\", encoding=\"utf-8\") as f_index, open(\n",
    "            \"index.tpl\", \"r\"\n",
    "        ) as file:\n",
    "            tpl = file.read()\n",
    "            index_template = Template(tpl)\n",
    "\n",
    "            text_link = \"<ul>\\n\"\n",
    "            for _index, _title, _url in searches_by_dates.reset_index()[\n",
    "                [\"index\", \"title\", \"url\"]\n",
    "            ].values:\n",
    "                text_link += (\n",
    "                    f\"\\t<li><a href='{_url}' rel='external' \"\n",
    "                    f\"data-token='{_index}'>{_title}</a></li>\\n\"\n",
    "                )\n",
    "            text_link += \"</ul>\"\n",
    "            # Create index.html from index.tpl\n",
    "            f_index.write(index_template.substitute(CUBADEBATE_LINKS=text_link))\n",
    "        print(\"Rewrite index.html.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open** [index.html](index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
