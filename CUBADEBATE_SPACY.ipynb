{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import Counter, OrderedDict\n",
    "from datetime import datetime\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from string import Template\n",
    "import time\n",
    "from typing import List, Optional, Dict, Tuple, Iterable, Any\n",
    "\n",
    "import requests\n",
    "import dask\n",
    "from dask import bag as db\n",
    "from dask import dataframe as dd\n",
    "from dask.delayed import delayed, Delayed\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Annotations\n",
    "ElementList = List[Optional[Dict[str, Any]]]\n",
    "Document = str\n",
    "DocumentList = List[Document]\n",
    "DocumentNormalizedList = List[DocumentList]\n",
    "TermFrequency = Dict[str, float]\n",
    "IDF = Dict[str, float]\n",
    "TFIDF = Dict[str, float]\n",
    "TFIDF_List = List[TFIDF]\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "SEARCH_ENDPOINT = CUBADEBATE_API + \"search/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionErrorRequests = requests.exceptions.ConnectionError\n",
    "\n",
    "FECHA_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "\n",
    "def create_datetime(string: str) -> Optional[datetime]:\n",
    "    \"\"\"Create datetime from string '2014-07-03T23:27:51'\"\"\"\n",
    "    date_time = None\n",
    "\n",
    "    try:\n",
    "        if string is not None:\n",
    "            date_time = datetime.strptime(string, FECHA_FORMAT)\n",
    "    except ValueError:\n",
    "        print(\"Format no valid!\")\n",
    "\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def save_elements_json(name: str, line: str = None, mode=\"w\"):\n",
    "    \"\"\"Save elements to file, by lines JSON elments.\"\"\"\n",
    "    with open(name, mode) as _file:\n",
    "        if line:\n",
    "            _file.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def read_elements_json(name: str) -> Optional[list]:\n",
    "    \"\"\"Read by lines JSON data in file name.\n",
    "    Return list of elements or None\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(name, \"r\") as _file:\n",
    "            for line in _file.readlines():\n",
    "                if line:\n",
    "                    data += json.loads(line)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"Error reading lines of _file: {name}\")\n",
    "    except FileNotFoundError as f_error:\n",
    "        print(f_error)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_comments_file(name: str) -> DataFrame:\n",
    "    \"\"\"Get List of comments from file name\"\"\"\n",
    "    comments_json = read_elements_json(name)\n",
    "    comments_bag = db.from_sequence(comments_json).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[str] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_elements_json(url: str, **kwargs) -> ElementList:\n",
    "    \"\"\"Get JSON of list elements from endpoint API Wordpress v2\n",
    "\n",
    "    RETURNS (ElementList): Delayed Response List of elements in JSON format\n",
    "    to compute for Dask.Bag\n",
    "    \"\"\"\n",
    "    params = {\"page\": \"1\"}\n",
    "    params.update(kwargs)\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            resp = session.get(url, params=params)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json()\n",
    "                if params.get(\"file\"):\n",
    "                    save_elements_json(params.get(\"file\"), resp.text, \"a\")\n",
    "                return results\n",
    "    except ConnectionErrorRequests:\n",
    "        pass  # Try again! Occurred Connection Error\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_comments(pages: int) -> DataFrame:\n",
    "    \"\"\"Get List of comments per page\"\"\"\n",
    "    comments_delayed = (\n",
    "        get_elements_json(url=COMMENTS_ENDPOINT, page=str(page), file=\"comments.dat\")\n",
    "        for page in range(1, pages + 1)\n",
    "    )\n",
    "\n",
    "    comments_bag = db.from_delayed(comments_delayed).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[dict] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "def get_searches(words: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get List of searches one page\"\"\"\n",
    "    search_delayed = (\n",
    "        get_elements_json(url=SEARCH_ENDPOINT, search=word) for word in words\n",
    "    )\n",
    "\n",
    "    searches_bag = db.from_delayed(search_delayed)\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        searches = searches_bag.compute()\n",
    "    return searches\n",
    "\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "bow_lemma_token: Dict[str, Token] = dict()\n",
    "\n",
    "\n",
    "def remplace_accents(text: str) -> str:\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r\"á\", \"a\", text, flags=re.I)\n",
    "    text = re.sub(r\"é\", \"e\", text, flags=re.I)\n",
    "    text = re.sub(r\"í\", \"i\", text, flags=re.I)\n",
    "    text = re.sub(r\"ó\", \"o\", text, flags=re.I)\n",
    "    text = re.sub(r\"ú\", \"u\", text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(markup: str) -> str:\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", markup)\n",
    "    url_pattern = r\"(http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_token(token: Token) -> str:\n",
    "    \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "    result = remplace_accents(token.lemma_.strip().lower())\n",
    "    return result\n",
    "\n",
    "\n",
    "def is_token_allowed(token: Token) -> bool:\n",
    "    \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "    if (\n",
    "        not token\n",
    "        or not token.string.strip()\n",
    "        or token.is_stop\n",
    "        or token.is_punct\n",
    "        or len(token) < 3\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean(doc: str) -> List[str]:\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = []\n",
    "\n",
    "    for token in nlp(text):\n",
    "        if is_token_allowed(token):\n",
    "            word_lemma = preprocess_token(token)\n",
    "            bow_lemma_token[word_lemma] = token\n",
    "            tokens.append(word_lemma)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_ngram(document: DocumentList, ngram=1) -> DocumentList:\n",
    "    \"\"\"Create N-Gram from document\"\"\"\n",
    "    return [\n",
    "        \" \".join(document[i : i + ngram]) for i in range(len(document) - (ngram - 1))\n",
    "    ]\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "def comment_tf(document: DocumentList) -> TermFrequency:\n",
    "    \"\"\"Term Frequency for a document\"\"\"\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "\n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def comments_tf(documents: DocumentNormalizedList) -> List[TermFrequency]:\n",
    "    \"\"\"Term Frequency for many document\"\"\"\n",
    "    return [comment_tf(comment) for comment in documents]\n",
    "\n",
    "\n",
    "def count_dict(documents: DocumentNormalizedList) -> Dict[str, int]:\n",
    "    \"\"\"Counter the word in all documents at least ones\"\"\"\n",
    "    counts = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            _value = counts.get(word, 0)\n",
    "            counts[word] = _value + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def idf_dict(documents: DocumentNormalizedList) -> IDF:\n",
    "    \"\"\"Inverse Document Frequency in all Documents\"\"\"\n",
    "    idf_d = dict()\n",
    "\n",
    "    counts = count_dict(documents)\n",
    "\n",
    "    for word in counts:\n",
    "        idf_d[word] = math.log(len(documents) / counts[word])\n",
    "\n",
    "    return idf_d\n",
    "\n",
    "\n",
    "def comments_tfidf(documents: DocumentNormalizedList) -> TFIDF_List:\n",
    "    \"\"\"TF-IDF of all Documents Normalized\"\"\"\n",
    "    tfidf_comments = []\n",
    "\n",
    "    idf_comments = idf_dict(documents)\n",
    "\n",
    "    def compute_tfidf_comment(document: DocumentList) -> TFIDF:\n",
    "        \"\"\"Compute TF-IDF for a Document\"\"\"\n",
    "        tfidf_comment = dict()\n",
    "\n",
    "        tf_comment = comment_tf(document)\n",
    "\n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "\n",
    "        return tfidf_comment\n",
    "\n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "\n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "def sort_tfidf(tfidf_unordered) -> Iterable[Tuple[Any, Any]]:\n",
    "    return sorted(tfidf_unordered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def export_image2html(image_name: str) -> None:\n",
    "    \"\"\"Export a image to html file\"\"\"\n",
    "    with open(image_name, \"rb\") as file_img:\n",
    "        data = base64.b64encode(file_img.read()).decode(\"utf-8\")\n",
    "\n",
    "    img_str = \"\"\"\n",
    "    <img width=\"100%\" height=\"100%\" \n",
    "    src=\"data:image/png;base64,{}\" />\n",
    "    \"\"\".format(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    with open(\"wordcloud_cubadebate.html\", \"w\") as _html:\n",
    "        _html.write(img_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "## Download Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 9.90s\n"
     ]
    }
   ],
   "source": [
    "_start = time.time()\n",
    "df_comments =  get_comments(pages=1)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# df_comments.info()\n",
    "# df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990 Comments downloaded in 451.28s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7977897</td>\n",
       "      <td>1388847</td>\n",
       "      <td>Ana Betsy Sollet Romero</td>\n",
       "      <td>2020-07-03 17:15:28</td>\n",
       "      <td>&lt;p&gt;Como se llama la aplicación para sacar los ...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/06/29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7977891</td>\n",
       "      <td>1391313</td>\n",
       "      <td>Luisa Mercedes</td>\n",
       "      <td>2020-07-03 17:14:25</td>\n",
       "      <td>&lt;p&gt;Magnifico programa pero lamentablemente poc...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/07/03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7977885</td>\n",
       "      <td>1391205</td>\n",
       "      <td>Yoi</td>\n",
       "      <td>2020-07-03 17:12:55</td>\n",
       "      <td>&lt;p&gt;¿Pero están locos?... y sus padres también?...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7977879</td>\n",
       "      <td>1391215</td>\n",
       "      <td>Yayi</td>\n",
       "      <td>2020-07-03 17:10:48</td>\n",
       "      <td>&lt;p&gt;Gracias Jaqueline por tu regreso y dejar ke...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/07/03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7977877</td>\n",
       "      <td>1390947</td>\n",
       "      <td>adys Sitrans</td>\n",
       "      <td>2020-07-03 17:10:13</td>\n",
       "      <td>&lt;p&gt;Actualmente está en desarrollo la versión w...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/02/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>7972385</td>\n",
       "      <td>1390531</td>\n",
       "      <td>aperez</td>\n",
       "      <td>2020-07-02 14:07:49</td>\n",
       "      <td>&lt;p&gt;El gobierno en Estados Unidos dirigido por ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/02/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>7972381</td>\n",
       "      <td>1390647</td>\n",
       "      <td>Justicia</td>\n",
       "      <td>2020-07-02 14:06:08</td>\n",
       "      <td>&lt;p&gt;Pero que inmadurez los de los británicos ,a...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/02/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>7972379</td>\n",
       "      <td>1390445</td>\n",
       "      <td>Maritza Sastre</td>\n",
       "      <td>2020-07-02 14:05:55</td>\n",
       "      <td>&lt;p&gt;El servicio de Gacelas se reanudará con el ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/01/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>7972375</td>\n",
       "      <td>1390689</td>\n",
       "      <td>Eduardo</td>\n",
       "      <td>2020-07-02 14:05:37</td>\n",
       "      <td>&lt;p&gt;Q bueno lo que ocurrió en Rusia espero que ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/02/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>7972371</td>\n",
       "      <td>1390397</td>\n",
       "      <td>Rolando Escalona García</td>\n",
       "      <td>2020-07-02 14:05:00</td>\n",
       "      <td>&lt;p&gt;Sé debería de esperar al menos algunos días...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/01/l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post              author_name                date  \\\n",
       "0    7977897  1388847  Ana Betsy Sollet Romero 2020-07-03 17:15:28   \n",
       "1    7977891  1391313           Luisa Mercedes 2020-07-03 17:14:25   \n",
       "2    7977885  1391205                      Yoi 2020-07-03 17:12:55   \n",
       "3    7977879  1391215                     Yayi 2020-07-03 17:10:48   \n",
       "4    7977877  1390947             adys Sitrans 2020-07-03 17:10:13   \n",
       "..       ...      ...                      ...                 ...   \n",
       "985  7972385  1390531                   aperez 2020-07-02 14:07:49   \n",
       "986  7972381  1390647                 Justicia 2020-07-02 14:06:08   \n",
       "987  7972379  1390445           Maritza Sastre 2020-07-02 14:05:55   \n",
       "988  7972375  1390689                  Eduardo 2020-07-02 14:05:37   \n",
       "989  7972371  1390397  Rolando Escalona García 2020-07-02 14:05:00   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Como se llama la aplicación para sacar los ...   \n",
       "1    <p>Magnifico programa pero lamentablemente poc...   \n",
       "2    <p>¿Pero están locos?... y sus padres también?...   \n",
       "3    <p>Gracias Jaqueline por tu regreso y dejar ke...   \n",
       "4    <p>Actualmente está en desarrollo la versión w...   \n",
       "..                                                 ...   \n",
       "985  <p>El gobierno en Estados Unidos dirigido por ...   \n",
       "986  <p>Pero que inmadurez los de los británicos ,a...   \n",
       "987  <p>El servicio de Gacelas se reanudará con el ...   \n",
       "988  <p>Q bueno lo que ocurrió en Rusia espero que ...   \n",
       "989  <p>Sé debería de esperar al menos algunos días...   \n",
       "\n",
       "                                                  link  \n",
       "0    http://www.cubadebate.cu/especiales/2020/06/29...  \n",
       "1    http://www.cubadebate.cu/especiales/2020/07/03...  \n",
       "2    http://www.cubadebate.cu/noticias/2020/07/03/e...  \n",
       "3    http://www.cubadebate.cu/especiales/2020/07/03...  \n",
       "4    http://www.cubadebate.cu/noticias/2020/07/02/d...  \n",
       "..                                                 ...  \n",
       "985  http://www.cubadebate.cu/noticias/2020/07/02/c...  \n",
       "986  http://www.cubadebate.cu/noticias/2020/07/02/c...  \n",
       "987  http://www.cubadebate.cu/noticias/2020/07/01/l...  \n",
       "988  http://www.cubadebate.cu/noticias/2020/07/02/r...  \n",
       "989  http://www.cubadebate.cu/noticias/2020/07/01/l...  \n",
       "\n",
       "[990 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate file comments.dat\n",
    "save_elements_json(\"comments.dat\")\n",
    "NUM_PAGES = 100\n",
    "\n",
    "_start = time.time()\n",
    "df_comments = get_comments(pages=NUM_PAGES)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not df_comments.empty, \"No comments to process\"\n",
    "# Uncomment the next for get the comments from file\n",
    "# df_comments = get_comments_file(\"comments.dat\")\n",
    "# print(f\"{len(df_comments)} Comments from file comments.dat\")\n",
    "# df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process corpus. Normalize. Using threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990 Documents normalized in 699.58s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7977897</td>\n",
       "      <td>1388847</td>\n",
       "      <td>Ana Betsy Sollet Romero</td>\n",
       "      <td>2020-07-03 17:15:28</td>\n",
       "      <td>&lt;p&gt;Como se llama la aplicación para sacar los ...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/06/29...</td>\n",
       "      <td>[llamar, aplicacion, sacar, pasaje, casar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7977891</td>\n",
       "      <td>1391313</td>\n",
       "      <td>Luisa Mercedes</td>\n",
       "      <td>2020-07-03 17:14:25</td>\n",
       "      <td>&lt;p&gt;Magnifico programa pero lamentablemente poc...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/07/03...</td>\n",
       "      <td>[magnificar, programar, lamentablemente, conoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7977885</td>\n",
       "      <td>1391205</td>\n",
       "      <td>Yoi</td>\n",
       "      <td>2020-07-03 17:12:55</td>\n",
       "      <td>&lt;p&gt;¿Pero están locos?... y sus padres también?...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/e...</td>\n",
       "      <td>[loco, padre, acaso, andar, hijo, ocurrir, pai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7977879</td>\n",
       "      <td>1391215</td>\n",
       "      <td>Yayi</td>\n",
       "      <td>2020-07-03 17:10:48</td>\n",
       "      <td>&lt;p&gt;Gracias Jaqueline por tu regreso y dejar ke...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/07/03...</td>\n",
       "      <td>[gracia, jaqueline, regresar, dejar, poder, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7977877</td>\n",
       "      <td>1390947</td>\n",
       "      <td>adys Sitrans</td>\n",
       "      <td>2020-07-03 17:10:13</td>\n",
       "      <td>&lt;p&gt;Actualmente está en desarrollo la versión w...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/02/d...</td>\n",
       "      <td>[desarrollar, version, web, viajar, preferir, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>7972385</td>\n",
       "      <td>1390531</td>\n",
       "      <td>aperez</td>\n",
       "      <td>2020-07-02 14:07:49</td>\n",
       "      <td>&lt;p&gt;El gobierno en Estados Unidos dirigido por ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/02/c...</td>\n",
       "      <td>[gobernar, unir, dirigir, hombre, peligroso, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>7972381</td>\n",
       "      <td>1390647</td>\n",
       "      <td>Justicia</td>\n",
       "      <td>2020-07-02 14:06:08</td>\n",
       "      <td>&lt;p&gt;Pero que inmadurez los de los británicos ,a...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/02/c...</td>\n",
       "      <td>[inmadurez, britanico, acaso, ojo, vendar, ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>7972379</td>\n",
       "      <td>1390445</td>\n",
       "      <td>Maritza Sastre</td>\n",
       "      <td>2020-07-02 14:05:55</td>\n",
       "      <td>&lt;p&gt;El servicio de Gacelas se reanudará con el ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/01/l...</td>\n",
       "      <td>[servicio, gacela, reanudar, 100%, capacidad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>7972375</td>\n",
       "      <td>1390689</td>\n",
       "      <td>Eduardo</td>\n",
       "      <td>2020-07-02 14:05:37</td>\n",
       "      <td>&lt;p&gt;Q bueno lo que ocurrió en Rusia espero que ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/02/r...</td>\n",
       "      <td>[ocurrir, rusia, esperar, implicar, libertar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>7972371</td>\n",
       "      <td>1390397</td>\n",
       "      <td>Rolando Escalona García</td>\n",
       "      <td>2020-07-02 14:05:00</td>\n",
       "      <td>&lt;p&gt;Sé debería de esperar al menos algunos días...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/01/l...</td>\n",
       "      <td>[deber, esperar, abrir, habano]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post              author_name                date  \\\n",
       "0    7977897  1388847  Ana Betsy Sollet Romero 2020-07-03 17:15:28   \n",
       "1    7977891  1391313           Luisa Mercedes 2020-07-03 17:14:25   \n",
       "2    7977885  1391205                      Yoi 2020-07-03 17:12:55   \n",
       "3    7977879  1391215                     Yayi 2020-07-03 17:10:48   \n",
       "4    7977877  1390947             adys Sitrans 2020-07-03 17:10:13   \n",
       "..       ...      ...                      ...                 ...   \n",
       "985  7972385  1390531                   aperez 2020-07-02 14:07:49   \n",
       "986  7972381  1390647                 Justicia 2020-07-02 14:06:08   \n",
       "987  7972379  1390445           Maritza Sastre 2020-07-02 14:05:55   \n",
       "988  7972375  1390689                  Eduardo 2020-07-02 14:05:37   \n",
       "989  7972371  1390397  Rolando Escalona García 2020-07-02 14:05:00   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Como se llama la aplicación para sacar los ...   \n",
       "1    <p>Magnifico programa pero lamentablemente poc...   \n",
       "2    <p>¿Pero están locos?... y sus padres también?...   \n",
       "3    <p>Gracias Jaqueline por tu regreso y dejar ke...   \n",
       "4    <p>Actualmente está en desarrollo la versión w...   \n",
       "..                                                 ...   \n",
       "985  <p>El gobierno en Estados Unidos dirigido por ...   \n",
       "986  <p>Pero que inmadurez los de los británicos ,a...   \n",
       "987  <p>El servicio de Gacelas se reanudará con el ...   \n",
       "988  <p>Q bueno lo que ocurrió en Rusia espero que ...   \n",
       "989  <p>Sé debería de esperar al menos algunos días...   \n",
       "\n",
       "                                                  link  \\\n",
       "0    http://www.cubadebate.cu/especiales/2020/06/29...   \n",
       "1    http://www.cubadebate.cu/especiales/2020/07/03...   \n",
       "2    http://www.cubadebate.cu/noticias/2020/07/03/e...   \n",
       "3    http://www.cubadebate.cu/especiales/2020/07/03...   \n",
       "4    http://www.cubadebate.cu/noticias/2020/07/02/d...   \n",
       "..                                                 ...   \n",
       "985  http://www.cubadebate.cu/noticias/2020/07/02/c...   \n",
       "986  http://www.cubadebate.cu/noticias/2020/07/02/c...   \n",
       "987  http://www.cubadebate.cu/noticias/2020/07/01/l...   \n",
       "988  http://www.cubadebate.cu/noticias/2020/07/02/r...   \n",
       "989  http://www.cubadebate.cu/noticias/2020/07/01/l...   \n",
       "\n",
       "                                                  text  \n",
       "0           [llamar, aplicacion, sacar, pasaje, casar]  \n",
       "1    [magnificar, programar, lamentablemente, conoc...  \n",
       "2    [loco, padre, acaso, andar, hijo, ocurrir, pai...  \n",
       "3    [gracia, jaqueline, regresar, dejar, poder, di...  \n",
       "4    [desarrollar, version, web, viajar, preferir, ...  \n",
       "..                                                 ...  \n",
       "985  [gobernar, unir, dirigir, hombre, peligroso, m...  \n",
       "986  [inmadurez, britanico, acaso, ojo, vendar, ver...  \n",
       "987      [servicio, gacela, reanudar, 100%, capacidad]  \n",
       "988  [ocurrir, rusia, esperar, implicar, libertar, ...  \n",
       "989                    [deber, esperar, abrir, habano]  \n",
       "\n",
       "[990 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process corpus. Normalize. Using threads\n",
    "_start = time.time()\n",
    "ddf_comments = dd.from_pandas(df_comments, chunksize=10)\n",
    "df_comments[\"text\"] = ddf_comments[\"content\"].apply(clean, meta=('content', 'object')).compute()\n",
    "documents_normalized = list(df_comments[\"text\"].values)\n",
    "_end = time.time()\n",
    "print(f\"{len(documents_normalized)} Documents normalized in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Comments list\n",
    "tfidf_list = comments_tfidf(documents_normalized)\n",
    "\n",
    "unordered_tfidf: Dict[str, float] = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf[w] = unordered_tfidf.get(w, 0) + value\n",
    "\n",
    "\n",
    "# unordered_tfidf\n",
    "ordered_comments_tfidf = OrderedDict(sort_tfidf(unordered_tfidf))\n",
    "\n",
    "# WordCloud with word_token bigrams (ngram=2)\n",
    "token_comments_tfidf = dict()\n",
    "unordered_tfidf_ngram: Dict[str, float] = dict()\n",
    "documents_normalized_ngram = list()\n",
    "\n",
    "for doc_norm in documents_normalized:\n",
    "    documents_normalized_ngram.append(create_ngram(doc_norm, ngram=2))\n",
    "\n",
    "tfidf_list_ngram = comments_tfidf(documents_normalized_ngram)\n",
    "\n",
    "for tfidf in tfidf_list_ngram:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf_ngram[w] = unordered_tfidf_ngram.get(w, 0) + value\n",
    "\n",
    "ordered_comments_tfidf_ngram = OrderedDict(sort_tfidf(unordered_tfidf_ngram))\n",
    "\n",
    "for lemma_, value in ordered_comments_tfidf_ngram.items():\n",
    "    _token = \" \".join([str(bow_lemma_token[lm]) for lm in lemma_.split(\" \")])\n",
    "    token_comments_tfidf[_token] = value\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"comments_tfidf.json\", \"w\") as file_json:\n",
    "    json.dump(ordered_comments_tfidf_ngram, file_json)\n",
    "print(\"TF-IDF ordered saved to comments_tfidf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Habana Capitolio Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO: str = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK: str = \"https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg\"\n",
    "\n",
    "# download mask images\n",
    "# !wget http://media.cubadebate.cu/wp-content/gallery/la-habana-nocturna/app_la-habana_05.jpg -O la_habana.jpg\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, \"wb\") as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print(\"Image downloaded.\")\n",
    "    else:\n",
    "        print(\"Image No downloaded!\")\n",
    "else:\n",
    "    image_downloaded: str = IMG_CAPITOLIO.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "    print(f\"Image: {image_downloaded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordsCloud Cubadebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = \"wordcloud_cubadebate.png\"\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=500,\n",
    "    #     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color=\"white\",\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color=\"steelblue\",\n",
    "    stopwords=STOP_WORDS,\n",
    ").generate_from_frequencies(token_comments_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print(\"WordCloud Cubadebate image saved.\\n\")\n",
    "fig = plt.figure(figsize = (22, 15),)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Words (Lemma) with most TF-IDF (unigram: ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda DataFrame shape:(4604, 2)\n",
      "GroupBy DataFrame shape:(23, 2)\n",
      "\n",
      "    TF-IDF         Word\n",
      "0       45       gracia\n",
      "1       35       apoyar\n",
      "2       25       habano\n",
      "3       22         fase\n",
      "4       21       pasaje\n",
      "5       20      comprar\n",
      "6       16    funcionar\n",
      "7       15        salir\n",
      "8       14          apk\n",
      "9       13      esperar\n",
      "10      12     personar\n",
      "11      11     contacto\n",
      "12      10        abrir\n",
      "13       9  informacion\n",
      "14       8      viajero\n",
      "15       7        ganar\n",
      "16       6          app\n",
      "17       5  posibilidad\n",
      "18       4          2da\n",
      "19       3       seguro\n",
      "20       2       valido\n",
      "21       1     libertar\n",
      "22       0      mundial\n",
      "\n",
      "Top Word Token and Post\n",
      "\n",
      "                  id                                              title  \\\n",
      "pasaje       1390947  Comienza este viernes la venta online de pasaj...   \n",
      "ganar        1390129  Prueba tus conocimientos deportivos y puedes g...   \n",
      "fase         1390397  La Habana pasa a la primera fase de recuperaci...   \n",
      "abrir        1389719  Unión Europea acuerda reabrir sus fronteras a ...   \n",
      "app          1389711  Disponible para iOS y Android el primer pack d...   \n",
      "mundial      1389409  Ding aplasta a Nakamura y se cita con el campe...   \n",
      "gracia       1387861  Luego de 23 años, gemelos se encuentran en Bra...   \n",
      "seguro       1381549  Alcaldesa de Seattle a Trump: Haznos seguros a...   \n",
      "posibilidad  1380545  COVID-19 en el mundo: La OMS descarta posibili...   \n",
      "apoyar       1379305  Parten de Lombardía médicos cubanos que apoyar...   \n",
      "esperar      1377507  Fidel: “No es posible esperar, pues mañana pod...   \n",
      "funcionar    1368463  Demócratas investigan despido de funcionario q...   \n",
      "salir        1368631  VALIENTES: Maydolis \"De la Covid vamos a salir...   \n",
      "informacion  1368353  Cuba celebra el Día Mundial de las Telecomunic...   \n",
      "comprar      1351941  Paso a paso: ¿Cómo comprar a través de la plat...   \n",
      "personar     1350093           COVID-19: Flashazos internacionales (IX)   \n",
      "viajero      1344935  VALIENTES: La doctora que recibe a los viajero...   \n",
      "contacto     1335197  Pacientes que mantuvieron contacto con los tre...   \n",
      "habano       1327213  Abre en La Habana la edición 22 del Festival d...   \n",
      "valido       1190893  Cuba revalidó el título de la Copa Panamerican...   \n",
      "libertar     1025591  Tributo hoy a Martí, organizador de la gesta l...   \n",
      "2da           885767      En el vórtice del Béisbol Cubano (2da. parte)   \n",
      "\n",
      "                                                           url        date  \n",
      "pasaje       http://www.cubadebate.cu/noticias/2020/07/02/d...  2020/07/02  \n",
      "ganar        http://www.cubadebate.cu/noticias/2020/07/01/p...  2020/07/01  \n",
      "fase         http://www.cubadebate.cu/noticias/2020/07/01/l...  2020/07/01  \n",
      "abrir        http://www.cubadebate.cu/noticias/2020/06/30/u...  2020/06/30  \n",
      "app          http://www.cubadebate.cu/noticias/2020/06/30/d...  2020/06/30  \n",
      "mundial      http://www.cubadebate.cu/noticias/2020/06/29/d...  2020/06/29  \n",
      "gracia       http://www.cubadebate.cu/noticias/2020/06/26/l...  2020/06/26  \n",
      "seguro       http://www.cubadebate.cu/noticias/2020/06/12/a...  2020/06/12  \n",
      "posibilidad  http://www.cubadebate.cu/noticias/2020/06/10/c...  2020/06/10  \n",
      "apoyar       http://www.cubadebate.cu/noticias/2020/06/08/p...  2020/06/08  \n",
      "esperar      http://www.cubadebate.cu/especiales/2020/06/05...  2020/06/05  \n",
      "funcionar    http://www.cubadebate.cu/noticias/2020/05/17/d...  2020/05/17  \n",
      "salir        http://www.cubadebate.cu/noticias/2020/05/17/v...  2020/05/17  \n",
      "informacion  http://www.cubadebate.cu/especiales/2020/05/17...  2020/05/17  \n",
      "comprar      http://www.cubadebate.cu/especiales/2020/04/15...  2020/04/15  \n",
      "personar     http://www.cubadebate.cu/noticias/2020/04/11/c...  2020/04/11  \n",
      "viajero      http://www.cubadebate.cu/especiales/2020/04/01...  2020/04/01  \n",
      "contacto     http://www.cubadebate.cu/noticias/2020/03/12/p...  2020/03/12  \n",
      "habano       http://www.cubadebate.cu/cuba/2020/02/24/abre-...  2020/02/24  \n",
      "valido       http://www.cubadebate.cu/noticias/2019/04/30/c...  2019/04/30  \n",
      "libertar     http://www.cubadebate.cu/noticias/2018/02/24/t...  2018/02/24  \n",
      "2da          http://www.cubadebate.cu/opinion/2017/03/29/en...  2017/03/29  \n",
      "\n",
      "Saved top_word_post.json\n",
      "\n",
      "Rewrite index.html.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Words (Lemma) with most TF-IDF (unigram: ngram=1)\n",
    "df = (\n",
    "    DataFrame.from_dict(\n",
    "        data=ordered_comments_tfidf, dtype=int, orient=\"index\", columns=[\"TF-IDF\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(index=str, columns={\"index\": \"Word\"})\n",
    ")\n",
    "rows, columns = df.shape\n",
    "print(f\"Panda DataFrame shape:({rows}, {columns})\")\n",
    "\n",
    "df_gb = df.groupby([\"TF-IDF\"], sort=False).first().reset_index()\n",
    "rows, columns = df_gb.shape\n",
    "print(f\"GroupBy DataFrame shape:({rows}, {columns})\\n\")\n",
    "print(df_gb)\n",
    "\n",
    "words_token = list(df_gb[\"Word\"].values)\n",
    "searches_dict = dict()\n",
    "searches_ids = set()\n",
    "\n",
    "for w_token in words_token:\n",
    "    search_list = get_searches([w_token])\n",
    "    # Get first searched result\n",
    "    if len(search_list) > 0:\n",
    "        search_id = search_list[0][\"id\"]\n",
    "        if search_id not in searches_ids:\n",
    "            searches_ids.add(search_id)\n",
    "            searches_dict[w_token] = search_list[0]\n",
    "\n",
    "# DataFrame First Search Posts\n",
    "if len(searches_dict) > 0:\n",
    "    searches_df = DataFrame.from_dict(data=searches_dict, orient=\"index\",).drop(\n",
    "        columns=[\"type\", \"subtype\", \"_links\"]\n",
    "    )\n",
    "\n",
    "    df_dates = searches_df[\"url\"].str.extract(r\"(?P<date>\\d{4}/\\d{2}/\\d{2})\")\n",
    "\n",
    "    searches_by_dates = searches_df.join(df_dates).sort_values(\n",
    "        by=\"date\", ascending=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop Word Token and Post\\n\")\n",
    "    print(searches_by_dates)\n",
    "    searches_by_dates.to_json(os.path.join(_dir, \"top_word_post.json\"))\n",
    "    print(\"\\nSaved top_word_post.json\\n\")\n",
    "\n",
    "    with open(\"index.html\", \"w\", encoding=\"utf-8\") as f_index, open(\n",
    "        \"index.tpl\", \"r\"\n",
    "    ) as file:\n",
    "        tpl = file.read()\n",
    "        index_template = Template(tpl)\n",
    "\n",
    "        text_link = \"<ul>\\n\"\n",
    "        for _index, _title, _url in searches_by_dates.reset_index()[\n",
    "            [\"index\", \"title\", \"url\"]\n",
    "        ].values:\n",
    "            text_link += f\"\\t<li><a href='{_url}' rel='external' data-token='{_index}'>{_title}</a></li>\\n\"\n",
    "        text_link += \"</ul>\"\n",
    "        # Create index.html from index.tpl\n",
    "        f_index.write(index_template.substitute(CUBADEBATE_LINKS=text_link))\n",
    "\n",
    "    print(\"Rewrite index.html.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open** [index.html](index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
