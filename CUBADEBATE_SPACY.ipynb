{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17056,
     "status": "ok",
     "timestamp": 1587740583735,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "2uwNZ8LNlcXZ",
    "outputId": "d17cd409-2981-4c77-ea3a-0df6e11a47c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordCloud installed.\n",
      "Spacy es_core_news_model installed.\n",
      "Restart the runtime!\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print('WordCloud installed.')\n",
    "print('Spacy es_core_news_model installed.\\nRestart the runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Qi1yZ1EbYpQ"
   },
   "source": [
    "Go to Restart runtime..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def get_comments_json(page=1):\n",
    "    params = {\"page\": str(page)}\n",
    "    results = []\n",
    "    \n",
    "    with session:\n",
    "        time.sleep(1)\n",
    "        response = session.get(COMMENTS_ENDPOINT,\n",
    "                               params=params)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            return results\n",
    "    return results\n",
    "        \n",
    "comments_list = get_comments_json()\n",
    "print(len(comments_list))\n",
    "#comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15178,
     "status": "ok",
     "timestamp": 1587741814145,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "HPjBOXPMlK7w",
    "outputId": "1ec5b021-9962-47e0-e1bd-a6235195af83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "NUM_PAGES=100\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(get_comments_json, range(1,NUM_PAGES+1))\n",
    "    comments_list = list(results)\n",
    "    \n",
    "documents = [comment.get('content').get('rendered')\n",
    "          for comments in comments_list\n",
    "          for comment in comments]\n",
    "        \n",
    "print(len(documents))\n",
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20379,
     "status": "ok",
     "timestamp": 1587741838415,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "b4r5kXdGlK8r",
    "outputId": "c9245c95-647a-4e15-efb6-3b06ae130184",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents normalized\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def remplace_accents(text):\n",
    "    \"\"\"Remplace spanish accents vocab\"\"\"\n",
    "    text = re.sub(r'á', 'a', text)\n",
    "    text = re.sub(r'é', 'e', text)\n",
    "    text = re.sub(r'í', 'i', text)\n",
    "    text = re.sub(r'ó', 'o', text)\n",
    "    text = re.sub(r'ú', 'u', text)\n",
    "    return text\n",
    "\n",
    "def get_text(markup):\n",
    "    \"\"\"Remove html tags, URLs, grave accents and spaces using regexp\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', markup)\n",
    "    url_pattern = r'(http|ftp)s?://(?:[a-zA-Z]|\\\n",
    "                   [0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(url_pattern,'', text)\n",
    "    text = remplace_accents(text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean(doc):\n",
    "    \"\"\"Remove stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "\n",
    "    def is_token_allowed(token):\n",
    "      if (not token or not token.string.strip() or\n",
    "          token.is_stop or token.is_punct):\n",
    "        return False\n",
    "      return True\n",
    "\n",
    "    def preprocess_token(token):\n",
    "      return token.lemma_.strip().lower()\n",
    "\n",
    "    text = get_text(doc)\n",
    "    tokens = [preprocess_token(word) for word in nlp(text)\n",
    "              if is_token_allowed(word) and len(word) > 2]\n",
    "    normalized = \" \".join(word for word in tokens)\n",
    "    return normalized\n",
    "    \n",
    "\n",
    "documents_normalized = [clean(doc).split() for doc in documents]\n",
    "#documents_normalized\n",
    "print('Documents normalized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5335,
     "status": "ok",
     "timestamp": 1587741858758,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1Dce5BcYlK88",
    "outputId": "c47b6fdf-d088-4f58-a67f-1698c65f258a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF comments list\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def comment_tf_dict(document):\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "    \n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "    \n",
    "    return tf_dict\n",
    "\n",
    "def comments_tf_dict(documents):\n",
    "    return [comment_tf_dict(comment)\n",
    "            for comment in documents]\n",
    "\n",
    "def count_dict(documents):\n",
    "    counts = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            value = counts.get(word, 0)\n",
    "            counts[word] = value + 1\n",
    "    return counts\n",
    "\n",
    "def idf_dict(documents):\n",
    "    idf_dict = dict()\n",
    "    \n",
    "    counts = count_dict(documents)\n",
    "    \n",
    "    for word in counts:\n",
    "        idf_dict[word] = math.log(len(documents) / counts[word])\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "def comments_tfidf_dict(documents):\n",
    "    tfidf_comments = []\n",
    "    \n",
    "    idf_comments = idf_dict(documents)\n",
    "    \n",
    "    def compute_tfidf_comment(document):\n",
    "        tfidf_comment = dict()\n",
    "        \n",
    "        tf_comment = comment_tf_dict(document)\n",
    "        \n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "        \n",
    "        return tfidf_comment\n",
    "    \n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "    \n",
    "    \n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "tfidf_list = comments_tfidf_dict(documents_normalized)\n",
    "#tfidf_list\n",
    "print('TF-IDF comments list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "ordered_tfidf = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for word, value in tfidf.items():\n",
    "        if word in ordered_tfidf:\n",
    "            ordered_tfidf[word] += value\n",
    "        else:\n",
    "            ordered_tfidf[word] = value\n",
    "\n",
    "#ordered_tfidf\n",
    "ordered_tfidf = OrderedDict(sorted(ordered_tfidf.items(), key=lambda x: x[1], reverse=True))\n",
    "# Save to JSON file\n",
    "with open('comments_tfidf.json', 'w') as file_json:\n",
    "    json.dump(ordered_tfidf, file_json)\n",
    "print('TF-IDF ordered saved to comments_tfidf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36328,
     "status": "ok",
     "timestamp": 1587741915388,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "IJImAm8CaPuy",
    "outputId": "8a120b47-a3b2-4bad-d759-b3fe5280e677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "%env IMG_CAPITOLIO=capitolio.jpg\n",
    "\n",
    "\n",
    "# download mask images\n",
    "#!wget http://media.cubadebate.cu/wp-content/gallery/la-habana-nocturna/app_la-habana_05.jpg -O la_hababa.jpg\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg -O $IMG_CAPITOLIO\n",
    "\n",
    "clear_output()\n",
    "print('Image downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "IMG_WORDCLOUD = 'wordcloud_cubadebate.png'\n",
    "# Spacy Stop_Words\n",
    "STOP_WORDS = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "\n",
    "# read the mask image\n",
    "# taken from\n",
    "_mask = np.array(Image.open(os.path.join(_dir, IMG_CAPITOLIO)))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=1000,\n",
    "#     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color = 'white',\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color='steelblue',\n",
    "    stopwords = STOP_WORDS).generate_from_frequencies(ordered_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "fig = plt.figure(\n",
    "    figsize = (22, 15),\n",
    "    )\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1587742239903,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Gs20IRy8cvpQ",
    "outputId": "cb62b934-baf8-4f47-b9b0-b920740b9d4a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported image to html.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "data = ''\n",
    "with open(IMG_WORDCLOUD, 'rb') as file_img:\n",
    "    data = base64.b64encode(file_img.read()).decode('utf-8')\n",
    "\n",
    "img_str = '''\n",
    "<img width=\"100%\" height=\"100%\" \n",
    "src=\"data:image/png;base64,{}\" />\n",
    "'''.format(data)\n",
    "\n",
    "with open('wordcloud_cubadebate.html', 'w') as _html:\n",
    "    _html.write(img_str)\n",
    "\n",
    "print('Exported image to html.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2425,
     "status": "ok",
     "timestamp": 1587742258055,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "gZSQOAIcEpgO",
    "outputId": "a9552250-5cd7-4019-d971-80bf230fc761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capitolio.jpg\t     sample_data\t\twordcloud_cubadebate.png\n",
      "comments_tfidf.json  wordcloud_cubadebate.html\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
