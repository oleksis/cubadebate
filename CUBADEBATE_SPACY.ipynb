{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import Counter, OrderedDict\n",
    "from datetime import datetime\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from string import Template\n",
    "import time\n",
    "from typing import List, Optional, Dict, Tuple, Iterable, Any\n",
    "\n",
    "import requests\n",
    "import dask\n",
    "from dask import bag as db\n",
    "from dask import dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Annotations\n",
    "ElementList = List[Optional[Dict[str, Any]]]\n",
    "Document = str\n",
    "DocumentList = List[Document]\n",
    "DocumentNormalizedList = List[DocumentList]\n",
    "TermFrequency = Dict[str, float]\n",
    "IDF = Dict[str, float]\n",
    "TFIDF = Dict[str, float]\n",
    "TFIDF_List = List[TFIDF]\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "SEARCH_ENDPOINT = CUBADEBATE_API + \"search/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionErrorRequests = requests.exceptions.ConnectionError\n",
    "\n",
    "FECHA_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "\n",
    "def create_datetime(string: str) -> Optional[datetime]:\n",
    "    \"\"\"Create datetime from string '2014-07-03T23:27:51'\"\"\"\n",
    "    date_time = None\n",
    "\n",
    "    try:\n",
    "        if string is not None:\n",
    "            date_time = datetime.strptime(string, FECHA_FORMAT)\n",
    "    except ValueError:\n",
    "        print(\"Format no valid!\")\n",
    "\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def save_elements_json(name: str, line: str = None, mode=\"w\"):\n",
    "    \"\"\"Save elements to file, by lines JSON elments.\"\"\"\n",
    "    with open(name, mode) as _file:\n",
    "        if line:\n",
    "            _file.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def read_elements_json(name: str) -> Optional[list]:\n",
    "    \"\"\"Read by lines JSON data in file name.\n",
    "    Return list of elements or None\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(name, \"r\") as _file:\n",
    "            for line in _file.readlines():\n",
    "                if line:\n",
    "                    data += json.loads(line)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"Error reading lines of _file: {name}\")\n",
    "    except FileNotFoundError as f_error:\n",
    "        print(f_error)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_comments_file(name: str) -> DataFrame:\n",
    "    \"\"\"Get List of comments from file name\"\"\"\n",
    "    comments_json = read_elements_json(name)\n",
    "    comments_bag = db.from_sequence(comments_json).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[str] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_elements_json(url: str, **kwargs) -> ElementList:\n",
    "    \"\"\"Get JSON of list elements from endpoint API Wordpress v2\n",
    "\n",
    "    RETURNS (ElementList): Delayed Response List of elements in JSON format\n",
    "    to compute for Dask.Bag\n",
    "    \"\"\"\n",
    "    params = {\"page\": \"1\"}\n",
    "    params.update(kwargs)\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            resp = session.get(url, params=params)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json()\n",
    "                if params.get(\"file\"):\n",
    "                    save_elements_json(params.get(\"file\"), resp.text, \"a\")\n",
    "                return results\n",
    "    except ConnectionErrorRequests:\n",
    "        pass  # Try again! Occurred Connection Error\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_comments(pages: int) -> DataFrame:\n",
    "    \"\"\"Get List of comments per page\"\"\"\n",
    "    comments_delayed = (\n",
    "        get_elements_json(url=COMMENTS_ENDPOINT, page=str(page), file=\"comments.dat\")\n",
    "        for page in range(1, pages + 1)\n",
    "    )\n",
    "\n",
    "    comments_bag = db.from_delayed(comments_delayed).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[dict] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "def get_searches(words: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get List of searches one page\"\"\"\n",
    "    search_delayed = (\n",
    "        get_elements_json(url=SEARCH_ENDPOINT, search=word) for word in words\n",
    "    )\n",
    "\n",
    "    searches_bag = db.from_delayed(search_delayed)\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        searches = searches_bag.compute()\n",
    "    return searches\n",
    "\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "bow_lemma_token: Dict[str, Token] = dict()\n",
    "\n",
    "\n",
    "def remplace_accents(text: str) -> str:\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r\"á\", \"a\", text, flags=re.I)\n",
    "    text = re.sub(r\"é\", \"e\", text, flags=re.I)\n",
    "    text = re.sub(r\"í\", \"i\", text, flags=re.I)\n",
    "    text = re.sub(r\"ó\", \"o\", text, flags=re.I)\n",
    "    text = re.sub(r\"ú\", \"u\", text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(markup: str) -> str:\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", markup)\n",
    "    url_pattern = r\"(http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_token(token: Token) -> str:\n",
    "    \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "    result = remplace_accents(token.lemma_.strip().lower())\n",
    "    return result\n",
    "\n",
    "\n",
    "def is_token_allowed(token: Token) -> bool:\n",
    "    \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "    if (\n",
    "        not token\n",
    "        or not token.string.strip()\n",
    "        or token.is_stop\n",
    "        or token.is_punct\n",
    "        or len(token) < 3\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean(doc: str) -> List[str]:\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = []\n",
    "\n",
    "    for token in nlp(text):\n",
    "        if is_token_allowed(token):\n",
    "            word_lemma = preprocess_token(token)\n",
    "            bow_lemma_token[word_lemma] = token\n",
    "            tokens.append(word_lemma)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_ngram(document: DocumentList, ngram=1) -> DocumentList:\n",
    "    \"\"\"Create N-Gram from document\"\"\"\n",
    "    return [\n",
    "        \" \".join(document[i : i + ngram]) for i in range(len(document) - (ngram - 1))\n",
    "    ]\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "def comment_tf(document: DocumentList) -> TermFrequency:\n",
    "    \"\"\"Term Frequency for a document\"\"\"\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "\n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def comments_tf(documents: DocumentNormalizedList) -> List[TermFrequency]:\n",
    "    \"\"\"Term Frequency for many document\"\"\"\n",
    "    return [comment_tf(comment) for comment in documents]\n",
    "\n",
    "\n",
    "def count_dict(documents: DocumentNormalizedList) -> Dict[str, int]:\n",
    "    \"\"\"Counter the word in all documents at least ones\"\"\"\n",
    "    counts = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            _value = counts.get(word, 0)\n",
    "            counts[word] = _value + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def idf_dict(documents: DocumentNormalizedList) -> IDF:\n",
    "    \"\"\"Inverse Document Frequency in all Documents\"\"\"\n",
    "    idf_d = dict()\n",
    "\n",
    "    counts = count_dict(documents)\n",
    "\n",
    "    for word in counts:\n",
    "        idf_d[word] = math.log(len(documents) / counts[word])\n",
    "\n",
    "    return idf_d\n",
    "\n",
    "\n",
    "def comments_tfidf(documents: DocumentNormalizedList) -> TFIDF_List:\n",
    "    \"\"\"TF-IDF of all Documents Normalized\"\"\"\n",
    "    tfidf_comments = []\n",
    "\n",
    "    idf_comments = idf_dict(documents)\n",
    "\n",
    "    def compute_tfidf_comment(document: DocumentList) -> TFIDF:\n",
    "        \"\"\"Compute TF-IDF for a Document\"\"\"\n",
    "        tfidf_comment = dict()\n",
    "\n",
    "        tf_comment = comment_tf(document)\n",
    "\n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "\n",
    "        return tfidf_comment\n",
    "\n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "\n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "def sort_tfidf(tfidf_unordered) -> Iterable[Tuple[Any, Any]]:\n",
    "    return sorted(tfidf_unordered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def export_image2html(image_name: str) -> None:\n",
    "    \"\"\"Export a image to html file\"\"\"\n",
    "    with open(image_name, \"rb\") as file_img:\n",
    "        data = base64.b64encode(file_img.read()).decode(\"utf-8\")\n",
    "\n",
    "    img_str = \"\"\"\n",
    "    <img width=\"100%\" height=\"100%\"\n",
    "    src=\"data:image/png;base64,{}\" />\n",
    "    \"\"\".format(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    with open(\"wordcloud_cubadebate.html\", \"w\") as _html:\n",
    "        _html.write(img_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "## Download Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 18.97s\n"
     ]
    }
   ],
   "source": [
    "_start = time.time()\n",
    "df_comments =  get_comments(pages=1)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# df_comments.info()\n",
    "# df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Comments downloaded in 451.98s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7988241</td>\n",
       "      <td>1392451</td>\n",
       "      <td>Jorge Salazar Valdés</td>\n",
       "      <td>2020-07-06 13:02:09</td>\n",
       "      <td>&lt;p&gt;Lo que hace falta es que los precios bajen ...</td>\n",
       "      <td>http://www.cubadebate.cu/fotorreportajes/2020/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7988229</td>\n",
       "      <td>1391921</td>\n",
       "      <td>Jorge Luis García garcia</td>\n",
       "      <td>2020-07-06 13:00:17</td>\n",
       "      <td>&lt;p&gt;Llevó meses preguntándome como es posible q...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/07/06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7988215</td>\n",
       "      <td>1392377</td>\n",
       "      <td>Mary</td>\n",
       "      <td>2020-07-06 12:57:56</td>\n",
       "      <td>&lt;p&gt;Creo que la fase uno en la Habana ha sido u...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/06/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7988207</td>\n",
       "      <td>1392451</td>\n",
       "      <td>cubanito</td>\n",
       "      <td>2020-07-06 12:56:54</td>\n",
       "      <td>&lt;p&gt;porque en la habana con casos andando todav...</td>\n",
       "      <td>http://www.cubadebate.cu/fotorreportajes/2020/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7988205</td>\n",
       "      <td>1392377</td>\n",
       "      <td>Nabudoconosor</td>\n",
       "      <td>2020-07-06 12:56:29</td>\n",
       "      <td>&lt;p&gt;Eso es que se tomó un pepino de prevenghovi...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/06/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>7980711</td>\n",
       "      <td>1391385</td>\n",
       "      <td>libra</td>\n",
       "      <td>2020-07-04 13:00:44</td>\n",
       "      <td>&lt;p&gt;El tema de la carencia de medicamentos, com...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>7980707</td>\n",
       "      <td>1391621</td>\n",
       "      <td>Yandi Moreno Cruz</td>\n",
       "      <td>2020-07-04 13:00:24</td>\n",
       "      <td>&lt;p&gt;Creo q el calendario está muy bueno y tambi...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/04/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>7980705</td>\n",
       "      <td>1320307</td>\n",
       "      <td>Dairo Torres</td>\n",
       "      <td>2020-07-04 13:00:19</td>\n",
       "      <td>&lt;p&gt;Excelente producción. Buen profesionalismo....</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/02/10/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>7980703</td>\n",
       "      <td>1391695</td>\n",
       "      <td>CDMF</td>\n",
       "      <td>2020-07-04 12:59:56</td>\n",
       "      <td>&lt;p&gt;Quise decir medidas firmes para impedir la ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/04/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>7980697</td>\n",
       "      <td>1391695</td>\n",
       "      <td>Jorge R 09</td>\n",
       "      <td>2020-07-04 12:59:08</td>\n",
       "      <td>&lt;p&gt;Los índices sanitarios establecidos como re...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/04/c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post               author_name                date  \\\n",
       "0    7988241  1392451      Jorge Salazar Valdés 2020-07-06 13:02:09   \n",
       "1    7988229  1391921  Jorge Luis García garcia 2020-07-06 13:00:17   \n",
       "2    7988215  1392377                      Mary 2020-07-06 12:57:56   \n",
       "3    7988207  1392451                  cubanito 2020-07-06 12:56:54   \n",
       "4    7988205  1392377             Nabudoconosor 2020-07-06 12:56:29   \n",
       "..       ...      ...                       ...                 ...   \n",
       "995  7980711  1391385                     libra 2020-07-04 13:00:44   \n",
       "996  7980707  1391621         Yandi Moreno Cruz 2020-07-04 13:00:24   \n",
       "997  7980705  1320307              Dairo Torres 2020-07-04 13:00:19   \n",
       "998  7980703  1391695                      CDMF 2020-07-04 12:59:56   \n",
       "999  7980697  1391695                Jorge R 09 2020-07-04 12:59:08   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Lo que hace falta es que los precios bajen ...   \n",
       "1    <p>Llevó meses preguntándome como es posible q...   \n",
       "2    <p>Creo que la fase uno en la Habana ha sido u...   \n",
       "3    <p>porque en la habana con casos andando todav...   \n",
       "4    <p>Eso es que se tomó un pepino de prevenghovi...   \n",
       "..                                                 ...   \n",
       "995  <p>El tema de la carencia de medicamentos, com...   \n",
       "996  <p>Creo q el calendario está muy bueno y tambi...   \n",
       "997  <p>Excelente producción. Buen profesionalismo....   \n",
       "998  <p>Quise decir medidas firmes para impedir la ...   \n",
       "999  <p>Los índices sanitarios establecidos como re...   \n",
       "\n",
       "                                                  link  \n",
       "0    http://www.cubadebate.cu/fotorreportajes/2020/...  \n",
       "1    http://www.cubadebate.cu/especiales/2020/07/06...  \n",
       "2    http://www.cubadebate.cu/noticias/2020/07/06/c...  \n",
       "3    http://www.cubadebate.cu/fotorreportajes/2020/...  \n",
       "4    http://www.cubadebate.cu/noticias/2020/07/06/c...  \n",
       "..                                                 ...  \n",
       "995  http://www.cubadebate.cu/noticias/2020/07/03/m...  \n",
       "996  http://www.cubadebate.cu/noticias/2020/07/04/e...  \n",
       "997  http://www.cubadebate.cu/noticias/2020/02/10/e...  \n",
       "998  http://www.cubadebate.cu/noticias/2020/07/04/c...  \n",
       "999  http://www.cubadebate.cu/noticias/2020/07/04/c...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate file comments.dat\n",
    "save_elements_json(\"comments.dat\")\n",
    "NUM_PAGES = 100\n",
    "\n",
    "_start = time.time()\n",
    "df_comments = get_comments(pages=NUM_PAGES)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not df_comments.empty, \"No comments to process\"\n",
    "# Uncomment the next for get the comments from file\n",
    "# df_comments = get_comments_file(\"comments.dat\")\n",
    "# print(f\"{len(df_comments)} Comments from file comments.dat\")\n",
    "# df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process corpus. Normalize. Using threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Documents normalized in 764.23s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7988241</td>\n",
       "      <td>1392451</td>\n",
       "      <td>Jorge Salazar Valdés</td>\n",
       "      <td>2020-07-06 13:02:09</td>\n",
       "      <td>&lt;p&gt;Lo que hace falta es que los precios bajen ...</td>\n",
       "      <td>http://www.cubadebate.cu/fotorreportajes/2020/...</td>\n",
       "      <td>[falto, precio, bajar, normalizar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7988229</td>\n",
       "      <td>1391921</td>\n",
       "      <td>Jorge Luis García garcia</td>\n",
       "      <td>2020-07-06 13:00:17</td>\n",
       "      <td>&lt;p&gt;Llevó meses preguntándome como es posible q...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/07/06...</td>\n",
       "      <td>[llevar, mesar, preguntandome, año, empezar, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7988215</td>\n",
       "      <td>1392377</td>\n",
       "      <td>Mary</td>\n",
       "      <td>2020-07-06 12:57:56</td>\n",
       "      <td>&lt;p&gt;Creo que la fase uno en la Habana ha sido u...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/06/c...</td>\n",
       "      <td>[fase, habano, precipitar, pais, relacionar, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7988207</td>\n",
       "      <td>1392451</td>\n",
       "      <td>cubanito</td>\n",
       "      <td>2020-07-06 12:56:54</td>\n",
       "      <td>&lt;p&gt;porque en la habana con casos andando todav...</td>\n",
       "      <td>http://www.cubadebate.cu/fotorreportajes/2020/...</td>\n",
       "      <td>[habano, caso, andar, reportar, caso, entrar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7988205</td>\n",
       "      <td>1392377</td>\n",
       "      <td>Nabudoconosor</td>\n",
       "      <td>2020-07-06 12:56:29</td>\n",
       "      <td>&lt;p&gt;Eso es que se tomó un pepino de prevenghovi...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/06/c...</td>\n",
       "      <td>[tomar, pepino, prevenghovir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>7980711</td>\n",
       "      <td>1391385</td>\n",
       "      <td>libra</td>\n",
       "      <td>2020-07-04 13:00:44</td>\n",
       "      <td>&lt;p&gt;El tema de la carencia de medicamentos, com...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/m...</td>\n",
       "      <td>[temer, carencia, medicamento, carencia, perio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>7980707</td>\n",
       "      <td>1391621</td>\n",
       "      <td>Yandi Moreno Cruz</td>\n",
       "      <td>2020-07-04 13:00:24</td>\n",
       "      <td>&lt;p&gt;Creo q el calendario está muy bueno y tambi...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/04/e...</td>\n",
       "      <td>[calendario, ver, ojo, jugar, choque, beisbol,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>7980705</td>\n",
       "      <td>1320307</td>\n",
       "      <td>Dairo Torres</td>\n",
       "      <td>2020-07-04 13:00:19</td>\n",
       "      <td>&lt;p&gt;Excelente producción. Buen profesionalismo....</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/02/10/e...</td>\n",
       "      <td>[excelente, produccion, profesionalismo, actor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>7980703</td>\n",
       "      <td>1391695</td>\n",
       "      <td>CDMF</td>\n",
       "      <td>2020-07-04 12:59:56</td>\n",
       "      <td>&lt;p&gt;Quise decir medidas firmes para impedir la ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/04/c...</td>\n",
       "      <td>[querer, medir, firme, impedir, indisciplinar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>7980697</td>\n",
       "      <td>1391695</td>\n",
       "      <td>Jorge R 09</td>\n",
       "      <td>2020-07-04 12:59:08</td>\n",
       "      <td>&lt;p&gt;Los índices sanitarios establecidos como re...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/04/c...</td>\n",
       "      <td>[indice, sanitario, establecer, requerir, pasa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post               author_name                date  \\\n",
       "0    7988241  1392451      Jorge Salazar Valdés 2020-07-06 13:02:09   \n",
       "1    7988229  1391921  Jorge Luis García garcia 2020-07-06 13:00:17   \n",
       "2    7988215  1392377                      Mary 2020-07-06 12:57:56   \n",
       "3    7988207  1392451                  cubanito 2020-07-06 12:56:54   \n",
       "4    7988205  1392377             Nabudoconosor 2020-07-06 12:56:29   \n",
       "..       ...      ...                       ...                 ...   \n",
       "995  7980711  1391385                     libra 2020-07-04 13:00:44   \n",
       "996  7980707  1391621         Yandi Moreno Cruz 2020-07-04 13:00:24   \n",
       "997  7980705  1320307              Dairo Torres 2020-07-04 13:00:19   \n",
       "998  7980703  1391695                      CDMF 2020-07-04 12:59:56   \n",
       "999  7980697  1391695                Jorge R 09 2020-07-04 12:59:08   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Lo que hace falta es que los precios bajen ...   \n",
       "1    <p>Llevó meses preguntándome como es posible q...   \n",
       "2    <p>Creo que la fase uno en la Habana ha sido u...   \n",
       "3    <p>porque en la habana con casos andando todav...   \n",
       "4    <p>Eso es que se tomó un pepino de prevenghovi...   \n",
       "..                                                 ...   \n",
       "995  <p>El tema de la carencia de medicamentos, com...   \n",
       "996  <p>Creo q el calendario está muy bueno y tambi...   \n",
       "997  <p>Excelente producción. Buen profesionalismo....   \n",
       "998  <p>Quise decir medidas firmes para impedir la ...   \n",
       "999  <p>Los índices sanitarios establecidos como re...   \n",
       "\n",
       "                                                  link  \\\n",
       "0    http://www.cubadebate.cu/fotorreportajes/2020/...   \n",
       "1    http://www.cubadebate.cu/especiales/2020/07/06...   \n",
       "2    http://www.cubadebate.cu/noticias/2020/07/06/c...   \n",
       "3    http://www.cubadebate.cu/fotorreportajes/2020/...   \n",
       "4    http://www.cubadebate.cu/noticias/2020/07/06/c...   \n",
       "..                                                 ...   \n",
       "995  http://www.cubadebate.cu/noticias/2020/07/03/m...   \n",
       "996  http://www.cubadebate.cu/noticias/2020/07/04/e...   \n",
       "997  http://www.cubadebate.cu/noticias/2020/02/10/e...   \n",
       "998  http://www.cubadebate.cu/noticias/2020/07/04/c...   \n",
       "999  http://www.cubadebate.cu/noticias/2020/07/04/c...   \n",
       "\n",
       "                                                  text  \n",
       "0                   [falto, precio, bajar, normalizar]  \n",
       "1    [llevar, mesar, preguntandome, año, empezar, p...  \n",
       "2    [fase, habano, precipitar, pais, relacionar, c...  \n",
       "3    [habano, caso, andar, reportar, caso, entrar, ...  \n",
       "4                        [tomar, pepino, prevenghovir]  \n",
       "..                                                 ...  \n",
       "995  [temer, carencia, medicamento, carencia, perio...  \n",
       "996  [calendario, ver, ojo, jugar, choque, beisbol,...  \n",
       "997  [excelente, produccion, profesionalismo, actor...  \n",
       "998  [querer, medir, firme, impedir, indisciplinar,...  \n",
       "999  [indice, sanitario, establecer, requerir, pasa...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process corpus. Normalize. Using threads\n",
    "_start = time.time()\n",
    "ddf_comments = dd.from_pandas(df_comments, chunksize=10)\n",
    "df_comments[\"text\"] = ddf_comments[\"content\"].apply(clean, meta=('content', 'object')).compute()\n",
    "documents_normalized = list(df_comments[\"text\"].values)\n",
    "_end = time.time()\n",
    "print(f\"{len(documents_normalized)} Documents normalized in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Comments list\n",
    "tfidf_list = comments_tfidf(documents_normalized)\n",
    "\n",
    "unordered_tfidf: Dict[str, float] = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf[w] = unordered_tfidf.get(w, 0) + value\n",
    "\n",
    "\n",
    "# unordered_tfidf\n",
    "ordered_comments_tfidf = OrderedDict(sort_tfidf(unordered_tfidf))\n",
    "\n",
    "# WordCloud with word_token bigrams (ngram=2)\n",
    "token_comments_tfidf = dict()\n",
    "unordered_tfidf_ngram: Dict[str, float] = dict()\n",
    "documents_normalized_ngram = list()\n",
    "\n",
    "for doc_norm in documents_normalized:\n",
    "    documents_normalized_ngram.append(create_ngram(doc_norm, ngram=2))\n",
    "\n",
    "tfidf_list_ngram = comments_tfidf(documents_normalized_ngram)\n",
    "\n",
    "for tfidf in tfidf_list_ngram:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf_ngram[w] = unordered_tfidf_ngram.get(w, 0) + value\n",
    "\n",
    "ordered_comments_tfidf_ngram = OrderedDict(sort_tfidf(unordered_tfidf_ngram))\n",
    "\n",
    "for lemma_, value in ordered_comments_tfidf_ngram.items():\n",
    "    _token = \" \".join([str(bow_lemma_token[lm]) for lm in lemma_.split(\" \")])\n",
    "    token_comments_tfidf[_token] = value\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"comments_tfidf.json\", \"w\") as file_json:\n",
    "    json.dump(ordered_comments_tfidf_ngram, file_json)\n",
    "print(\"TF-IDF ordered saved to comments_tfidf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Habana Capitolio Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example\n",
    "# in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO: str = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK: str = (\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg\"\n",
    ")\n",
    "\n",
    "# download mask images\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, \"wb\") as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print(\"Image downloaded.\")\n",
    "    else:\n",
    "        print(\"Image No downloaded!\")\n",
    "else:\n",
    "    image_downloaded: str = IMG_CAPITOLIO.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "    print(f\"Image: {image_downloaded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordsCloud Cubadebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = \"wordcloud_cubadebate.png\"\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=500,\n",
    "    #     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color=\"white\",\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color=\"steelblue\",\n",
    "    stopwords=STOP_WORDS,\n",
    ").generate_from_frequencies(token_comments_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print(\"WordCloud Cubadebate image saved.\\n\")\n",
    "fig = plt.figure(figsize = (22, 15),)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Words (Lemma) with most TF-IDF (unigram: ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda DataFrame shape:(6468, 2)\n",
      "GroupBy DataFrame shape:(18, 2)\n",
      "\n",
      "    TF-IDF         Word\n",
      "0       20       habano\n",
      "1       19       gracia\n",
      "2       18         fase\n",
      "3       17         caso\n",
      "4       14     personar\n",
      "5       12   comentario\n",
      "6       11       querer\n",
      "7       10        pasar\n",
      "8        9       pensar\n",
      "9        8        favor\n",
      "10       7       cubano\n",
      "11       6         yeni\n",
      "12       5         niño\n",
      "13       4       entrar\n",
      "14       3  transportar\n",
      "15       2       social\n",
      "16       1         dato\n",
      "17       0        deuda\n",
      "\n",
      "Top Word Token and Post\n",
      "\n",
      "                  id                                              title  \\\n",
      "caso         1392517  China emite una alerta tras confirmarse un cas...   \n",
      "cubano       1392499  Médicos cubanos apoyarán lucha contra la COVID...   \n",
      "fase         1390397  La Habana pasa a la primera fase de recuperaci...   \n",
      "niño         1389297  Haila regala a los niños “Un canto a la sonris...   \n",
      "dato         1389743  Dictadura boliviana se dispone a detener a can...   \n",
      "pasar        1389733  COVID-19 en el mundo: EEUU puede pasar de 40 m...   \n",
      "social       1389243  Boicot a Facebook por no comprometerse contra ...   \n",
      "entrar       1389093  Morales Ojeda: Estábamos convencidos de que La...   \n",
      "querer       1387867  ¿Producir todos los alimentos que necesitamos ...   \n",
      "gracia       1387861  Luego de 23 años, gemelos se encuentran en Bra...   \n",
      "favor        1385303  Cuba proseguirá trabajando en favor de la unid...   \n",
      "pensar       1373409  Jorge Fornet: Espero que volvamos a tener la c...   \n",
      "deuda        1372889  Una pinza letal presiona a los países pobres: ...   \n",
      "personar     1350093           COVID-19: Flashazos internacionales (IX)   \n",
      "habano       1327213  Abre en La Habana la edición 22 del Festival d...   \n",
      "comentario   1313213  Indígenas prometen llevar a Bolsonaro ante los...   \n",
      "transportar  1289899  Trump ataca otra vez: Sancionan a seis buques ...   \n",
      "yeni          993679  Yenisel Valdés: \"Trabajar con los Van Van fue ...   \n",
      "\n",
      "                                                           url        date  \n",
      "caso         http://www.cubadebate.cu/noticias/2020/07/06/c...  2020/07/06  \n",
      "cubano       http://www.cubadebate.cu/noticias/2020/07/06/m...  2020/07/06  \n",
      "fase         http://www.cubadebate.cu/noticias/2020/07/01/l...  2020/07/01  \n",
      "niño         http://www.cubadebate.cu/noticias/2020/06/30/h...  2020/06/30  \n",
      "dato         http://www.cubadebate.cu/noticias/2020/06/30/d...  2020/06/30  \n",
      "pasar        http://www.cubadebate.cu/noticias/2020/06/30/c...  2020/06/30  \n",
      "social       http://www.cubadebate.cu/noticias/2020/06/29/b...  2020/06/29  \n",
      "entrar       http://www.cubadebate.cu/noticias/2020/06/29/m...  2020/06/29  \n",
      "querer       http://www.cubadebate.cu/opinion/2020/06/26/pr...  2020/06/26  \n",
      "gracia       http://www.cubadebate.cu/noticias/2020/06/26/l...  2020/06/26  \n",
      "favor        http://www.cubadebate.cu/noticias/2020/06/20/c...  2020/06/20  \n",
      "pensar       http://www.cubadebate.cu/especiales/2020/05/27...  2020/05/27  \n",
      "deuda        http://www.cubadebate.cu/especiales/2020/05/26...  2020/05/26  \n",
      "personar     http://www.cubadebate.cu/noticias/2020/04/11/c...  2020/04/11  \n",
      "habano       http://www.cubadebate.cu/cuba/2020/02/24/abre-...  2020/02/24  \n",
      "comentario   http://www.cubadebate.cu/noticias/2020/01/24/i...  2020/01/24  \n",
      "transportar  http://www.cubadebate.cu/noticias/2019/12/03/t...  2019/12/03  \n",
      "yeni         http://www.cubadebate.cu/especiales/2017/12/08...  2017/12/08  \n",
      "\n",
      "Saved top_word_post.json\n",
      "\n",
      "Rewrite index.html.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Words (Lemma) with most TF-IDF (unigram: ngram=1)\n",
    "df = (\n",
    "    DataFrame.from_dict(\n",
    "        data=ordered_comments_tfidf, dtype=int, orient=\"index\", columns=[\"TF-IDF\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(index=str, columns={\"index\": \"Word\"})\n",
    ")\n",
    "rows, columns = df.shape\n",
    "print(f\"Panda DataFrame shape:({rows}, {columns})\")\n",
    "\n",
    "df_gb = df.groupby([\"TF-IDF\"], sort=False).first().reset_index()\n",
    "rows, columns = df_gb.shape\n",
    "print(f\"GroupBy DataFrame shape:({rows}, {columns})\\n\")\n",
    "print(df_gb)\n",
    "\n",
    "words_token = list(df_gb[\"Word\"].values)\n",
    "searches_dict = dict()\n",
    "searches_ids = set()\n",
    "\n",
    "for w_token in words_token:\n",
    "    search_list = get_searches([w_token])\n",
    "    # Get first searched result\n",
    "    if len(search_list) > 0:\n",
    "        search_id = search_list[0][\"id\"]\n",
    "        if search_id not in searches_ids:\n",
    "            searches_ids.add(search_id)\n",
    "            searches_dict[w_token] = search_list[0]\n",
    "\n",
    "# DataFrame First Search Posts\n",
    "if len(searches_dict) > 0:\n",
    "    searches_df = DataFrame.from_dict(data=searches_dict, orient=\"index\",).drop(\n",
    "        columns=[\"type\", \"subtype\", \"_links\"]\n",
    "    )\n",
    "\n",
    "    df_dates = searches_df[\"url\"].str.extract(r\"(?P<date>\\d{4}/\\d{2}/\\d{2})\")\n",
    "\n",
    "    searches_by_dates = searches_df.join(df_dates).sort_values(\n",
    "        by=\"date\", ascending=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop Word Token and Post\\n\")\n",
    "    print(searches_by_dates)\n",
    "    searches_by_dates.to_json(os.path.join(_dir, \"top_word_post.json\"))\n",
    "    print(\"\\nSaved top_word_post.json\\n\")\n",
    "\n",
    "    if os.path.isfile(\"index.tpl\"):\n",
    "        with open(\"index.html\", \"w\", encoding=\"utf-8\") as f_index, open(\n",
    "            \"index.tpl\", \"r\"\n",
    "        ) as file:\n",
    "            tpl = file.read()\n",
    "            index_template = Template(tpl)\n",
    "\n",
    "            text_link = \"<ul>\\n\"\n",
    "            for _index, _title, _url in searches_by_dates.reset_index()[\n",
    "                [\"index\", \"title\", \"url\"]\n",
    "            ].values:\n",
    "                text_link += (\n",
    "                    f\"\\t<li><a href='{_url}' rel='external' \"\n",
    "                    f\"data-token='{_index}'>{_title}</a></li>\\n\"\n",
    "                )\n",
    "            text_link += \"</ul>\"\n",
    "            # Create index.html from index.tpl\n",
    "            f_index.write(index_template.substitute(CUBADEBATE_LINKS=text_link))\n",
    "        print(\"Rewrite index.html.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open** [index.html](index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
