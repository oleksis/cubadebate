{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import Counter, OrderedDict\n",
    "from datetime import datetime\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from string import Template\n",
    "import time\n",
    "from typing import List, Optional, Dict, Tuple, Iterable, Any\n",
    "\n",
    "import requests\n",
    "import dask\n",
    "from dask import bag as db\n",
    "from dask import dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Annotations\n",
    "ElementList = List[Optional[Dict[str, Any]]]\n",
    "Document = str\n",
    "DocumentList = List[Document]\n",
    "DocumentNormalizedList = List[DocumentList]\n",
    "TermFrequency = Dict[str, float]\n",
    "IDF = Dict[str, float]\n",
    "TFIDF = Dict[str, float]\n",
    "TFIDF_List = List[TFIDF]\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "SEARCH_ENDPOINT = CUBADEBATE_API + \"search/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionErrorRequests = requests.exceptions.ConnectionError\n",
    "\n",
    "FECHA_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "\n",
    "def create_datetime(string: str) -> Optional[datetime]:\n",
    "    \"\"\"Create datetime from string '2014-07-03T23:27:51'\"\"\"\n",
    "    date_time = None\n",
    "\n",
    "    try:\n",
    "        if string is not None:\n",
    "            date_time = datetime.strptime(string, FECHA_FORMAT)\n",
    "    except ValueError:\n",
    "        print(\"Format no valid!\")\n",
    "\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def save_elements_json(name: str, line: str = None, mode=\"w\"):\n",
    "    \"\"\"Save elements to file, by lines JSON elments.\"\"\"\n",
    "    with open(name, mode) as _file:\n",
    "        if line:\n",
    "            _file.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def read_elements_json(name: str) -> Optional[list]:\n",
    "    \"\"\"Read by lines JSON data in file name.\n",
    "    Return list of elements or None\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(name, \"r\") as _file:\n",
    "            for line in _file.readlines():\n",
    "                if line:\n",
    "                    data += json.loads(line)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"Error reading lines of _file: {name}\")\n",
    "    except FileNotFoundError as f_error:\n",
    "        print(f_error)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_comments_file(name: str) -> DataFrame:\n",
    "    \"\"\"Get List of comments from file name\"\"\"\n",
    "    comments_json = read_elements_json(name)\n",
    "    comments_bag = db.from_sequence(comments_json).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[str] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_elements_json(url: str, **kwargs) -> ElementList:\n",
    "    \"\"\"Get JSON of list elements from endpoint API Wordpress v2\n",
    "\n",
    "    RETURNS (ElementList): Delayed Response List of elements in JSON format\n",
    "    to compute for Dask.Bag\n",
    "    \"\"\"\n",
    "    params = {\"page\": \"1\"}\n",
    "    params.update(kwargs)\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            resp = session.get(url, params=params)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json()\n",
    "                if params.get(\"file\"):\n",
    "                    save_elements_json(params.get(\"file\"), resp.text, \"a\")\n",
    "                return results\n",
    "    except ConnectionErrorRequests:\n",
    "        pass  # Try again! Occurred Connection Error\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_comments(pages: int) -> DataFrame:\n",
    "    \"\"\"Get List of comments per page\"\"\"\n",
    "    comments_delayed = (\n",
    "        get_elements_json(url=COMMENTS_ENDPOINT, page=str(page), file=\"comments.dat\")\n",
    "        for page in range(1, pages + 1)\n",
    "    )\n",
    "\n",
    "    comments_bag = db.from_delayed(comments_delayed).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[dict] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "def get_searches(words: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get List of searches one page\"\"\"\n",
    "    search_delayed = (\n",
    "        get_elements_json(url=SEARCH_ENDPOINT, search=word) for word in words\n",
    "    )\n",
    "\n",
    "    searches_bag = db.from_delayed(search_delayed)\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        searches = searches_bag.compute()\n",
    "    return searches\n",
    "\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "bow_lemma_token: Dict[str, Token] = dict()\n",
    "\n",
    "\n",
    "def remplace_accents(text: str) -> str:\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r\"á\", \"a\", text, flags=re.I)\n",
    "    text = re.sub(r\"é\", \"e\", text, flags=re.I)\n",
    "    text = re.sub(r\"í\", \"i\", text, flags=re.I)\n",
    "    text = re.sub(r\"ó\", \"o\", text, flags=re.I)\n",
    "    text = re.sub(r\"ú\", \"u\", text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(markup: str) -> str:\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", markup)\n",
    "    url_pattern = r\"(http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_token(token: Token) -> str:\n",
    "    \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "    result = remplace_accents(token.lemma_.strip().lower())\n",
    "    return result\n",
    "\n",
    "\n",
    "def is_token_allowed(token: Token) -> bool:\n",
    "    \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "    if (\n",
    "        not token\n",
    "        or not token.string.strip()\n",
    "        or token.is_stop\n",
    "        or token.is_punct\n",
    "        or len(token) < 3\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean(doc: str) -> List[str]:\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = []\n",
    "\n",
    "    for token in nlp(text):\n",
    "        if is_token_allowed(token):\n",
    "            word_lemma = preprocess_token(token)\n",
    "            bow_lemma_token[word_lemma] = token\n",
    "            tokens.append(word_lemma)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_ngram(document: DocumentList, ngram=1) -> DocumentList:\n",
    "    \"\"\"Create N-Gram from document\"\"\"\n",
    "    return [\n",
    "        \" \".join(document[i : i + ngram]) for i in range(len(document) - (ngram - 1))\n",
    "    ]\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "def comment_tf(document: DocumentList) -> TermFrequency:\n",
    "    \"\"\"Term Frequency for a document\"\"\"\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "\n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def comments_tf(documents: DocumentNormalizedList) -> List[TermFrequency]:\n",
    "    \"\"\"Term Frequency for many document\"\"\"\n",
    "    return [comment_tf(comment) for comment in documents]\n",
    "\n",
    "\n",
    "def count_dict(documents: DocumentNormalizedList) -> Dict[str, int]:\n",
    "    \"\"\"Counter the word in all documents at least ones\"\"\"\n",
    "    counts = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            _value = counts.get(word, 0)\n",
    "            counts[word] = _value + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def idf_dict(documents: DocumentNormalizedList) -> IDF:\n",
    "    \"\"\"Inverse Document Frequency in all Documents\"\"\"\n",
    "    idf_d = dict()\n",
    "\n",
    "    counts = count_dict(documents)\n",
    "\n",
    "    for word in counts:\n",
    "        idf_d[word] = math.log(len(documents) / counts[word])\n",
    "\n",
    "    return idf_d\n",
    "\n",
    "\n",
    "def comments_tfidf(documents: DocumentNormalizedList) -> TFIDF_List:\n",
    "    \"\"\"TF-IDF of all Documents Normalized\"\"\"\n",
    "    tfidf_comments = []\n",
    "\n",
    "    idf_comments = idf_dict(documents)\n",
    "\n",
    "    def compute_tfidf_comment(document: DocumentList) -> TFIDF:\n",
    "        \"\"\"Compute TF-IDF for a Document\"\"\"\n",
    "        tfidf_comment = dict()\n",
    "\n",
    "        tf_comment = comment_tf(document)\n",
    "\n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "\n",
    "        return tfidf_comment\n",
    "\n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "\n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "def sort_tfidf(tfidf_unordered) -> Iterable[Tuple[Any, Any]]:\n",
    "    return sorted(tfidf_unordered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def export_image2html(image_name: str) -> None:\n",
    "    \"\"\"Export a image to html file\"\"\"\n",
    "    with open(image_name, \"rb\") as file_img:\n",
    "        data = base64.b64encode(file_img.read()).decode(\"utf-8\")\n",
    "\n",
    "    img_str = \"\"\"\n",
    "    <img width=\"100%\" height=\"100%\"\n",
    "    src=\"data:image/png;base64,{}\" />\n",
    "    \"\"\".format(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    with open(\"wordcloud_cubadebate.html\", \"w\") as _html:\n",
    "        _html.write(img_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "## Download Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 8.82s\n"
     ]
    }
   ],
   "source": [
    "_start = time.time()\n",
    "df_comments =  get_comments(pages=1)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# df_comments.info()\n",
    "# df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Comments downloaded in 446.87s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7983777</td>\n",
       "      <td>1392061</td>\n",
       "      <td>Laura V</td>\n",
       "      <td>2020-07-05 10:17:08</td>\n",
       "      <td>&lt;p&gt;Saludos cordiales, y gracias a todo el pers...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7983775</td>\n",
       "      <td>1392061</td>\n",
       "      <td>Alexander Sánchez Pérez</td>\n",
       "      <td>2020-07-05 10:16:13</td>\n",
       "      <td>&lt;p&gt;Seguimos ganando la batalla, ahora bien deb...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7983767</td>\n",
       "      <td>1392061</td>\n",
       "      <td>Wemm</td>\n",
       "      <td>2020-07-05 10:13:59</td>\n",
       "      <td>&lt;p&gt;Y siguen los niños menores de 6 años con ma...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7983759</td>\n",
       "      <td>1392061</td>\n",
       "      <td>JLS</td>\n",
       "      <td>2020-07-05 10:10:17</td>\n",
       "      <td>&lt;p&gt;Al caer la tarde juegan MUCHOS jóvenes y ni...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7983757</td>\n",
       "      <td>1392061</td>\n",
       "      <td>debemos cuidarnos</td>\n",
       "      <td>2020-07-05 10:09:01</td>\n",
       "      <td>&lt;p&gt;Muchas personas se han tomado esta Fase 1 a...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>7978437</td>\n",
       "      <td>1391215</td>\n",
       "      <td>Jacqueline Arenal</td>\n",
       "      <td>2020-07-03 20:03:11</td>\n",
       "      <td>&lt;p&gt;Gracias a Paquita por el privilegio de preg...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/07/03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>7978433</td>\n",
       "      <td>1391205</td>\n",
       "      <td>Heriberto</td>\n",
       "      <td>2020-07-03 20:00:34</td>\n",
       "      <td>&lt;p&gt;Pero si más anormal fue que en ese país mil...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>7978429</td>\n",
       "      <td>1391361</td>\n",
       "      <td>OERM</td>\n",
       "      <td>2020-07-03 19:58:31</td>\n",
       "      <td>&lt;p&gt;Solo decirle  a todo el pueblo de Cuba, que...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>7978425</td>\n",
       "      <td>1391333</td>\n",
       "      <td>alexs</td>\n",
       "      <td>2020-07-03 19:58:05</td>\n",
       "      <td>&lt;p&gt;ahora si van a ver la messi dependencia, jj...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>7978419</td>\n",
       "      <td>1371663</td>\n",
       "      <td>Lucina</td>\n",
       "      <td>2020-07-03 19:56:49</td>\n",
       "      <td>&lt;p&gt;Me quedó pendiente plantear que para mas di...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/05/23/d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post              author_name                date  \\\n",
       "0    7983777  1392061                  Laura V 2020-07-05 10:17:08   \n",
       "1    7983775  1392061  Alexander Sánchez Pérez 2020-07-05 10:16:13   \n",
       "2    7983767  1392061                     Wemm 2020-07-05 10:13:59   \n",
       "3    7983759  1392061                      JLS 2020-07-05 10:10:17   \n",
       "4    7983757  1392061        debemos cuidarnos 2020-07-05 10:09:01   \n",
       "..       ...      ...                      ...                 ...   \n",
       "995  7978437  1391215        Jacqueline Arenal 2020-07-03 20:03:11   \n",
       "996  7978433  1391205                Heriberto 2020-07-03 20:00:34   \n",
       "997  7978429  1391361                     OERM 2020-07-03 19:58:31   \n",
       "998  7978425  1391333                    alexs 2020-07-03 19:58:05   \n",
       "999  7978419  1371663                   Lucina 2020-07-03 19:56:49   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Saludos cordiales, y gracias a todo el pers...   \n",
       "1    <p>Seguimos ganando la batalla, ahora bien deb...   \n",
       "2    <p>Y siguen los niños menores de 6 años con ma...   \n",
       "3    <p>Al caer la tarde juegan MUCHOS jóvenes y ni...   \n",
       "4    <p>Muchas personas se han tomado esta Fase 1 a...   \n",
       "..                                                 ...   \n",
       "995  <p>Gracias a Paquita por el privilegio de preg...   \n",
       "996  <p>Pero si más anormal fue que en ese país mil...   \n",
       "997  <p>Solo decirle  a todo el pueblo de Cuba, que...   \n",
       "998  <p>ahora si van a ver la messi dependencia, jj...   \n",
       "999  <p>Me quedó pendiente plantear que para mas di...   \n",
       "\n",
       "                                                  link  \n",
       "0    http://www.cubadebate.cu/noticias/2020/07/05/c...  \n",
       "1    http://www.cubadebate.cu/noticias/2020/07/05/c...  \n",
       "2    http://www.cubadebate.cu/noticias/2020/07/05/c...  \n",
       "3    http://www.cubadebate.cu/noticias/2020/07/05/c...  \n",
       "4    http://www.cubadebate.cu/noticias/2020/07/05/c...  \n",
       "..                                                 ...  \n",
       "995  http://www.cubadebate.cu/especiales/2020/07/03...  \n",
       "996  http://www.cubadebate.cu/noticias/2020/07/03/e...  \n",
       "997  http://www.cubadebate.cu/noticias/2020/07/03/r...  \n",
       "998  http://www.cubadebate.cu/noticias/2020/07/03/l...  \n",
       "999  http://www.cubadebate.cu/noticias/2020/05/23/d...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate file comments.dat\n",
    "save_elements_json(\"comments.dat\")\n",
    "NUM_PAGES = 100\n",
    "\n",
    "_start = time.time()\n",
    "df_comments = get_comments(pages=NUM_PAGES)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not df_comments.empty, \"No comments to process\"\n",
    "# Uncomment the next for get the comments from file\n",
    "# df_comments = get_comments_file(\"comments.dat\")\n",
    "# print(f\"{len(df_comments)} Comments from file comments.dat\")\n",
    "# df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process corpus. Normalize. Using threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Documents normalized in 704.67s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7983777</td>\n",
       "      <td>1392061</td>\n",
       "      <td>Laura V</td>\n",
       "      <td>2020-07-05 10:17:08</td>\n",
       "      <td>&lt;p&gt;Saludos cordiales, y gracias a todo el pers...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "      <td>[saludo, cordial, gracia, personal, salud, ard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7983775</td>\n",
       "      <td>1392061</td>\n",
       "      <td>Alexander Sánchez Pérez</td>\n",
       "      <td>2020-07-05 10:16:13</td>\n",
       "      <td>&lt;p&gt;Seguimos ganando la batalla, ahora bien deb...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "      <td>[seguir, ganar, batallar, deber, cuidar, padre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7983767</td>\n",
       "      <td>1392061</td>\n",
       "      <td>Wemm</td>\n",
       "      <td>2020-07-05 10:13:59</td>\n",
       "      <td>&lt;p&gt;Y siguen los niños menores de 6 años con ma...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "      <td>[seguir, niño, menor, año, contacto, engañar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7983759</td>\n",
       "      <td>1392061</td>\n",
       "      <td>JLS</td>\n",
       "      <td>2020-07-05 10:10:17</td>\n",
       "      <td>&lt;p&gt;Al caer la tarde juegan MUCHOS jóvenes y ni...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "      <td>[caer, jugar, joven, niño, beisbol, futbol, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7983757</td>\n",
       "      <td>1392061</td>\n",
       "      <td>debemos cuidarnos</td>\n",
       "      <td>2020-07-05 10:09:01</td>\n",
       "      <td>&lt;p&gt;Muchas personas se han tomado esta Fase 1 a...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/05/c...</td>\n",
       "      <td>[personar, tomar, fase, ligero, razonar, conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>7978437</td>\n",
       "      <td>1391215</td>\n",
       "      <td>Jacqueline Arenal</td>\n",
       "      <td>2020-07-03 20:03:11</td>\n",
       "      <td>&lt;p&gt;Gracias a Paquita por el privilegio de preg...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2020/07/03...</td>\n",
       "      <td>[gracia, paquita, privilegiar, preguntar, inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>7978433</td>\n",
       "      <td>1391205</td>\n",
       "      <td>Heriberto</td>\n",
       "      <td>2020-07-03 20:00:34</td>\n",
       "      <td>&lt;p&gt;Pero si más anormal fue que en ese país mil...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/e...</td>\n",
       "      <td>[anormal, pais, millon, personar, votar, elegi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>7978429</td>\n",
       "      <td>1391361</td>\n",
       "      <td>OERM</td>\n",
       "      <td>2020-07-03 19:58:31</td>\n",
       "      <td>&lt;p&gt;Solo decirle  a todo el pueblo de Cuba, que...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/r...</td>\n",
       "      <td>[decirle, poblar, cuba, asumir, realidad, pand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>7978425</td>\n",
       "      <td>1391333</td>\n",
       "      <td>alexs</td>\n",
       "      <td>2020-07-03 19:58:05</td>\n",
       "      <td>&lt;p&gt;ahora si van a ver la messi dependencia, jj...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/07/03/l...</td>\n",
       "      <td>[messi, dependencia, jjaaa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>7978419</td>\n",
       "      <td>1371663</td>\n",
       "      <td>Lucina</td>\n",
       "      <td>2020-07-03 19:56:49</td>\n",
       "      <td>&lt;p&gt;Me quedó pendiente plantear que para mas di...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2020/05/23/d...</td>\n",
       "      <td>[pendiente, plantear, dificil, proceder, aplic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post              author_name                date  \\\n",
       "0    7983777  1392061                  Laura V 2020-07-05 10:17:08   \n",
       "1    7983775  1392061  Alexander Sánchez Pérez 2020-07-05 10:16:13   \n",
       "2    7983767  1392061                     Wemm 2020-07-05 10:13:59   \n",
       "3    7983759  1392061                      JLS 2020-07-05 10:10:17   \n",
       "4    7983757  1392061        debemos cuidarnos 2020-07-05 10:09:01   \n",
       "..       ...      ...                      ...                 ...   \n",
       "995  7978437  1391215        Jacqueline Arenal 2020-07-03 20:03:11   \n",
       "996  7978433  1391205                Heriberto 2020-07-03 20:00:34   \n",
       "997  7978429  1391361                     OERM 2020-07-03 19:58:31   \n",
       "998  7978425  1391333                    alexs 2020-07-03 19:58:05   \n",
       "999  7978419  1371663                   Lucina 2020-07-03 19:56:49   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Saludos cordiales, y gracias a todo el pers...   \n",
       "1    <p>Seguimos ganando la batalla, ahora bien deb...   \n",
       "2    <p>Y siguen los niños menores de 6 años con ma...   \n",
       "3    <p>Al caer la tarde juegan MUCHOS jóvenes y ni...   \n",
       "4    <p>Muchas personas se han tomado esta Fase 1 a...   \n",
       "..                                                 ...   \n",
       "995  <p>Gracias a Paquita por el privilegio de preg...   \n",
       "996  <p>Pero si más anormal fue que en ese país mil...   \n",
       "997  <p>Solo decirle  a todo el pueblo de Cuba, que...   \n",
       "998  <p>ahora si van a ver la messi dependencia, jj...   \n",
       "999  <p>Me quedó pendiente plantear que para mas di...   \n",
       "\n",
       "                                                  link  \\\n",
       "0    http://www.cubadebate.cu/noticias/2020/07/05/c...   \n",
       "1    http://www.cubadebate.cu/noticias/2020/07/05/c...   \n",
       "2    http://www.cubadebate.cu/noticias/2020/07/05/c...   \n",
       "3    http://www.cubadebate.cu/noticias/2020/07/05/c...   \n",
       "4    http://www.cubadebate.cu/noticias/2020/07/05/c...   \n",
       "..                                                 ...   \n",
       "995  http://www.cubadebate.cu/especiales/2020/07/03...   \n",
       "996  http://www.cubadebate.cu/noticias/2020/07/03/e...   \n",
       "997  http://www.cubadebate.cu/noticias/2020/07/03/r...   \n",
       "998  http://www.cubadebate.cu/noticias/2020/07/03/l...   \n",
       "999  http://www.cubadebate.cu/noticias/2020/05/23/d...   \n",
       "\n",
       "                                                  text  \n",
       "0    [saludo, cordial, gracia, personal, salud, ard...  \n",
       "1    [seguir, ganar, batallar, deber, cuidar, padre...  \n",
       "2    [seguir, niño, menor, año, contacto, engañar, ...  \n",
       "3    [caer, jugar, joven, niño, beisbol, futbol, ce...  \n",
       "4    [personar, tomar, fase, ligero, razonar, conse...  \n",
       "..                                                 ...  \n",
       "995  [gracia, paquita, privilegiar, preguntar, inte...  \n",
       "996  [anormal, pais, millon, personar, votar, elegi...  \n",
       "997  [decirle, poblar, cuba, asumir, realidad, pand...  \n",
       "998                        [messi, dependencia, jjaaa]  \n",
       "999  [pendiente, plantear, dificil, proceder, aplic...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process corpus. Normalize. Using threads\n",
    "_start = time.time()\n",
    "ddf_comments = dd.from_pandas(df_comments, chunksize=10)\n",
    "df_comments[\"text\"] = ddf_comments[\"content\"].apply(clean, meta=('content', 'object')).compute()\n",
    "documents_normalized = list(df_comments[\"text\"].values)\n",
    "_end = time.time()\n",
    "print(f\"{len(documents_normalized)} Documents normalized in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Comments list\n",
    "tfidf_list = comments_tfidf(documents_normalized)\n",
    "\n",
    "unordered_tfidf: Dict[str, float] = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf[w] = unordered_tfidf.get(w, 0) + value\n",
    "\n",
    "\n",
    "# unordered_tfidf\n",
    "ordered_comments_tfidf = OrderedDict(sort_tfidf(unordered_tfidf))\n",
    "\n",
    "# WordCloud with word_token bigrams (ngram=2)\n",
    "token_comments_tfidf = dict()\n",
    "unordered_tfidf_ngram: Dict[str, float] = dict()\n",
    "documents_normalized_ngram = list()\n",
    "\n",
    "for doc_norm in documents_normalized:\n",
    "    documents_normalized_ngram.append(create_ngram(doc_norm, ngram=2))\n",
    "\n",
    "tfidf_list_ngram = comments_tfidf(documents_normalized_ngram)\n",
    "\n",
    "for tfidf in tfidf_list_ngram:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf_ngram[w] = unordered_tfidf_ngram.get(w, 0) + value\n",
    "\n",
    "ordered_comments_tfidf_ngram = OrderedDict(sort_tfidf(unordered_tfidf_ngram))\n",
    "\n",
    "for lemma_, value in ordered_comments_tfidf_ngram.items():\n",
    "    _token = \" \".join([str(bow_lemma_token[lm]) for lm in lemma_.split(\" \")])\n",
    "    token_comments_tfidf[_token] = value\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"comments_tfidf.json\", \"w\") as file_json:\n",
    "    json.dump(ordered_comments_tfidf_ngram, file_json)\n",
    "print(\"TF-IDF ordered saved to comments_tfidf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Habana Capitolio Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example\n",
    "# in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO: str = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK: str = (\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg\"\n",
    ")\n",
    "\n",
    "# download mask images\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, \"wb\") as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print(\"Image downloaded.\")\n",
    "    else:\n",
    "        print(\"Image No downloaded!\")\n",
    "else:\n",
    "    image_downloaded: str = IMG_CAPITOLIO.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "    print(f\"Image: {image_downloaded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordsCloud Cubadebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = \"wordcloud_cubadebate.png\"\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=500,\n",
    "    #     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color=\"white\",\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color=\"steelblue\",\n",
    "    stopwords=STOP_WORDS,\n",
    ").generate_from_frequencies(token_comments_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print(\"WordCloud Cubadebate image saved.\\n\")\n",
    "fig = plt.figure(figsize = (22, 15),)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Words (Lemma) with most TF-IDF (unigram: ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda DataFrame shape:(6057, 2)\n",
      "GroupBy DataFrame shape:(18, 2)\n",
      "\n",
      "    TF-IDF        Word\n",
      "0       20      habano\n",
      "1       17      gracia\n",
      "2       15        fase\n",
      "3       14        caso\n",
      "4       13        pais\n",
      "5       12       falto\n",
      "6       11  comentario\n",
      "7       10      cubano\n",
      "8        9      saludo\n",
      "9        8      viajar\n",
      "10       7      entrar\n",
      "11       6      llorar\n",
      "12       5        dios\n",
      "13       4    nacional\n",
      "14       3     materia\n",
      "15       2    prohibir\n",
      "16       1      fiesta\n",
      "17       0     caballo\n",
      "\n",
      "Top Word Token and Post\n",
      "\n",
      "                 id                                              title  \\\n",
      "dios        1392153  Nueva oleada de incendios en la Amazonia, en j...   \n",
      "caso        1392065  COVID-19 en el mundo: Récord diario de casos e...   \n",
      "pais        1392013  México llega a 30 mil fallecidos y ya es el qu...   \n",
      "nacional    1391621  Exclusiva: La Serie Nacional 60 ya tiene calen...   \n",
      "fiesta      1391205  Estados Unidos: Estudiantes organizan “fiestas...   \n",
      "cubano      1390965  Primera Ministra de Barbados intercambia con c...   \n",
      "fase        1390397  La Habana pasa a la primera fase de recuperaci...   \n",
      "entrar      1389093  Morales Ojeda: Estábamos convencidos de que La...   \n",
      "gracia      1387861  Luego de 23 años, gemelos se encuentran en Bra...   \n",
      "materia     1384867  Decisiones que tomará Cuba en materia de comer...   \n",
      "viajar      1375071  Merkel descarta viajar a EE.UU. para cumbre de...   \n",
      "falto       1350601         La COVID-19 y un mundo falto de estadistas   \n",
      "caballo     1345963     El caballo de Menocal y los gallos de Mendieta   \n",
      "habano      1327213  Abre en La Habana la edición 22 del Festival d...   \n",
      "comentario  1313213  Indígenas prometen llevar a Bolsonaro ante los...   \n",
      "saludo      1301309  Inauguración de obras en La Habana en saludo a...   \n",
      "llorar      1293605  Festival de Cine: ¿Reír, llorar o reflexionar?...   \n",
      "prohibir    1270289          Prohibir o incentivar, he ahí la cuestión   \n",
      "\n",
      "                                                          url        date  \n",
      "dios        http://www.cubadebate.cu/noticias/2020/07/05/n...  2020/07/05  \n",
      "caso        http://www.cubadebate.cu/noticias/2020/07/05/c...  2020/07/05  \n",
      "pais        http://www.cubadebate.cu/noticias/2020/07/04/m...  2020/07/04  \n",
      "nacional    http://www.cubadebate.cu/noticias/2020/07/04/e...  2020/07/04  \n",
      "fiesta      http://www.cubadebate.cu/noticias/2020/07/03/e...  2020/07/03  \n",
      "cubano      http://www.cubadebate.cu/noticias/2020/07/02/p...  2020/07/02  \n",
      "fase        http://www.cubadebate.cu/noticias/2020/07/01/l...  2020/07/01  \n",
      "entrar      http://www.cubadebate.cu/noticias/2020/06/29/m...  2020/06/29  \n",
      "gracia      http://www.cubadebate.cu/noticias/2020/06/26/l...  2020/06/26  \n",
      "materia     http://www.cubadebate.cu/noticias/2020/06/18/e...  2020/06/18  \n",
      "viajar      http://www.cubadebate.cu/noticias/2020/05/30/m...  2020/05/30  \n",
      "falto       http://www.cubadebate.cu/opinion/2020/04/14/la...  2020/04/14  \n",
      "caballo     http://www.cubadebate.cu/especiales/2020/04/03...  2020/04/03  \n",
      "habano      http://www.cubadebate.cu/cuba/2020/02/24/abre-...  2020/02/24  \n",
      "comentario  http://www.cubadebate.cu/noticias/2020/01/24/i...  2020/01/24  \n",
      "saludo      http://www.cubadebate.cu/noticias/2019/12/27/i...  2019/12/27  \n",
      "llorar      http://www.cubadebate.cu/especiales/2019/12/12...  2019/12/12  \n",
      "prohibir    http://www.cubadebate.cu/opinion/2019/10/27/pr...  2019/10/27  \n",
      "\n",
      "Saved top_word_post.json\n",
      "\n",
      "Rewrite index.html.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Words (Lemma) with most TF-IDF (unigram: ngram=1)\n",
    "df = (\n",
    "    DataFrame.from_dict(\n",
    "        data=ordered_comments_tfidf, dtype=int, orient=\"index\", columns=[\"TF-IDF\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(index=str, columns={\"index\": \"Word\"})\n",
    ")\n",
    "rows, columns = df.shape\n",
    "print(f\"Panda DataFrame shape:({rows}, {columns})\")\n",
    "\n",
    "df_gb = df.groupby([\"TF-IDF\"], sort=False).first().reset_index()\n",
    "rows, columns = df_gb.shape\n",
    "print(f\"GroupBy DataFrame shape:({rows}, {columns})\\n\")\n",
    "print(df_gb)\n",
    "\n",
    "words_token = list(df_gb[\"Word\"].values)\n",
    "searches_dict = dict()\n",
    "searches_ids = set()\n",
    "\n",
    "for w_token in words_token:\n",
    "    search_list = get_searches([w_token])\n",
    "    # Get first searched result\n",
    "    if len(search_list) > 0:\n",
    "        search_id = search_list[0][\"id\"]\n",
    "        if search_id not in searches_ids:\n",
    "            searches_ids.add(search_id)\n",
    "            searches_dict[w_token] = search_list[0]\n",
    "\n",
    "# DataFrame First Search Posts\n",
    "if len(searches_dict) > 0:\n",
    "    searches_df = DataFrame.from_dict(data=searches_dict, orient=\"index\",).drop(\n",
    "        columns=[\"type\", \"subtype\", \"_links\"]\n",
    "    )\n",
    "\n",
    "    df_dates = searches_df[\"url\"].str.extract(r\"(?P<date>\\d{4}/\\d{2}/\\d{2})\")\n",
    "\n",
    "    searches_by_dates = searches_df.join(df_dates).sort_values(\n",
    "        by=\"date\", ascending=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop Word Token and Post\\n\")\n",
    "    print(searches_by_dates)\n",
    "    searches_by_dates.to_json(os.path.join(_dir, \"top_word_post.json\"))\n",
    "    print(\"\\nSaved top_word_post.json\\n\")\n",
    "\n",
    "    with open(\"index.html\", \"w\", encoding=\"utf-8\") as f_index, open(\n",
    "        \"index.tpl\", \"r\"\n",
    "    ) as file:\n",
    "        tpl = file.read()\n",
    "        index_template = Template(tpl)\n",
    "\n",
    "        text_link = \"<ul>\\n\"\n",
    "        for _index, _title, _url in searches_by_dates.reset_index()[\n",
    "            [\"index\", \"title\", \"url\"]\n",
    "        ].values:\n",
    "            text_link += (\n",
    "                f\"\\t<li><a href='{_url}' rel='external' \"\n",
    "                f\"data-token='{_index}'>{_title}</a></li>\\n\"\n",
    "            )\n",
    "        text_link += \"</ul>\"\n",
    "        # Create index.html from index.tpl\n",
    "        f_index.write(index_template.substitute(CUBADEBATE_LINKS=text_link))\n",
    "\n",
    "    print(\"Rewrite index.html.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open** [index.html](index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
