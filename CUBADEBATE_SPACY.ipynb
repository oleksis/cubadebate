{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 52.70s\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from collections import Counter, OrderedDict\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from string import Template\n",
    "import time\n",
    "from typing import List, Optional, Dict, Tuple, Iterable, Any\n",
    "\n",
    "import requests\n",
    "import dask\n",
    "from dask import bag as db\n",
    "from dask.delayed import delayed, Delayed\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Annotations\n",
    "ElementList = List[Optional[Dict[str, Any]]]\n",
    "Document = str\n",
    "DocumentList = List[Document]\n",
    "DocumentNormalizedList = List[DocumentList]\n",
    "TermFrequency = Dict[str, float]\n",
    "IDF = Dict[str, float]\n",
    "TFIDF = Dict[str, float]\n",
    "TFIDF_List = List[TFIDF]\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "SEARCH_ENDPOINT = CUBADEBATE_API + \"search/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionErrorRequests = requests.exceptions.ConnectionError\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_elements_json(url: str, **kwargs) -> ElementList:\n",
    "    \"\"\"Get JSON of list elements from endpoint API Wordpress v2\n",
    "\n",
    "    RETURNS (ElementList): Delayed Response List of elements in JSON format\n",
    "    to compute for Dask.Bag\n",
    "    \"\"\"\n",
    "    params = {\"page\": '1'}\n",
    "    params.update(kwargs)\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            resp = session.get(url, params=params)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json()\n",
    "                return results\n",
    "    except ConnectionErrorRequests:\n",
    "        pass  # Try again! Occurred Connection Error\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_comments(pages: int) -> List[str]:\n",
    "    \"\"\"Get List of comments per page\"\"\"\n",
    "    comments_delayed = (get_elements_json(url=COMMENTS_ENDPOINT, page=str(page))\n",
    "                        for page in range(1, pages + 1))\n",
    "\n",
    "    comments_bag = db.from_delayed(comments_delayed).map(lambda d: d['content']['rendered'])\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler='threads'):\n",
    "        comments: List[str] = comments_bag.compute()\n",
    "    return comments\n",
    "\n",
    "\n",
    "def get_searches(words: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get List of searches one page\"\"\"\n",
    "    search_delayed = (get_elements_json(url=SEARCH_ENDPOINT, search=word)\n",
    "                      for word in words)\n",
    "\n",
    "    searches_bag = db.from_delayed(search_delayed)\n",
    "    with dask.config.set(scheduler='threads'):\n",
    "        searches = searches_bag.compute()\n",
    "    return searches\n",
    "\n",
    "\n",
    "_start = time.time()\n",
    "comments_list = get_comments(pages=1)\n",
    "_end = time.time()\n",
    "print(f'{len(comments_list)} Comments downloaded in {(_end - _start):.2f}s')\n",
    "# comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Comments downloaded in 379.75s\n"
     ]
    }
   ],
   "source": [
    "NUM_PAGES = 100\n",
    "\n",
    "_start = time.time()\n",
    "corpus = get_comments(pages=NUM_PAGES)\n",
    "_end = time.time()\n",
    "print(f'{len(corpus)} Comments downloaded in {(_end - _start):.2f}s')\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20379,
     "status": "ok",
     "timestamp": 1587741838415,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "b4r5kXdGlK8r",
    "outputId": "c9245c95-647a-4e15-efb6-3b06ae130184",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Documents normalized in 327.63s\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "bow_lemma_token = dict()\n",
    "\n",
    "\n",
    "def remplace_accents(text: str) -> str:\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r'á', 'a', text, flags=re.I)\n",
    "    text = re.sub(r'é', 'e', text, flags=re.I)\n",
    "    text = re.sub(r'í', 'i', text, flags=re.I)\n",
    "    text = re.sub(r'ó', 'o', text, flags=re.I)\n",
    "    text = re.sub(r'ú', 'u', text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(markup: str) -> str:\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', markup)\n",
    "    url_pattern = r'(http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_token(token: Token) -> str:\n",
    "        \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "        result = remplace_accents(token.lemma_.strip().lower())\n",
    "        return result\n",
    "\n",
    "\n",
    "def is_token_allowed(token: Token) -> bool:\n",
    "        \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "        if (not token or not token.string.strip() or\n",
    "                token.is_stop or token.is_punct or len(token) < 3):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    \n",
    "@delayed\n",
    "def clean(doc: str) -> List[str]:\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = []\n",
    "    \n",
    "    for token in nlp(text):\n",
    "        if is_token_allowed(token):\n",
    "            word_lemma = preprocess_token(token)\n",
    "            bow_lemma_token[word_lemma] = token\n",
    "            tokens.append(word_lemma)\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_documents_delayed(documents: DocumentList) -> List[Delayed]:\n",
    "    \"\"\"Return List[Delayed] clean document\"\"\"\n",
    "    results = []  # type: List[dask.delayed.Delayed]\n",
    "\n",
    "    for document in documents:\n",
    "        results.append(clean(document))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_documents_normalized(documents: DocumentList) -> DocumentNormalizedList:\n",
    "    \"\"\"Return DocumentNormalizedList clean document\"\"\"\n",
    "    with dask.config.set(scheduler='threads'):\n",
    "        docs_normalized = list(*dask.compute(get_documents_delayed(documents)))\n",
    "    return docs_normalized\n",
    "\n",
    "\n",
    "# Process corpus using Dask.delayed\n",
    "_start = time.time()\n",
    "documents_normalized = get_documents_normalized(corpus)\n",
    "_end = time.time()\n",
    "print(f'{len(documents_normalized)} Documents normalized in {(_end - _start):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5335,
     "status": "ok",
     "timestamp": 1587741858758,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1Dce5BcYlK88",
    "outputId": "c47b6fdf-d088-4f58-a67f-1698c65f258a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF comments list\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "def comment_tf(document: DocumentList) -> TermFrequency:\n",
    "    \"\"\"Term Frequency for a document\"\"\"\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "\n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def comments_tf(documents: DocumentNormalizedList) -> List[TermFrequency]:\n",
    "    \"\"\"Term Frequency for many document\"\"\"\n",
    "    return [comment_tf(comment)\n",
    "            for comment in documents]\n",
    "\n",
    "\n",
    "def count_dict(documents: DocumentNormalizedList) -> Dict[str, int]:\n",
    "    \"\"\"Counter the word in all documents at least ones\"\"\"\n",
    "    counts = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            _value = counts.get(word, 0)\n",
    "            counts[word] = _value + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def idf_dict(documents: DocumentNormalizedList) -> IDF:\n",
    "    \"\"\"Inverse Document Frequency in all Documents\"\"\"\n",
    "    idf_d = dict()\n",
    "\n",
    "    counts = count_dict(documents)\n",
    "\n",
    "    for word in counts:\n",
    "        idf_d[word] = math.log(len(documents) / counts[word])\n",
    "\n",
    "    return idf_d\n",
    "\n",
    "\n",
    "def comments_tfidf(documents: DocumentNormalizedList) -> TFIDF_List:\n",
    "    \"\"\"TF-IDF of all Documents Normalized\"\"\"\n",
    "    tfidf_comments = []\n",
    "\n",
    "    idf_comments = idf_dict(documents)\n",
    "\n",
    "    def compute_tfidf_comment(document: DocumentList) -> TFIDF:\n",
    "        \"\"\"Compute TF-IDF for a Document\"\"\"\n",
    "        tfidf_comment = dict()\n",
    "\n",
    "        tf_comment = comment_tf(document)\n",
    "\n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "\n",
    "        return tfidf_comment\n",
    "\n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "\n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "tfidf_list = comments_tfidf(documents_normalized)\n",
    "# tfidf_list\n",
    "print('TF-IDF comments list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "unordered_tfidf: Dict[str, float] = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for w, value in tfidf.items():\n",
    "        if w in unordered_tfidf:\n",
    "            unordered_tfidf[w] += value\n",
    "        else:\n",
    "            unordered_tfidf[w] = value\n",
    "\n",
    "\n",
    "def sort_tfidf(tfidf_unordered) -> Iterable[Tuple[Any, Any]]:\n",
    "    return sorted(tfidf_unordered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# unordered_tfidf\n",
    "ordered_comments_tfidf = OrderedDict(sort_tfidf(unordered_tfidf))\n",
    "\n",
    "#WordCloud with word_token\n",
    "token_comments_tfidf = dict()\n",
    "\n",
    "for lemma_, value in ordered_comments_tfidf.items():\n",
    "    _token = str(bow_lemma_token[lemma_])\n",
    "    token_comments_tfidf[_token] = value\n",
    "\n",
    "# Save to JSON file\n",
    "with open('comments_tfidf.json', 'w') as file_json:\n",
    "    json.dump(token_comments_tfidf, file_json)\n",
    "print('TF-IDF ordered saved to comments_tfidf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO: str = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK: str = 'https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg'\n",
    "\n",
    "# download mask images\n",
    "# !wget http://media.cubadebate.cu/wp-content/gallery/la-habana-nocturna/app_la-habana_05.jpg -O la_hababa.jpg\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, 'wb') as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print('Image downloaded.')\n",
    "    else:\n",
    "        print('Image No downloaded!')\n",
    "else:\n",
    "    image_downloaded: str = IMG_CAPITOLIO.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "    print(f\"Image: {image_downloaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = 'wordcloud_cubadebate.png'\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=1000,\n",
    "    #     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color='white',\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color='steelblue',\n",
    "    stopwords=STOP_WORDS).generate_from_frequencies(token_comments_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print('WordCloud Cubadebate image saved.')\n",
    "fig = plt.figure(figsize = (22, 15),)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1587742239903,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Gs20IRy8cvpQ",
    "outputId": "cb62b934-baf8-4f47-b9b0-b920740b9d4a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def export_image2html(image_name: str) -> None:\n",
    "    \"\"\"Export a image to html file\"\"\"\n",
    "    with open(image_name, 'rb') as file_img:\n",
    "        data = base64.b64encode(file_img.read()).decode('utf-8')\n",
    "\n",
    "    img_str = '''\n",
    "    <img width=\"100%\" height=\"100%\" \n",
    "    src=\"data:image/png;base64,{}\" />\n",
    "    '''.format(data)\n",
    "\n",
    "    with open('wordcloud_cubadebate.html', 'w') as _html:\n",
    "        _html.write(img_str)\n",
    "\n",
    "\n",
    "# export_image2html(IMG_WORDCLOUD)\n",
    "# print('Exported image to html.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda DataFrame shape:(6069, 2)\n",
      "GroupBy DataFrame shape:(18, 2)\n",
      "\n",
      "    TF-IDF        Word\n",
      "0       19     gracias\n",
      "1       18  comentario\n",
      "2       16        cuba\n",
      "3       15      cubana\n",
      "4       14       casos\n",
      "5       12     turismo\n",
      "6       11    personas\n",
      "7       10      medida\n",
      "8        9       abrir\n",
      "9        8      espero\n",
      "10       7       vivir\n",
      "11       6      dinero\n",
      "12       5     olvidan\n",
      "13       4    opciones\n",
      "14       3      causar\n",
      "15       2       cobro\n",
      "16       1     afueras\n",
      "17       0   movilidad\n",
      "\n",
      "Top Word Token and Post\n",
      "\n",
      "                 id                                              title  \\\n",
      "cuba        1368567   Roberlandy Simón y Roamy Alonso ya están en Cuba   \n",
      "cubana      1368439  Qué trae la prensa cubana, domingo 17 de mayo ...   \n",
      "casos       1368483  Cuba reporta 10 casos positivos a la COVID-19,...   \n",
      "medida      1368335  Medidas adoptadas en Bundesliga podrían aplica...   \n",
      "turismo     1367727  Ministerio del Turismo: Cuba no reabrirá aún s...   \n",
      "personas    1365763  COVID-19 en el mundo: Los contagiados con el n...   \n",
      "vivir       1360907  Romerías de Mayo se vivirán desde Internet a p...   \n",
      "dinero      1360021  COVID-19: ¿Y si la vacuna pagada con dinero pú...   \n",
      "gracias     1358705  La opinión del lector: ¡Gracias, ya tengo mi m...   \n",
      "causar      1358407  COVID-19 en el mundo: Eliminar confinamiento d...   \n",
      "movilidad   1351325  ¿Cómo se ha reordenado el transporte para gara...   \n",
      "opciones    1330955  Desarrolladores ofrecen nuevas opciones para e...   \n",
      "comentario  1313213  Indígenas prometen llevar a Bolsonaro ante los...   \n",
      "cobro       1312023  Aplican en Holguín nueva modalidad de cobro de...   \n",
      "espero      1312245  “El diario de René”: Espero que para el 2002 t...   \n",
      "olvidan     1147005  Los premios Globe Soccer se olvidan de Modrić ...   \n",
      "afueras     1023425  Filme \"Tres anuncios en las afueras\", gana el ...   \n",
      "\n",
      "                                                          url        date  \n",
      "cuba        http://www.cubadebate.cu/noticias/2020/05/17/r...  2020/05/17  \n",
      "cubana      http://www.cubadebate.cu/noticias/2020/05/17/q...  2020/05/17  \n",
      "casos       http://www.cubadebate.cu/noticias/2020/05/17/c...  2020/05/17  \n",
      "medida      http://www.cubadebate.cu/noticias/2020/05/16/m...  2020/05/16  \n",
      "turismo     http://www.cubadebate.cu/noticias/2020/05/15/m...  2020/05/15  \n",
      "personas    http://www.cubadebate.cu/noticias/2020/05/12/c...  2020/05/12  \n",
      "vivir       http://www.cubadebate.cu/noticias/2020/05/02/r...  2020/05/02  \n",
      "dinero      http://www.cubadebate.cu/especiales/2020/05/01...  2020/05/01  \n",
      "gracias     http://www.cubadebate.cu/noticias/2020/04/28/l...  2020/04/28  \n",
      "causar      http://www.cubadebate.cu/noticias/2020/04/28/c...  2020/04/28  \n",
      "movilidad   http://www.cubadebate.cu/noticias/2020/04/14/c...  2020/04/14  \n",
      "opciones    http://www.cubadebate.cu/noticias/2020/03/04/d...  2020/03/04  \n",
      "comentario  http://www.cubadebate.cu/noticias/2020/01/24/i...  2020/01/24  \n",
      "cobro       http://www.cubadebate.cu/noticias/2020/01/22/a...  2020/01/22  \n",
      "espero      http://www.cubadebate.cu/especiales/2019/12/25...  2019/12/25  \n",
      "olvidan     http://www.cubadebate.cu/noticias/2018/12/26/l...  2018/12/26  \n",
      "afueras     http://www.cubadebate.cu/noticias/2018/02/19/f...  2018/02/19  \n",
      "\n",
      "Saved top_word_post.json\n",
      "\n",
      "Rewrite index.html.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Words (Tokens) with most TF-IDF\n",
    "df = DataFrame.from_dict(data=token_comments_tfidf, dtype=int,\n",
    "                         orient='index', columns=['TF-IDF']). \\\n",
    "    reset_index().rename(index=str, columns={'index': 'Word'})\n",
    "rows, columns = df.shape\n",
    "print(f'Panda DataFrame shape:({rows}, {columns})')\n",
    "\n",
    "df_gb = df.groupby(['TF-IDF'], sort=False).first().reset_index()\n",
    "rows, columns = df_gb.shape\n",
    "print(f'GroupBy DataFrame shape:({rows}, {columns})\\n')\n",
    "print(df_gb)\n",
    "\n",
    "words_token = list(df_gb['Word'].values)\n",
    "searches_dict = dict()\n",
    "searches_ids = set()\n",
    "\n",
    "for w_token in words_token:\n",
    "    search_list = get_searches([w_token])\n",
    "    # Get first searched result\n",
    "    if len(search_list) > 0:\n",
    "        search_id = search_list[0]['id']\n",
    "        if not search_id in searches_ids:\n",
    "            searches_ids.add(search_id)\n",
    "            searches_dict[w_token] = search_list[0]\n",
    "\n",
    "# DataFrame First Search Posts\n",
    "if len(searches_dict) > 0:\n",
    "    searches_df = DataFrame.from_dict(data=searches_dict, orient='index', ) \\\n",
    "        .drop(columns=['type', 'subtype', '_links'])\n",
    "    \n",
    "    df_dates = searches_df['url'].str.extract('(?P<date>\\d{4}/\\d{2}/\\d{2})')\n",
    "\n",
    "    searches_by_dates = searches_df.join(df_dates).sort_values(by='date', ascending=False)\n",
    "\n",
    "    print('\\nTop Word Token and Post\\n')\n",
    "    print(searches_by_dates)\n",
    "    searches_by_dates.to_json(os.path.join(_dir, 'top_word_post.json'))\n",
    "    print('\\nSaved top_word_post.json\\n')\n",
    "\n",
    "    with open('index.html', 'w', encoding='utf-8') as f_index, open('index.tpl', 'r') as file:\n",
    "        tpl = file.read()\n",
    "        index_template = Template(tpl)\n",
    "        \n",
    "        text_link = \"<ul>\\n\"\n",
    "        for _index, _title, _url in searches_by_dates.reset_index()[['index', 'title', 'url']].values:\n",
    "            text_link += f\"\\t<li><a href='{_url}' rel='external' data-token='{_index}'>{_title}</a></li>\\n\"\n",
    "        text_link += \"</ul>\"\n",
    "        # Create index.html from index.tpl\n",
    "        f_index.write(index_template.substitute(CUBADEBATE_LINKS=text_link))\n",
    "\n",
    "    print('Rewrite index.html.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
