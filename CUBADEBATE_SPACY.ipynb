{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import Counter, OrderedDict\n",
    "from datetime import datetime\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from string import Template\n",
    "import time\n",
    "from typing import List, Optional, Dict, Tuple, Iterable, Any\n",
    "\n",
    "import requests\n",
    "import dask\n",
    "from dask import bag as db\n",
    "from dask import dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Annotations\n",
    "ElementList = List[Optional[Dict[str, Any]]]\n",
    "Document = str\n",
    "DocumentList = List[Document]\n",
    "DocumentNormalizedList = List[DocumentList]\n",
    "TermFrequency = Dict[str, float]\n",
    "IDF = Dict[str, float]\n",
    "TFIDF = Dict[str, float]\n",
    "TFIDF_List = List[TFIDF]\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "SEARCH_ENDPOINT = CUBADEBATE_API + \"search/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionErrorRequests = requests.exceptions.ConnectionError\n",
    "\n",
    "FECHA_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "\n",
    "def create_datetime(string: str) -> Optional[datetime]:\n",
    "    \"\"\"Create datetime from string '2014-07-03T23:27:51'\"\"\"\n",
    "    date_time = None\n",
    "\n",
    "    try:\n",
    "        if string is not None:\n",
    "            date_time = datetime.strptime(string, FECHA_FORMAT)\n",
    "    except ValueError:\n",
    "        print(\"Format no valid!\")\n",
    "\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def save_elements_json(name: str, line: str = None, mode=\"w\"):\n",
    "    \"\"\"Save elements to file, by lines JSON elments.\"\"\"\n",
    "    with open(name, mode) as _file:\n",
    "        if line:\n",
    "            _file.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def read_elements_json(name: str) -> Optional[list]:\n",
    "    \"\"\"Read by lines JSON data in file name.\n",
    "    Return list of elements or None\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(name, \"r\") as _file:\n",
    "            for line in _file.readlines():\n",
    "                if line:\n",
    "                    data += json.loads(line)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"Error reading lines of _file: {name}\")\n",
    "    except FileNotFoundError as f_error:\n",
    "        print(f_error)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_comments_file(name: str) -> DataFrame:\n",
    "    \"\"\"Get List of comments from file name\"\"\"\n",
    "    comments_json = read_elements_json(name)\n",
    "    comments_bag = db.from_sequence(comments_json).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[str] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_elements_json(url: str, **kwargs) -> ElementList:\n",
    "    \"\"\"Get JSON of list elements from endpoint API Wordpress v2\n",
    "\n",
    "    RETURNS (ElementList): Delayed Response List of elements in JSON format\n",
    "    to compute for Dask.Bag\n",
    "    \"\"\"\n",
    "    params = {\"page\": \"1\"}\n",
    "    params.update(kwargs)\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            resp = session.get(url, params=params)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json()\n",
    "                if params.get(\"file\"):\n",
    "                    save_elements_json(\n",
    "                        params.get(\"file\", \"elements.dat\"), resp.text, \"a\"\n",
    "                    )\n",
    "                return results\n",
    "    except ConnectionErrorRequests:\n",
    "        pass  # Try again! Occurred Connection Error\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_comments(pages: int) -> DataFrame:\n",
    "    \"\"\"Get List of comments per page\"\"\"\n",
    "    comments_delayed = (\n",
    "        get_elements_json(url=COMMENTS_ENDPOINT, page=str(page), file=\"comments.dat\")\n",
    "        for page in range(1, pages + 1)\n",
    "    )\n",
    "\n",
    "    comments_bag = db.from_delayed(comments_delayed).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[dict] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "def get_searches(words: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get List of searches one page\"\"\"\n",
    "    search_delayed = (\n",
    "        get_elements_json(url=SEARCH_ENDPOINT, search=word) for word in words\n",
    "    )\n",
    "\n",
    "    searches_bag = db.from_delayed(search_delayed)\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        searches = searches_bag.compute()\n",
    "    return searches\n",
    "\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "bow_lemma_token: Dict[str, Token] = dict()\n",
    "\n",
    "\n",
    "def remplace_accents(text: str) -> str:\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r\"á\", \"a\", text, flags=re.I)\n",
    "    text = re.sub(r\"é\", \"e\", text, flags=re.I)\n",
    "    text = re.sub(r\"í\", \"i\", text, flags=re.I)\n",
    "    text = re.sub(r\"ó\", \"o\", text, flags=re.I)\n",
    "    text = re.sub(r\"ú\", \"u\", text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(markup: str) -> str:\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", markup)\n",
    "    url_pattern = r\"(http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_token(token: Token) -> str:\n",
    "    \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "    result = remplace_accents(token.lemma_.strip().lower())\n",
    "    return result\n",
    "\n",
    "\n",
    "def is_token_allowed(token: Token) -> bool:\n",
    "    \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "    # Avoid token: inmiscuyéndose lemma_ \"inmiscuir el\"\n",
    "    if (\n",
    "        not token\n",
    "        or token.is_space\n",
    "        or token.is_stop\n",
    "        or token.is_punct\n",
    "        or len(token) < 3\n",
    "        or \" \" in token.lemma_.strip()\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean(doc: str) -> List[str]:\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = []\n",
    "\n",
    "    for token in nlp(text):\n",
    "        if is_token_allowed(token):\n",
    "            word_lemma = preprocess_token(token)\n",
    "            bow_lemma_token[word_lemma] = token\n",
    "            tokens.append(word_lemma)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_ngram(document: DocumentList, ngram=1) -> DocumentList:\n",
    "    \"\"\"Create N-Gram from document\"\"\"\n",
    "    return [\n",
    "        \" \".join(document[i : i + ngram]) for i in range(len(document) - (ngram - 1))\n",
    "    ]\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "def comment_tf(document: DocumentList) -> TermFrequency:\n",
    "    \"\"\"Term Frequency for a document\"\"\"\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "\n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def comments_tf(documents: DocumentNormalizedList) -> List[TermFrequency]:\n",
    "    \"\"\"Term Frequency for many document\"\"\"\n",
    "    return [comment_tf(comment) for comment in documents]\n",
    "\n",
    "\n",
    "def count_dict(documents: DocumentNormalizedList) -> Dict[str, int]:\n",
    "    \"\"\"Counter the word in all documents at least ones\"\"\"\n",
    "    counts: Dict[str, int] = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            _value = counts.get(word, 0)\n",
    "            counts[word] = _value + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def idf_dict(documents: DocumentNormalizedList) -> IDF:\n",
    "    \"\"\"Inverse Document Frequency in all Documents\"\"\"\n",
    "    idf_d = dict()\n",
    "\n",
    "    counts = count_dict(documents)\n",
    "\n",
    "    for word in counts:\n",
    "        idf_d[word] = math.log(len(documents) / counts[word])\n",
    "\n",
    "    return idf_d\n",
    "\n",
    "\n",
    "def comments_tfidf(documents: DocumentNormalizedList) -> TFIDF_List:\n",
    "    \"\"\"TF-IDF of all Documents Normalized\"\"\"\n",
    "    tfidf_comments = []\n",
    "\n",
    "    idf_comments = idf_dict(documents)\n",
    "\n",
    "    def compute_tfidf_comment(document: DocumentList) -> TFIDF:\n",
    "        \"\"\"Compute TF-IDF for a Document\"\"\"\n",
    "        tfidf_comment = dict()\n",
    "\n",
    "        tf_comment = comment_tf(document)\n",
    "\n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "\n",
    "        return tfidf_comment\n",
    "\n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "\n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "def sort_tfidf(tfidf_unordered) -> Iterable[Tuple[Any, Any]]:\n",
    "    return sorted(tfidf_unordered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def export_image2html(image_name: str) -> None:\n",
    "    \"\"\"Export a image to html file\"\"\"\n",
    "    with open(image_name, \"rb\") as file_img:\n",
    "        data = base64.b64encode(file_img.read()).decode(\"utf-8\")\n",
    "\n",
    "    img_str = \"\"\"\n",
    "    <img width=\"100%\" height=\"100%\"\n",
    "    src=\"data:image/png;base64,{}\" />\n",
    "    \"\"\".format(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    with open(\"wordcloud_cubadebate.html\", \"w\") as _html:\n",
    "        _html.write(img_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "## Download Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 1.56s\n"
     ]
    }
   ],
   "source": [
    "_start = time.time()\n",
    "df_comments =  get_comments(pages=1)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# df_comments.info()\n",
    "# df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Comments downloaded in 259.69s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9162977</td>\n",
       "      <td>1517181</td>\n",
       "      <td>Hugo Andrés Govin Díaz</td>\n",
       "      <td>2021-03-21 09:21:25</td>\n",
       "      <td>&lt;p&gt;Es decir, muchos lo vieron, otros tantos lo...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9162805</td>\n",
       "      <td>1517181</td>\n",
       "      <td>Yury Rivas</td>\n",
       "      <td>2021-03-21 08:14:29</td>\n",
       "      <td>&lt;p&gt;Interesante artículo,sería genial darle seg...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9162611</td>\n",
       "      <td>1517181</td>\n",
       "      <td>De_De</td>\n",
       "      <td>2021-03-21 06:15:55</td>\n",
       "      <td>&lt;p&gt;Bueno testigos ahí bastantes aquí en mi cas...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9162501</td>\n",
       "      <td>1517181</td>\n",
       "      <td>Lpf</td>\n",
       "      <td>2021-03-21 03:32:54</td>\n",
       "      <td>&lt;p&gt;Muy interesante y explicativo el artículo. ...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9162411</td>\n",
       "      <td>1517181</td>\n",
       "      <td>Jorge</td>\n",
       "      <td>2021-03-21 00:47:17</td>\n",
       "      <td>&lt;p&gt;Nadie save x fin q es eso&lt;/p&gt;\\n</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9158151</td>\n",
       "      <td>1516599</td>\n",
       "      <td>Aries</td>\n",
       "      <td>2021-03-19 23:52:37</td>\n",
       "      <td>&lt;p&gt;Como?? Vamos a ser abducidos por los aliení...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9158149</td>\n",
       "      <td>1516599</td>\n",
       "      <td>varem vargas</td>\n",
       "      <td>2021-03-19 23:52:17</td>\n",
       "      <td>&lt;p&gt;en el municipio caimanera pudimos ver el de...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>9158147</td>\n",
       "      <td>1516563</td>\n",
       "      <td>Cynthia</td>\n",
       "      <td>2021-03-19 23:51:47</td>\n",
       "      <td>&lt;p&gt;Para MFP:&lt;br /&gt;\\nSi la Resolución dice que ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9158145</td>\n",
       "      <td>1516599</td>\n",
       "      <td>EMS</td>\n",
       "      <td>2021-03-19 23:51:43</td>\n",
       "      <td>&lt;p&gt;A esa misma hora y con esas mismas caracter...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9158141</td>\n",
       "      <td>1516599</td>\n",
       "      <td>Daniela Amonzabel Ortega</td>\n",
       "      <td>2021-03-19 23:51:42</td>\n",
       "      <td>&lt;p&gt;Nosotras vivimos en Holguin y claramente  m...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post               author_name                date  \\\n",
       "0    9162977  1517181    Hugo Andrés Govin Díaz 2021-03-21 09:21:25   \n",
       "1    9162805  1517181                Yury Rivas 2021-03-21 08:14:29   \n",
       "2    9162611  1517181                     De_De 2021-03-21 06:15:55   \n",
       "3    9162501  1517181                       Lpf 2021-03-21 03:32:54   \n",
       "4    9162411  1517181                     Jorge 2021-03-21 00:47:17   \n",
       "..       ...      ...                       ...                 ...   \n",
       "995  9158151  1516599                     Aries 2021-03-19 23:52:37   \n",
       "996  9158149  1516599              varem vargas 2021-03-19 23:52:17   \n",
       "997  9158147  1516563                   Cynthia 2021-03-19 23:51:47   \n",
       "998  9158145  1516599                       EMS 2021-03-19 23:51:43   \n",
       "999  9158141  1516599  Daniela Amonzabel Ortega 2021-03-19 23:51:42   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Es decir, muchos lo vieron, otros tantos lo...   \n",
       "1    <p>Interesante artículo,sería genial darle seg...   \n",
       "2    <p>Bueno testigos ahí bastantes aquí en mi cas...   \n",
       "3    <p>Muy interesante y explicativo el artículo. ...   \n",
       "4                   <p>Nadie save x fin q es eso</p>\\n   \n",
       "..                                                 ...   \n",
       "995  <p>Como?? Vamos a ser abducidos por los aliení...   \n",
       "996  <p>en el municipio caimanera pudimos ver el de...   \n",
       "997  <p>Para MFP:<br />\\nSi la Resolución dice que ...   \n",
       "998  <p>A esa misma hora y con esas mismas caracter...   \n",
       "999  <p>Nosotras vivimos en Holguin y claramente  m...   \n",
       "\n",
       "                                                  link  \n",
       "0    http://www.cubadebate.cu/especiales/2021/03/21...  \n",
       "1    http://www.cubadebate.cu/especiales/2021/03/21...  \n",
       "2    http://www.cubadebate.cu/especiales/2021/03/21...  \n",
       "3    http://www.cubadebate.cu/especiales/2021/03/21...  \n",
       "4    http://www.cubadebate.cu/especiales/2021/03/21...  \n",
       "..                                                 ...  \n",
       "995  http://www.cubadebate.cu/noticias/2021/03/19/r...  \n",
       "996  http://www.cubadebate.cu/noticias/2021/03/19/r...  \n",
       "997  http://www.cubadebate.cu/noticias/2021/03/19/p...  \n",
       "998  http://www.cubadebate.cu/noticias/2021/03/19/r...  \n",
       "999  http://www.cubadebate.cu/noticias/2021/03/19/r...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate file comments.dat\n",
    "save_elements_json(\"comments.dat\")\n",
    "NUM_PAGES = 100\n",
    "\n",
    "_start = time.time()\n",
    "df_comments = get_comments(pages=NUM_PAGES)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not df_comments.empty, \"No comments to process\"\n",
    "# Uncomment the next for get the comments from file\n",
    "# df_comments = get_comments_file(\"comments.dat\")\n",
    "# print(f\"{len(df_comments)} Comments from file comments.dat\")\n",
    "# df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process corpus. Normalize. Using threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Documents normalized in 245.31s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9162977</td>\n",
       "      <td>1517181</td>\n",
       "      <td>Hugo Andrés Govin Díaz</td>\n",
       "      <td>2021-03-21 09:21:25</td>\n",
       "      <td>&lt;p&gt;Es decir, muchos lo vieron, otros tantos lo...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "      <td>[ver, tanto, reportar, sentir, cientifico, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9162805</td>\n",
       "      <td>1517181</td>\n",
       "      <td>Yury Rivas</td>\n",
       "      <td>2021-03-21 08:14:29</td>\n",
       "      <td>&lt;p&gt;Interesante artículo,sería genial darle seg...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "      <td>[interesante, articulo, genial, seguimiento, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9162611</td>\n",
       "      <td>1517181</td>\n",
       "      <td>De_De</td>\n",
       "      <td>2021-03-21 06:15:55</td>\n",
       "      <td>&lt;p&gt;Bueno testigos ahí bastantes aquí en mi cas...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "      <td>[testigo, bastante, casa, hija, año, encotrar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9162501</td>\n",
       "      <td>1517181</td>\n",
       "      <td>Lpf</td>\n",
       "      <td>2021-03-21 03:32:54</td>\n",
       "      <td>&lt;p&gt;Muy interesante y explicativo el artículo. ...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "      <td>[interesante, explicativo, articulo, gracia, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9162411</td>\n",
       "      <td>1517181</td>\n",
       "      <td>Jorge</td>\n",
       "      <td>2021-03-21 00:47:17</td>\n",
       "      <td>&lt;p&gt;Nadie save x fin q es eso&lt;/p&gt;\\n</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/21...</td>\n",
       "      <td>[savir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9158151</td>\n",
       "      <td>1516599</td>\n",
       "      <td>Aries</td>\n",
       "      <td>2021-03-19 23:52:37</td>\n",
       "      <td>&lt;p&gt;Como?? Vamos a ser abducidos por los aliení...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/r...</td>\n",
       "      <td>[abducir, alienigena]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9158149</td>\n",
       "      <td>1516599</td>\n",
       "      <td>varem vargas</td>\n",
       "      <td>2021-03-19 23:52:17</td>\n",
       "      <td>&lt;p&gt;en el municipio caimanera pudimos ver el de...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/r...</td>\n",
       "      <td>[municipio, caimanero, poder, destello, flash,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>9158147</td>\n",
       "      <td>1516563</td>\n",
       "      <td>Cynthia</td>\n",
       "      <td>2021-03-19 23:51:47</td>\n",
       "      <td>&lt;p&gt;Para MFP:&lt;br /&gt;\\nSi la Resolución dice que ...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/p...</td>\n",
       "      <td>[mfp, resolucion, cuc, cambiar, cup, aprobar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9158145</td>\n",
       "      <td>1516599</td>\n",
       "      <td>EMS</td>\n",
       "      <td>2021-03-19 23:51:43</td>\n",
       "      <td>&lt;p&gt;A esa misma hora y con esas mismas caracter...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/r...</td>\n",
       "      <td>[hora, caracteristica, sentir, frente]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9158141</td>\n",
       "      <td>1516599</td>\n",
       "      <td>Daniela Amonzabel Ortega</td>\n",
       "      <td>2021-03-19 23:51:42</td>\n",
       "      <td>&lt;p&gt;Nosotras vivimos en Holguin y claramente  m...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/19/r...</td>\n",
       "      <td>[vivir, holguin, claramente, niña, ver, luz, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post               author_name                date  \\\n",
       "0    9162977  1517181    Hugo Andrés Govin Díaz 2021-03-21 09:21:25   \n",
       "1    9162805  1517181                Yury Rivas 2021-03-21 08:14:29   \n",
       "2    9162611  1517181                     De_De 2021-03-21 06:15:55   \n",
       "3    9162501  1517181                       Lpf 2021-03-21 03:32:54   \n",
       "4    9162411  1517181                     Jorge 2021-03-21 00:47:17   \n",
       "..       ...      ...                       ...                 ...   \n",
       "995  9158151  1516599                     Aries 2021-03-19 23:52:37   \n",
       "996  9158149  1516599              varem vargas 2021-03-19 23:52:17   \n",
       "997  9158147  1516563                   Cynthia 2021-03-19 23:51:47   \n",
       "998  9158145  1516599                       EMS 2021-03-19 23:51:43   \n",
       "999  9158141  1516599  Daniela Amonzabel Ortega 2021-03-19 23:51:42   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Es decir, muchos lo vieron, otros tantos lo...   \n",
       "1    <p>Interesante artículo,sería genial darle seg...   \n",
       "2    <p>Bueno testigos ahí bastantes aquí en mi cas...   \n",
       "3    <p>Muy interesante y explicativo el artículo. ...   \n",
       "4                   <p>Nadie save x fin q es eso</p>\\n   \n",
       "..                                                 ...   \n",
       "995  <p>Como?? Vamos a ser abducidos por los aliení...   \n",
       "996  <p>en el municipio caimanera pudimos ver el de...   \n",
       "997  <p>Para MFP:<br />\\nSi la Resolución dice que ...   \n",
       "998  <p>A esa misma hora y con esas mismas caracter...   \n",
       "999  <p>Nosotras vivimos en Holguin y claramente  m...   \n",
       "\n",
       "                                                  link  \\\n",
       "0    http://www.cubadebate.cu/especiales/2021/03/21...   \n",
       "1    http://www.cubadebate.cu/especiales/2021/03/21...   \n",
       "2    http://www.cubadebate.cu/especiales/2021/03/21...   \n",
       "3    http://www.cubadebate.cu/especiales/2021/03/21...   \n",
       "4    http://www.cubadebate.cu/especiales/2021/03/21...   \n",
       "..                                                 ...   \n",
       "995  http://www.cubadebate.cu/noticias/2021/03/19/r...   \n",
       "996  http://www.cubadebate.cu/noticias/2021/03/19/r...   \n",
       "997  http://www.cubadebate.cu/noticias/2021/03/19/p...   \n",
       "998  http://www.cubadebate.cu/noticias/2021/03/19/r...   \n",
       "999  http://www.cubadebate.cu/noticias/2021/03/19/r...   \n",
       "\n",
       "                                                  text  \n",
       "0    [ver, tanto, reportar, sentir, cientifico, con...  \n",
       "1    [interesante, articulo, genial, seguimiento, g...  \n",
       "2    [testigo, bastante, casa, hija, año, encotrar,...  \n",
       "3    [interesante, explicativo, articulo, gracia, h...  \n",
       "4                                              [savir]  \n",
       "..                                                 ...  \n",
       "995                              [abducir, alienigena]  \n",
       "996  [municipio, caimanero, poder, destello, flash,...  \n",
       "997  [mfp, resolucion, cuc, cambiar, cup, aprobar, ...  \n",
       "998             [hora, caracteristica, sentir, frente]  \n",
       "999  [vivir, holguin, claramente, niña, ver, luz, r...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process corpus. Normalize. Using threads\n",
    "_start = time.time()\n",
    "ddf_comments = dd.from_pandas(df_comments, chunksize=10)\n",
    "df_comments[\"text\"] = ddf_comments[\"content\"].apply(clean, meta=('content', 'object')).compute()\n",
    "documents_normalized = list(df_comments[\"text\"].values)\n",
    "_end = time.time()\n",
    "print(f\"{len(documents_normalized)} Documents normalized in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Comments list\n",
    "tfidf_list = comments_tfidf(documents_normalized)\n",
    "\n",
    "unordered_tfidf: Dict[str, float] = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf[w] = unordered_tfidf.get(w, 0) + value\n",
    "\n",
    "\n",
    "# unordered_tfidf\n",
    "ordered_comments_tfidf = OrderedDict(sort_tfidf(unordered_tfidf))\n",
    "\n",
    "# WordCloud with word_token bigrams (ngram=2)\n",
    "token_comments_tfidf = dict()\n",
    "unordered_tfidf_ngram: Dict[str, float] = dict()\n",
    "documents_normalized_ngram = list()\n",
    "\n",
    "for doc_norm in documents_normalized:\n",
    "    documents_normalized_ngram.append(create_ngram(doc_norm, ngram=2))\n",
    "\n",
    "tfidf_list_ngram = comments_tfidf(documents_normalized_ngram)\n",
    "\n",
    "for tfidf in tfidf_list_ngram:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf_ngram[w] = unordered_tfidf_ngram.get(w, 0) + value\n",
    "\n",
    "ordered_comments_tfidf_ngram = OrderedDict(sort_tfidf(unordered_tfidf_ngram))\n",
    "\n",
    "for lemma_, value in ordered_comments_tfidf_ngram.items():\n",
    "    _token = \" \".join([str(bow_lemma_token[lm]) for lm in lemma_.split(\" \")])\n",
    "    token_comments_tfidf[_token] = value\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"comments_tfidf.json\", \"w\") as file_json:\n",
    "    json.dump(ordered_comments_tfidf_ngram, file_json)\n",
    "print(\"TF-IDF ordered saved to comments_tfidf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Habana Capitolio Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO: str = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example\n",
    "# in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK: str = (\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg\"\n",
    ")\n",
    "\n",
    "# download mask images\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, \"wb\") as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print(\"Image downloaded.\")\n",
    "    else:\n",
    "        print(\"Image No downloaded!\")\n",
    "else:\n",
    "    image_downloaded: str = IMG_CAPITOLIO.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "    print(f\"Image: {image_downloaded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordsCloud Cubadebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = \"wordcloud_cubadebate.png\"\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=500,\n",
    "    #     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color=\"white\",\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color=\"steelblue\",\n",
    "    stopwords=STOP_WORDS,\n",
    ").generate_from_frequencies(token_comments_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print(\"WordCloud Cubadebate image saved.\\n\")\n",
    "fig = plt.figure(figsize = (22, 15),)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Words (Lemma) with most TF-IDF (unigram: ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda DataFrame shape:(5827, 2)\n",
      "GroupBy DataFrame shape:(20, 2)\n",
      "\n",
      "    TF-IDF        Word\n",
      "0       29         ver\n",
      "1       21         luz\n",
      "2       20        caer\n",
      "3       19        cuba\n",
      "4       15      sentir\n",
      "5       14       pasar\n",
      "6       13    articulo\n",
      "7       12         dio\n",
      "8       11   estruendo\n",
      "9       10      entrar\n",
      "10       9      habana\n",
      "11       8         año\n",
      "12       7    explicar\n",
      "13       6    camagüey\n",
      "14       5        foto\n",
      "15       4   responder\n",
      "16       3   situacion\n",
      "17       2  desarrollo\n",
      "18       1         mar\n",
      "19       0    protejar\n",
      "\n",
      "Top Word Token and Post\n",
      "\n",
      "                 id                                              title  \\\n",
      "mar         1517271                                       Leer a Martí   \n",
      "cuba        1517299  Cuba cierra la jornada de este sábado con 796 ...   \n",
      "año         1517085  La Tira de los Lectores: ¿Qué paisaje cubano t...   \n",
      "estruendo   1517017  Investigaciones preliminares: No se han encont...   \n",
      "foto        1516981  En fotos, un volcán entra en erupción por prim...   \n",
      "luz         1516747  La danza de las palabras en la luz: un pacto e...   \n",
      "ver         1516945  Universidad Tecnológica de La Habana anuncia i...   \n",
      "pasar       1516935  El asteroide más grande del año pasará mañana ...   \n",
      "explicar    1516145  ¿Cómo explicar y hablar de ciencia en Cuba? (+...   \n",
      "caer        1516221  Servicios de Whatsapp, Facebook e Instagram se...   \n",
      "situacion   1516105  Aplican medidas más estrictas en Las Tunas ant...   \n",
      "dio         1515661  La economía mundial, enrgía y el medio ambient...   \n",
      "desarrollo  1514993  Cuba ratificó en foro de la CEPAL compromiso c...   \n",
      "sentir      1514051  Vale la pena… volver a escuchar: Pero no es co...   \n",
      "camagüey    1513333  Aplicación móvil TrabajarEnCuba gana aceptació...   \n",
      "articulo    1502341  Comercializan en La Habana artículos deportivo...   \n",
      "responder   1486487  ¿Qué número ves en la imagen? Pocas personas l...   \n",
      "entrar      1460041  Trump: “Biden solo entrará en la Casa Blanca s...   \n",
      "\n",
      "                                                          url        date  \n",
      "mar         http://www.cubadebate.cu/opinion/2021/03/21/le...  2021/03/21  \n",
      "cuba        http://www.cubadebate.cu/noticias/2021/03/21/c...  2021/03/21  \n",
      "año         http://www.cubadebate.cu/fotorreportajes/2021/...  2021/03/21  \n",
      "estruendo   http://www.cubadebate.cu/noticias/2021/03/20/i...  2021/03/20  \n",
      "foto        http://www.cubadebate.cu/fotorreportajes/2021/...  2021/03/20  \n",
      "luz         http://www.cubadebate.cu/especiales/2021/03/20...  2021/03/20  \n",
      "ver         http://www.cubadebate.cu/noticias/2021/03/20/u...  2021/03/20  \n",
      "pasar       http://www.cubadebate.cu/noticias/2021/03/20/e...  2021/03/20  \n",
      "explicar    http://www.cubadebate.cu/especiales/2021/03/19...  2021/03/19  \n",
      "caer        http://www.cubadebate.cu/noticias/2021/03/19/s...  2021/03/19  \n",
      "situacion   http://www.cubadebate.cu/noticias/2021/03/19/a...  2021/03/19  \n",
      "dio         http://www.cubadebate.cu/noticias/2021/03/18/l...  2021/03/18  \n",
      "desarrollo  http://www.cubadebate.cu/noticias/2021/03/16/c...  2021/03/16  \n",
      "sentir      http://www.cubadebate.cu/especiales/2021/03/15...  2021/03/15  \n",
      "camagüey    http://www.cubadebate.cu/noticias/2021/03/14/a...  2021/03/14  \n",
      "articulo    http://www.cubadebate.cu/noticias/2021/02/23/c...  2021/02/23  \n",
      "responder   http://www.cubadebate.cu/noticias/2021/01/25/q...  2021/01/25  \n",
      "entrar      http://www.cubadebate.cu/noticias/2020/11/27/t...  2020/11/27  \n",
      "\n",
      "Saved top_word_post.json\n",
      "\n",
      "Rewrite index.html.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Words (Lemma) with most TF-IDF (unigram: ngram=1)\n",
    "df = (\n",
    "    DataFrame.from_dict(\n",
    "        data=ordered_comments_tfidf, dtype=int, orient=\"index\", columns=[\"TF-IDF\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(index=str, columns={\"index\": \"Word\"})\n",
    ")\n",
    "rows, columns = df.shape\n",
    "print(f\"Panda DataFrame shape:({rows}, {columns})\")\n",
    "\n",
    "df_gb = df.groupby([\"TF-IDF\"], sort=False).first().reset_index()\n",
    "rows, columns = df_gb.shape\n",
    "print(f\"GroupBy DataFrame shape:({rows}, {columns})\\n\")\n",
    "print(df_gb)\n",
    "\n",
    "words_token = list(df_gb[\"Word\"].values)\n",
    "searches_dict = dict()\n",
    "searches_ids = set()\n",
    "\n",
    "for w_token in words_token:\n",
    "    search_list = get_searches([w_token])\n",
    "    # Get first searched result\n",
    "    if len(search_list) > 0:\n",
    "        search_id = search_list[0][\"id\"]\n",
    "        if search_id not in searches_ids:\n",
    "            searches_ids.add(search_id)\n",
    "            searches_dict[w_token] = search_list[0]\n",
    "\n",
    "# DataFrame First Search Posts\n",
    "if len(searches_dict) > 0:\n",
    "    searches_df = DataFrame.from_dict(\n",
    "        data=searches_dict,\n",
    "        orient=\"index\",\n",
    "    ).drop(columns=[\"type\", \"subtype\", \"_links\"])\n",
    "\n",
    "    df_dates = searches_df[\"url\"].str.extract(r\"(?P<date>\\d{4}/\\d{2}/\\d{2})\")\n",
    "\n",
    "    searches_by_dates = searches_df.join(df_dates).sort_values(\n",
    "        by=\"date\", ascending=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop Word Token and Post\\n\")\n",
    "    print(searches_by_dates)\n",
    "    searches_by_dates.to_json(os.path.join(_dir, \"top_word_post.json\"))\n",
    "    print(\"\\nSaved top_word_post.json\\n\")\n",
    "\n",
    "    if os.path.isfile(\"index.tpl\"):\n",
    "        with open(\"index.html\", \"w\", encoding=\"utf-8\") as f_index, open(\n",
    "            \"index.tpl\", \"r\"\n",
    "        ) as file:\n",
    "            tpl = file.read()\n",
    "            index_template = Template(tpl)\n",
    "\n",
    "            text_link = \"<ul>\\n\"\n",
    "            for _index, _title, _url in searches_by_dates.reset_index()[\n",
    "                [\"index\", \"title\", \"url\"]\n",
    "            ].values:\n",
    "                text_link += (\n",
    "                    f\"\\t<li><a href='{_url}' rel='external' \"\n",
    "                    f\"data-token='{_index}'>{_title}</a></li>\\n\"\n",
    "                )\n",
    "            text_link += \"</ul>\"\n",
    "            # Create index.html from index.tpl\n",
    "            f_index.write(index_template.substitute(CUBADEBATE_LINKS=text_link))\n",
    "        print(\"Rewrite index.html.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open** [index.html](index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
