{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2718,
     "status": "ok",
     "timestamp": 1587741794124,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "enR_SSI1lK7F",
    "outputId": "998b5448-dbcf-49c0-eee5-269fa3bbab07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import Counter, OrderedDict\n",
    "from datetime import datetime\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from string import Template\n",
    "import time\n",
    "from typing import cast, List, Optional, Dict, Tuple, Iterable, Any\n",
    "\n",
    "import requests\n",
    "import dask\n",
    "from dask import bag as db\n",
    "from dask import dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Annotations\n",
    "ElementList = List[Optional[Dict[str, Any]]]\n",
    "Document = str\n",
    "DocumentList = List[Document]\n",
    "DocumentNormalizedList = List[DocumentList]\n",
    "TermFrequency = Dict[str, float]\n",
    "IDF = Dict[str, float]\n",
    "TFIDF = Dict[str, float]\n",
    "TFIDF_List = List[TFIDF]\n",
    "\n",
    "CUBADEBATE_API = \"http://www.cubadebate.cu/wp-json/wp/v2/\"\n",
    "COMMENTS_ENDPOINT = CUBADEBATE_API + \"comments/\"\n",
    "SEARCH_ENDPOINT = CUBADEBATE_API + \"search/\"\n",
    "\n",
    "session = requests.Session()\n",
    "ConnectionErrorRequests = requests.exceptions.ConnectionError\n",
    "\n",
    "FECHA_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "\n",
    "def create_datetime(string: str) -> Optional[datetime]:\n",
    "    \"\"\"Create datetime from string '2014-07-03T23:27:51'\"\"\"\n",
    "    date_time = None\n",
    "\n",
    "    try:\n",
    "        if string is not None:\n",
    "            date_time = datetime.strptime(string, FECHA_FORMAT)\n",
    "    except ValueError:\n",
    "        print(\"Format no valid!\")\n",
    "\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def save_elements_json(name: str, line: str = None, mode=\"w\"):\n",
    "    \"\"\"Save elements to file, by lines JSON elments.\"\"\"\n",
    "    with open(name, mode) as _file:\n",
    "        if line:\n",
    "            _file.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def read_elements_json(name: str) -> Optional[list]:\n",
    "    \"\"\"Read by lines JSON data in file name.\n",
    "    Return list of elements or None\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(name, \"r\") as _file:\n",
    "            for line in _file.readlines():\n",
    "                if line:\n",
    "                    data += json.loads(line)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"Error reading lines of _file: {name}\")\n",
    "    except FileNotFoundError as f_error:\n",
    "        print(f_error)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_comments_file(name: str) -> DataFrame:\n",
    "    \"\"\"Get List of comments from file name\"\"\"\n",
    "    comments_json = read_elements_json(name)\n",
    "    comments_bag = db.from_sequence(comments_json).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[str] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_elements_json(url: str, **kwargs) -> ElementList:\n",
    "    \"\"\"Get JSON of list elements from endpoint API Wordpress v2\n",
    "\n",
    "    RETURNS (ElementList): Delayed Response List of elements in JSON format\n",
    "    to compute for Dask.Bag\n",
    "    \"\"\"\n",
    "    params = {\"page\": \"1\"}\n",
    "    params.update(kwargs)\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with session:\n",
    "            time.sleep(1)\n",
    "            resp = session.get(url, params=params)\n",
    "            if resp.status_code == 200:\n",
    "                results = resp.json()\n",
    "                if params.get(\"file\"):\n",
    "                    save_elements_json(\n",
    "                        params.get(\"file\", \"elements.dat\"), resp.text, \"a\"\n",
    "                    )\n",
    "                return results\n",
    "    except ConnectionErrorRequests:\n",
    "        pass  # Try again! Occurred Connection Error\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_comments(pages: int) -> DataFrame:\n",
    "    \"\"\"Get List of comments per page\"\"\"\n",
    "    comments_delayed = (\n",
    "        get_elements_json(url=COMMENTS_ENDPOINT, page=str(page), file=\"comments.dat\")\n",
    "        for page in range(1, pages + 1)\n",
    "    )\n",
    "\n",
    "    comments_bag = db.from_delayed(comments_delayed).map(\n",
    "        lambda d: {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"post\": d[\"post\"],\n",
    "            \"author_name\": d[\"author_name\"],\n",
    "            \"date\": create_datetime(d[\"date\"]),\n",
    "            \"content\": d[\"content\"][\"rendered\"],\n",
    "            \"link\": d[\"link\"],\n",
    "        }\n",
    "    )\n",
    "    # In Windows the Dask.Bag is multiprocessing by default, change to threads\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        comments: List[dict] = comments_bag.compute()\n",
    "    df_comments = DataFrame(comments)\n",
    "    return df_comments\n",
    "\n",
    "\n",
    "def get_searches(words: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get List of searches one page\"\"\"\n",
    "    search_delayed = (\n",
    "        get_elements_json(url=SEARCH_ENDPOINT, search=word) for word in words\n",
    "    )\n",
    "\n",
    "    searches_bag = db.from_delayed(search_delayed)\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        searches = searches_bag.compute()\n",
    "    return searches\n",
    "\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "bow_lemma_token: Dict[str, Token] = dict()\n",
    "\n",
    "\n",
    "def remplace_accents(text: str) -> str:\n",
    "    \"\"\"Remplace spanish accents vocab lower case: Unicode code point literal to str\"\"\"\n",
    "    text = re.sub(r\"á\", \"a\", text, flags=re.I)\n",
    "    text = re.sub(r\"é\", \"e\", text, flags=re.I)\n",
    "    text = re.sub(r\"í\", \"i\", text, flags=re.I)\n",
    "    text = re.sub(r\"ó\", \"o\", text, flags=re.I)\n",
    "    text = re.sub(r\"ú\", \"u\", text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(markup: str) -> str:\n",
    "    \"\"\"Remove html tags, URLs and spaces using regexp\"\"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", markup)\n",
    "    url_pattern = r\"(http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&#+]|[!*\\(\\),]|\\\n",
    "                   (?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_token(token: Token) -> str:\n",
    "    \"\"\"Remove grave accents and return lemmatized token lower case\"\"\"\n",
    "    result = remplace_accents(token.lemma_.strip().lower())\n",
    "    return result\n",
    "\n",
    "\n",
    "def is_token_allowed(token: Token) -> bool:\n",
    "    \"\"\"No Stop words, No Punctuations or len token >= 3\"\"\"\n",
    "    # Avoid token: inmiscuyéndose lemma_ \"inmiscuir el\"\n",
    "    if (\n",
    "        not token\n",
    "        or token.is_space\n",
    "        or token.is_stop\n",
    "        or token.is_punct\n",
    "        or len(token) < 3\n",
    "        or \" \" in token.lemma_.strip()\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean(doc: str) -> List[str]:\n",
    "    \"\"\"Remove grave accents, stopwords, the punctuations and normalize the corpus.\"\"\"\n",
    "    text = get_text(doc)\n",
    "    text = text.lower()\n",
    "    tokens = []\n",
    "\n",
    "    for token in nlp(text):\n",
    "        if is_token_allowed(token):\n",
    "            word_lemma = preprocess_token(token)\n",
    "            bow_lemma_token[word_lemma] = token\n",
    "            tokens.append(word_lemma)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_ngram(document: DocumentList, ngram=1) -> DocumentList:\n",
    "    \"\"\"Create N-Gram from document\"\"\"\n",
    "    return [\n",
    "        \" \".join(document[i : i + ngram]) for i in range(len(document) - (ngram - 1))\n",
    "    ]\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "def comment_tf(document: DocumentList) -> TermFrequency:\n",
    "    \"\"\"Term Frequency for a document\"\"\"\n",
    "    bow = Counter(document)\n",
    "    tf_dict = dict()\n",
    "\n",
    "    for k, v in bow.most_common():\n",
    "        tf_dict[k] = v / len(document)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def comments_tf(documents: DocumentNormalizedList) -> List[TermFrequency]:\n",
    "    \"\"\"Term Frequency for many document\"\"\"\n",
    "    return [comment_tf(comment) for comment in documents]\n",
    "\n",
    "\n",
    "def count_dict(documents: DocumentNormalizedList) -> Dict[str, int]:\n",
    "    \"\"\"Counter the word in all documents at least ones\"\"\"\n",
    "    counts: Dict[str, int] = dict()\n",
    "    for document in documents:\n",
    "        uniq_words = set(document)\n",
    "        for word in uniq_words:\n",
    "            _value = counts.get(word, 0)\n",
    "            counts[word] = _value + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def idf_dict(documents: DocumentNormalizedList) -> IDF:\n",
    "    \"\"\"Inverse Document Frequency in all Documents\"\"\"\n",
    "    idf_d = dict()\n",
    "\n",
    "    counts = count_dict(documents)\n",
    "\n",
    "    for word in counts:\n",
    "        idf_d[word] = math.log(len(documents) / counts[word])\n",
    "\n",
    "    return idf_d\n",
    "\n",
    "\n",
    "def comments_tfidf(documents: DocumentNormalizedList) -> TFIDF_List:\n",
    "    \"\"\"TF-IDF of all Documents Normalized\"\"\"\n",
    "    tfidf_comments = []\n",
    "\n",
    "    idf_comments = idf_dict(documents)\n",
    "\n",
    "    def compute_tfidf_comment(document: DocumentList) -> TFIDF:\n",
    "        \"\"\"Compute TF-IDF for a Document\"\"\"\n",
    "        tfidf_comment = dict()\n",
    "\n",
    "        tf_comment = comment_tf(document)\n",
    "\n",
    "        for word in set(document):\n",
    "            tfidf_comment[word] = tf_comment[word] * idf_comments[word]\n",
    "\n",
    "        return tfidf_comment\n",
    "\n",
    "    for comment in documents:\n",
    "        tfidf_comments.append(compute_tfidf_comment(comment))\n",
    "\n",
    "    return tfidf_comments\n",
    "\n",
    "\n",
    "def sort_tfidf(tfidf_unordered) -> Iterable[Tuple[Any, Any]]:\n",
    "    return sorted(tfidf_unordered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def export_image2html(image_name: str) -> None:\n",
    "    \"\"\"Export a image to html file\"\"\"\n",
    "    with open(image_name, \"rb\") as file_img:\n",
    "        data = base64.b64encode(file_img.read()).decode(\"utf-8\")\n",
    "\n",
    "    img_str = \"\"\"\n",
    "    <img width=\"100%\" height=\"100%\"\n",
    "    src=\"data:image/png;base64,{}\" />\n",
    "    \"\"\".format(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    with open(\"wordcloud_cubadebate.html\", \"w\") as _html:\n",
    "        _html.write(img_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "## Download Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Comments downloaded in 6.86s\n"
     ]
    }
   ],
   "source": [
    "_start = time.time()\n",
    "df_comments =  get_comments(pages=1)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# df_comments.info()\n",
    "# df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Comments downloaded in 361.52s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9186445</td>\n",
       "      <td>1519975</td>\n",
       "      <td>Eliades Hidalgo Torres</td>\n",
       "      <td>2021-03-26 16:05:02</td>\n",
       "      <td>&lt;p&gt;Impresionante historia. Y como dijo un fori...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9186443</td>\n",
       "      <td>1519975</td>\n",
       "      <td>Juan Emilio</td>\n",
       "      <td>2021-03-26 16:04:33</td>\n",
       "      <td>&lt;p&gt;Mis respetos y admiración Enoel, a cubanos ...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186397</td>\n",
       "      <td>1519975</td>\n",
       "      <td>OERM</td>\n",
       "      <td>2021-03-26 15:50:43</td>\n",
       "      <td>&lt;p&gt;Felicitaciones para todos los hombres y muj...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9186357</td>\n",
       "      <td>1519975</td>\n",
       "      <td>JAMEZ</td>\n",
       "      <td>2021-03-26 15:35:46</td>\n",
       "      <td>&lt;p&gt;Hermosa historia....cuanto orgullo siento d...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9186321</td>\n",
       "      <td>1519975</td>\n",
       "      <td>Vamos por mas</td>\n",
       "      <td>2021-03-26 15:28:51</td>\n",
       "      <td>&lt;p&gt;Gracias Yunier, porque con este articulo es...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9179987</td>\n",
       "      <td>1519231</td>\n",
       "      <td>Sergio</td>\n",
       "      <td>2021-03-24 23:45:01</td>\n",
       "      <td>&lt;p&gt;Hasta que no sea tipificado el RACISMO como...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9179983</td>\n",
       "      <td>1519231</td>\n",
       "      <td>Ramón</td>\n",
       "      <td>2021-03-24 23:43:47</td>\n",
       "      <td>&lt;p&gt;Cuando estoy viendo la TV, principalmente c...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>9179975</td>\n",
       "      <td>1519235</td>\n",
       "      <td>Nydan</td>\n",
       "      <td>2021-03-24 23:42:54</td>\n",
       "      <td>&lt;p&gt;No me digas, de verdad? Que Cuba recoja a s...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/24/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9179965</td>\n",
       "      <td>1517343</td>\n",
       "      <td>fjglez</td>\n",
       "      <td>2021-03-24 23:33:22</td>\n",
       "      <td>&lt;p&gt;Cubadebate contactó para la realización de ...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9179961</td>\n",
       "      <td>1510573</td>\n",
       "      <td>Ali Suarez</td>\n",
       "      <td>2021-03-24 23:27:53</td>\n",
       "      <td>&lt;p&gt;Éramos felices y no lo sabíamos.&lt;br /&gt;\\nSom...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/10/e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post             author_name                date  \\\n",
       "0    9186445  1519975  Eliades Hidalgo Torres 2021-03-26 16:05:02   \n",
       "1    9186443  1519975             Juan Emilio 2021-03-26 16:04:33   \n",
       "2    9186397  1519975                    OERM 2021-03-26 15:50:43   \n",
       "3    9186357  1519975                   JAMEZ 2021-03-26 15:35:46   \n",
       "4    9186321  1519975           Vamos por mas 2021-03-26 15:28:51   \n",
       "..       ...      ...                     ...                 ...   \n",
       "995  9179987  1519231                  Sergio 2021-03-24 23:45:01   \n",
       "996  9179983  1519231                   Ramón 2021-03-24 23:43:47   \n",
       "997  9179975  1519235                   Nydan 2021-03-24 23:42:54   \n",
       "998  9179965  1517343                  fjglez 2021-03-24 23:33:22   \n",
       "999  9179961  1510573              Ali Suarez 2021-03-24 23:27:53   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Impresionante historia. Y como dijo un fori...   \n",
       "1    <p>Mis respetos y admiración Enoel, a cubanos ...   \n",
       "2    <p>Felicitaciones para todos los hombres y muj...   \n",
       "3    <p>Hermosa historia....cuanto orgullo siento d...   \n",
       "4    <p>Gracias Yunier, porque con este articulo es...   \n",
       "..                                                 ...   \n",
       "995  <p>Hasta que no sea tipificado el RACISMO como...   \n",
       "996  <p>Cuando estoy viendo la TV, principalmente c...   \n",
       "997  <p>No me digas, de verdad? Que Cuba recoja a s...   \n",
       "998  <p>Cubadebate contactó para la realización de ...   \n",
       "999  <p>Éramos felices y no lo sabíamos.<br />\\nSom...   \n",
       "\n",
       "                                                  link  \n",
       "0    http://www.cubadebate.cu/especiales/2021/03/26...  \n",
       "1    http://www.cubadebate.cu/especiales/2021/03/26...  \n",
       "2    http://www.cubadebate.cu/especiales/2021/03/26...  \n",
       "3    http://www.cubadebate.cu/especiales/2021/03/26...  \n",
       "4    http://www.cubadebate.cu/especiales/2021/03/26...  \n",
       "..                                                 ...  \n",
       "995  http://www.cubadebate.cu/especiales/2021/03/24...  \n",
       "996  http://www.cubadebate.cu/especiales/2021/03/24...  \n",
       "997  http://www.cubadebate.cu/noticias/2021/03/24/c...  \n",
       "998  http://www.cubadebate.cu/especiales/2021/03/22...  \n",
       "999  http://www.cubadebate.cu/noticias/2021/03/10/e...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate file comments.dat\n",
    "save_elements_json(\"comments.dat\")\n",
    "NUM_PAGES = 100\n",
    "\n",
    "_start = time.time()\n",
    "df_comments = get_comments(pages=NUM_PAGES)\n",
    "_end = time.time()\n",
    "print(f\"{len(df_comments)} Comments downloaded in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not df_comments.empty, \"No comments to process\"\n",
    "# Uncomment the next for get the comments from file\n",
    "# df_comments = get_comments_file(\"comments.dat\")\n",
    "# print(f\"{len(df_comments)} Comments from file comments.dat\")\n",
    "# df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process corpus. Normalize. Using threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Documents normalized in 225.84s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9186445</td>\n",
       "      <td>1519975</td>\n",
       "      <td>Eliades Hidalgo Torres</td>\n",
       "      <td>2021-03-26 16:05:02</td>\n",
       "      <td>&lt;p&gt;Impresionante historia. Y como dijo un fori...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "      <td>[impresionante, historia, forista, hombre, abn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9186443</td>\n",
       "      <td>1519975</td>\n",
       "      <td>Juan Emilio</td>\n",
       "      <td>2021-03-26 16:04:33</td>\n",
       "      <td>&lt;p&gt;Mis respetos y admiración Enoel, a cubanos ...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "      <td>[respeto, admiracion, enoel, cubano, digno, ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186397</td>\n",
       "      <td>1519975</td>\n",
       "      <td>OERM</td>\n",
       "      <td>2021-03-26 15:50:43</td>\n",
       "      <td>&lt;p&gt;Felicitaciones para todos los hombres y muj...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "      <td>[felicitacion, hombre, mujer, enoel, sacrifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9186357</td>\n",
       "      <td>1519975</td>\n",
       "      <td>JAMEZ</td>\n",
       "      <td>2021-03-26 15:35:46</td>\n",
       "      <td>&lt;p&gt;Hermosa historia....cuanto orgullo siento d...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "      <td>[hermoso, historia, orgullo, sentir, joven, cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9186321</td>\n",
       "      <td>1519975</td>\n",
       "      <td>Vamos por mas</td>\n",
       "      <td>2021-03-26 15:28:51</td>\n",
       "      <td>&lt;p&gt;Gracias Yunier, porque con este articulo es...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/26...</td>\n",
       "      <td>[gracia, yunier, articulo, dar, heroe, anonimo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9179987</td>\n",
       "      <td>1519231</td>\n",
       "      <td>Sergio</td>\n",
       "      <td>2021-03-24 23:45:01</td>\n",
       "      <td>&lt;p&gt;Hasta que no sea tipificado el RACISMO como...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/24...</td>\n",
       "      <td>[tipificar, racismo, delito, pena, correspondi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9179983</td>\n",
       "      <td>1519231</td>\n",
       "      <td>Ramón</td>\n",
       "      <td>2021-03-24 23:43:47</td>\n",
       "      <td>&lt;p&gt;Cuando estoy viendo la TV, principalmente c...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/24...</td>\n",
       "      <td>[ver, ver, video, clic, musical, preguntar, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>9179975</td>\n",
       "      <td>1519235</td>\n",
       "      <td>Nydan</td>\n",
       "      <td>2021-03-24 23:42:54</td>\n",
       "      <td>&lt;p&gt;No me digas, de verdad? Que Cuba recoja a s...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/24/c...</td>\n",
       "      <td>[digar, cubir, recojar, pelotero, profesionali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9179965</td>\n",
       "      <td>1517343</td>\n",
       "      <td>fjglez</td>\n",
       "      <td>2021-03-24 23:33:22</td>\n",
       "      <td>&lt;p&gt;Cubadebate contactó para la realización de ...</td>\n",
       "      <td>http://www.cubadebate.cu/especiales/2021/03/22...</td>\n",
       "      <td>[cubadebate, contactar, realizacion, corporaci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9179961</td>\n",
       "      <td>1510573</td>\n",
       "      <td>Ali Suarez</td>\n",
       "      <td>2021-03-24 23:27:53</td>\n",
       "      <td>&lt;p&gt;Éramos felices y no lo sabíamos.&lt;br /&gt;\\nSom...</td>\n",
       "      <td>http://www.cubadebate.cu/noticias/2021/03/10/e...</td>\n",
       "      <td>[ser, feliz, saber, vulnerable, comprend, año,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     post             author_name                date  \\\n",
       "0    9186445  1519975  Eliades Hidalgo Torres 2021-03-26 16:05:02   \n",
       "1    9186443  1519975             Juan Emilio 2021-03-26 16:04:33   \n",
       "2    9186397  1519975                    OERM 2021-03-26 15:50:43   \n",
       "3    9186357  1519975                   JAMEZ 2021-03-26 15:35:46   \n",
       "4    9186321  1519975           Vamos por mas 2021-03-26 15:28:51   \n",
       "..       ...      ...                     ...                 ...   \n",
       "995  9179987  1519231                  Sergio 2021-03-24 23:45:01   \n",
       "996  9179983  1519231                   Ramón 2021-03-24 23:43:47   \n",
       "997  9179975  1519235                   Nydan 2021-03-24 23:42:54   \n",
       "998  9179965  1517343                  fjglez 2021-03-24 23:33:22   \n",
       "999  9179961  1510573              Ali Suarez 2021-03-24 23:27:53   \n",
       "\n",
       "                                               content  \\\n",
       "0    <p>Impresionante historia. Y como dijo un fori...   \n",
       "1    <p>Mis respetos y admiración Enoel, a cubanos ...   \n",
       "2    <p>Felicitaciones para todos los hombres y muj...   \n",
       "3    <p>Hermosa historia....cuanto orgullo siento d...   \n",
       "4    <p>Gracias Yunier, porque con este articulo es...   \n",
       "..                                                 ...   \n",
       "995  <p>Hasta que no sea tipificado el RACISMO como...   \n",
       "996  <p>Cuando estoy viendo la TV, principalmente c...   \n",
       "997  <p>No me digas, de verdad? Que Cuba recoja a s...   \n",
       "998  <p>Cubadebate contactó para la realización de ...   \n",
       "999  <p>Éramos felices y no lo sabíamos.<br />\\nSom...   \n",
       "\n",
       "                                                  link  \\\n",
       "0    http://www.cubadebate.cu/especiales/2021/03/26...   \n",
       "1    http://www.cubadebate.cu/especiales/2021/03/26...   \n",
       "2    http://www.cubadebate.cu/especiales/2021/03/26...   \n",
       "3    http://www.cubadebate.cu/especiales/2021/03/26...   \n",
       "4    http://www.cubadebate.cu/especiales/2021/03/26...   \n",
       "..                                                 ...   \n",
       "995  http://www.cubadebate.cu/especiales/2021/03/24...   \n",
       "996  http://www.cubadebate.cu/especiales/2021/03/24...   \n",
       "997  http://www.cubadebate.cu/noticias/2021/03/24/c...   \n",
       "998  http://www.cubadebate.cu/especiales/2021/03/22...   \n",
       "999  http://www.cubadebate.cu/noticias/2021/03/10/e...   \n",
       "\n",
       "                                                  text  \n",
       "0    [impresionante, historia, forista, hombre, abn...  \n",
       "1    [respeto, admiracion, enoel, cubano, digno, ri...  \n",
       "2    [felicitacion, hombre, mujer, enoel, sacrifica...  \n",
       "3    [hermoso, historia, orgullo, sentir, joven, cu...  \n",
       "4    [gracia, yunier, articulo, dar, heroe, anonimo...  \n",
       "..                                                 ...  \n",
       "995  [tipificar, racismo, delito, pena, correspondi...  \n",
       "996  [ver, ver, video, clic, musical, preguntar, ra...  \n",
       "997  [digar, cubir, recojar, pelotero, profesionali...  \n",
       "998  [cubadebate, contactar, realizacion, corporaci...  \n",
       "999  [ser, feliz, saber, vulnerable, comprend, año,...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process corpus. Normalize. Using threads\n",
    "_start = time.time()\n",
    "dtype = {\n",
    "    \"id\": \"int64\",\n",
    "    \"post\": \"int64\",\n",
    "    \"author_name\": \"object\",\n",
    "    \"date\": \"datetime64[ns]\",\n",
    "    \"content\": \"object\",\n",
    "    \"link\": \"object\",\n",
    "    \"text\": \"object\",\n",
    "}\n",
    "ddf_comments = dd.from_pandas(df_comments, npartitions=10)\n",
    "ddf_comments = cast(dd.core.DataFrame, ddf_comments).map_partitions(\n",
    "    lambda df: df.assign(text=[clean(str(content)) for content in df.content]),\n",
    "    meta=dtype,\n",
    ")\n",
    "# Depend graphviz\n",
    "# ddf_comments.visualize(filename=\"comments_text.png\", rankdir=\"TD\")\n",
    "df_comments = ddf_comments.compute()\n",
    "documents_normalized = list(df_comments[\"text\"].values)\n",
    "_end = time.time()\n",
    "print(f\"{len(documents_normalized)} Documents normalized in {(_end - _start):.2f}s\")\n",
    "# print(df_comments.info())\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1587741868078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1W3S2crllK9M",
    "outputId": "b669d690-6a08-4693-ee32-a770288ad699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ordered saved to comments_tfidf.json\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Comments list\n",
    "tfidf_list = comments_tfidf(documents_normalized)\n",
    "\n",
    "unordered_tfidf: Dict[str, float] = dict()\n",
    "\n",
    "for tfidf in tfidf_list:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf[w] = unordered_tfidf.get(w, 0) + value\n",
    "\n",
    "\n",
    "# unordered_tfidf\n",
    "ordered_comments_tfidf = OrderedDict(sort_tfidf(unordered_tfidf))\n",
    "\n",
    "# WordCloud with word_token bigrams (ngram=2)\n",
    "token_comments_tfidf = dict()\n",
    "unordered_tfidf_ngram: Dict[str, float] = dict()\n",
    "documents_normalized_ngram = list()\n",
    "\n",
    "for doc_norm in documents_normalized:\n",
    "    documents_normalized_ngram.append(create_ngram(doc_norm, ngram=2))\n",
    "\n",
    "tfidf_list_ngram = comments_tfidf(documents_normalized_ngram)\n",
    "\n",
    "for tfidf in tfidf_list_ngram:\n",
    "    for w, value in tfidf.items():\n",
    "        unordered_tfidf_ngram[w] = unordered_tfidf_ngram.get(w, 0) + value\n",
    "\n",
    "ordered_comments_tfidf_ngram = OrderedDict(sort_tfidf(unordered_tfidf_ngram))\n",
    "\n",
    "for lemma_, value in ordered_comments_tfidf_ngram.items():\n",
    "    _token = \" \".join([str(bow_lemma_token[lm]) for lm in lemma_.split(\" \")])\n",
    "    token_comments_tfidf[_token] = value\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"comments_tfidf.json\", \"w\") as file_json:\n",
    "    json.dump(ordered_comments_tfidf_ngram, file_json)\n",
    "print(\"TF-IDF ordered saved to comments_tfidf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Habana Capitolio Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: capitolio.jpg\n"
     ]
    }
   ],
   "source": [
    "# Capitolio Habana\n",
    "IMG_CAPITOLIO: str = os.getenv(\"IMG_CAPITOLIO\") or \"capitolio.jpg\"\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example\n",
    "# in generated IPython notebook)\n",
    "_dir = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "IMG_CAPITOLIO = os.path.join(_dir, IMG_CAPITOLIO)\n",
    "IMG_CAPITOLIO_LINK: str = (\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/8/8f/Capitolio_full.jpg\"\n",
    ")\n",
    "\n",
    "# download mask images\n",
    "if not os.path.isfile(IMG_CAPITOLIO):\n",
    "    response = requests.get(IMG_CAPITOLIO_LINK)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(IMG_CAPITOLIO, \"wb\") as _capitolio:\n",
    "            _capitolio.write(response.content)\n",
    "            print(\"Image downloaded.\")\n",
    "    else:\n",
    "        print(\"Image No downloaded!\")\n",
    "else:\n",
    "    image_downloaded: str = IMG_CAPITOLIO.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "    print(f\"Image: {image_downloaded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordsCloud Cubadebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBHrq118aPvV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# WordsCloud Cubadebate\n",
    "IMG_WORDCLOUD = \"wordcloud_cubadebate.png\"\n",
    "\n",
    "# read the mask image\n",
    "_mask = np.array(Image.open(IMG_CAPITOLIO))\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    max_words=500,\n",
    "    #     max_font_size=50,\n",
    "    height=1440,\n",
    "    width=2160,\n",
    "    background_color=\"white\",\n",
    "    mask=_mask,\n",
    "    contour_width=1,\n",
    "    contour_color=\"steelblue\",\n",
    "    stopwords=STOP_WORDS,\n",
    ").generate_from_frequencies(token_comments_tfidf)\n",
    "# Save to file\n",
    "wordcloud.to_file(IMG_WORDCLOUD)\n",
    "print(\"WordCloud Cubadebate image saved.\\n\")\n",
    "fig = plt.figure(figsize = (22, 15),)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Words (Lemma) with most TF-IDF (unigram: ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda DataFrame shape:(7260, 2)\n",
      "GroupBy DataFrame shape:(18, 2)\n",
      "\n",
      "    TF-IDF        Word\n",
      "0       22        cuba\n",
      "1       20   felicidad\n",
      "2       19    articulo\n",
      "3       14        foto\n",
      "4       13        pais\n",
      "5       12     gracias\n",
      "6       11     pequeño\n",
      "7       10        vida\n",
      "8        9       mundo\n",
      "9        8       pasar\n",
      "10       7      granma\n",
      "11       6  revolucion\n",
      "12       5      precio\n",
      "13       4      futuro\n",
      "14       3    escribir\n",
      "15       2         mes\n",
      "16       1       lista\n",
      "17       0   actividad\n",
      "\n",
      "Top Word Token and Post\n",
      "\n",
      "                 id                                              title  \\\n",
      "cuba        1520383  Cuba ratifica voluntad de ampliar relaciones e...   \n",
      "vida        1520257  La actividad humana pone a los mamíferos oceán...   \n",
      "precio      1520169  Los precios de los materiales de la construcci...   \n",
      "lista       1519875  Punto Deportivo: Debut de Cuba en eliminatoria...   \n",
      "articulo    1519937  The New York Times vende un artículo periodíst...   \n",
      "foto        1519849  Recibe Díaz-Canel reconocimiento de la Univers...   \n",
      "pais        1519477  Nadie puede decir exactamente qué día llegará:...   \n",
      "mundo       1519393  Noruega construirá primer túnel para buques en...   \n",
      "revolucion  1519189  Fidel hace 60 años: ¡Tanto más podremos sentir...   \n",
      "granma      1517397       Matanzas vs Granma: El rey contra el verdugo   \n",
      "felicidad   1516825  Día Internacional de la Felicidad: ¿Qué nos ha...   \n",
      "pasar       1516935  El asteroide más grande del año pasará mañana ...   \n",
      "futuro      1516841  Retransmitirán Mesa Redonda, la Unión Eléctric...   \n",
      "pequeño     1512177     Chang, pequeño de estatura, gigante de talento   \n",
      "escribir    1499845  Descargue el libro de Leonardo Depestre “La Ha...   \n",
      "gracias     1497603                        Gracias a la vida (+ Video)   \n",
      "\n",
      "                                                          url        date  \n",
      "cuba        http://www.cubadebate.cu/noticias/2021/03/26/c...  2021/03/26  \n",
      "vida        http://www.cubadebate.cu/noticias/2021/03/26/l...  2021/03/26  \n",
      "precio      http://www.cubadebate.cu/noticias/2021/03/26/l...  2021/03/26  \n",
      "lista       http://www.cubadebate.cu/especiales/2021/03/26...  2021/03/26  \n",
      "articulo    http://www.cubadebate.cu/noticias/2021/03/25/t...  2021/03/25  \n",
      "foto        http://www.cubadebate.cu/noticias/2021/03/25/r...  2021/03/25  \n",
      "pais        http://www.cubadebate.cu/especiales/2021/03/25...  2021/03/25  \n",
      "mundo       http://www.cubadebate.cu/noticias/2021/03/25/n...  2021/03/25  \n",
      "revolucion  http://www.cubadebate.cu/especiales/2021/03/25...  2021/03/25  \n",
      "granma      http://www.cubadebate.cu/especiales/2021/03/21...  2021/03/21  \n",
      "felicidad   http://www.cubadebate.cu/fotorreportajes/2021/...  2021/03/20  \n",
      "pasar       http://www.cubadebate.cu/noticias/2021/03/20/e...  2021/03/20  \n",
      "futuro      http://www.cubadebate.cu/noticias/2021/03/20/r...  2021/03/20  \n",
      "pequeño     http://www.cubadebate.cu/especiales/2021/03/12...  2021/03/12  \n",
      "escribir    http://www.cubadebate.cu/libros-libres/2021/02...  2021/02/20  \n",
      "gracias     http://www.cubadebate.cu/especiales/2021/02/16...  2021/02/16  \n",
      "\n",
      "Saved top_word_post.json\n",
      "\n",
      "Rewrite index.html.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Words (Lemma) with most TF-IDF (unigram: ngram=1)\n",
    "df = (\n",
    "    DataFrame.from_dict(\n",
    "        data=ordered_comments_tfidf, dtype=int, orient=\"index\", columns=[\"TF-IDF\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(index=str, columns={\"index\": \"Word\"})\n",
    ")\n",
    "rows, columns = df.shape\n",
    "print(f\"Panda DataFrame shape:({rows}, {columns})\")\n",
    "\n",
    "df_gb = df.groupby([\"TF-IDF\"], sort=False).first().reset_index()\n",
    "rows, columns = df_gb.shape\n",
    "print(f\"GroupBy DataFrame shape:({rows}, {columns})\\n\")\n",
    "print(df_gb)\n",
    "\n",
    "words_token = list(df_gb[\"Word\"].values)\n",
    "searches_dict = dict()\n",
    "searches_ids = set()\n",
    "\n",
    "for w_token in words_token:\n",
    "    search_list = get_searches([w_token])\n",
    "    # Get first searched result\n",
    "    if len(search_list) > 0:\n",
    "        search_id = search_list[0][\"id\"]\n",
    "        if search_id not in searches_ids:\n",
    "            searches_ids.add(search_id)\n",
    "            searches_dict[w_token] = search_list[0]\n",
    "\n",
    "# DataFrame First Search Posts\n",
    "if len(searches_dict) > 0:\n",
    "    searches_df = DataFrame.from_dict(\n",
    "        data=searches_dict,\n",
    "        orient=\"index\",\n",
    "    ).drop(columns=[\"type\", \"subtype\", \"_links\"])\n",
    "\n",
    "    df_dates = searches_df[\"url\"].str.extract(r\"(?P<date>\\d{4}/\\d{2}/\\d{2})\")\n",
    "\n",
    "    searches_by_dates = searches_df.join(df_dates).sort_values(\n",
    "        by=\"date\", ascending=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop Word Token and Post\\n\")\n",
    "    print(searches_by_dates)\n",
    "    searches_by_dates.to_json(os.path.join(_dir, \"top_word_post.json\"))\n",
    "    print(\"\\nSaved top_word_post.json\\n\")\n",
    "\n",
    "    if os.path.isfile(\"index.tpl\"):\n",
    "        with open(\"index.html\", \"w\", encoding=\"utf-8\") as f_index, open(\n",
    "            \"index.tpl\", \"r\"\n",
    "        ) as file:\n",
    "            tpl = file.read()\n",
    "            index_template = Template(tpl)\n",
    "\n",
    "            text_link = \"<ul>\\n\"\n",
    "            for _index, _title, _url in searches_by_dates.reset_index()[\n",
    "                [\"index\", \"title\", \"url\"]\n",
    "            ].values:\n",
    "                text_link += (\n",
    "                    f\"\\t<li><a href='{_url}' rel='external' \"\n",
    "                    f\"data-token='{_index}'>{_title}</a></li>\\n\"\n",
    "                )\n",
    "            text_link += \"</ul>\"\n",
    "            # Create index.html from index.tpl\n",
    "            f_index.write(index_template.substitute(CUBADEBATE_LINKS=text_link))\n",
    "        print(\"Rewrite index.html.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open** [index.html](index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUBADEBATE_SPACY.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587742373686
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1587735882025
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586838864452
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586455776456
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586383118458
    },
    {
     "file_id": "https://github.com/oleksis/cubadebate/blob/master/CUBADEBATE_SPACY.ipynb",
     "timestamp": 1586377302982
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
